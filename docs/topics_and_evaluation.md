# Topics and Evaluation

This document describes the process of generating topics and relevance judgments (qrels) for evaluating information retrieval systems.

## ğŸ§ª Topic Generation

We generated NIST-style topics by prompting multiple language models (GPT-4o, Claude 3.5, and Gemini 2.5 Pro) to simulate university stakeholders (students, faculty, staff, parents, administration). Each was asked to create realistic search tasks reflecting information needs.

### Topic Generator Prompt

We used the following prompt to generate the topics:

```
You are acting as a NIST assessor tasked with creating evaluation topics for an academic question-answering system. Your goal is to generate topics that reflect realistic information needs of students, faculty, or staff seeking information from a university department's website. Each topic should be structured with the following components:

1. Title: A concise question or query that a user might pose.

2. Description: A brief explanation providing context or elaboration on the title.

3. Narrative: Detailed criteria outlining what constitutes a relevant answer, including any nuances or specific information that should be included or excluded.

Please generate a set of 20 such topics, ensuring diversity in user roles (e.g., undergraduate student, graduate student, faculty member, administrative staff) and information needs (e.g., course information, departmental policies, research opportunities, administrative procedures).

ğŸ“ Example Output:

Topic 1
* Title: How can I declare a minor in Data Science?
* Description: An undergraduate student is interested in adding a Data Science minor to their current major and seeks information on the declaration process.
* Narrative: Relevant documents should detail the steps required to declare a Data Science minor, including eligibility criteria, necessary forms, deadlines, and advising resources. Information solely about major declarations or unrelated minors is not relevant.

Topic 2
* Title: What are the prerequisites for enrolling in Advanced Algorithms (CS 5010)?
* Description: A graduate student wants to know the required background before enrolling in CS 5010.
* Narrative: Relevant content should specify prerequisite courses, knowledge areas, or skills needed for CS 5010. General course descriptions without prerequisite information are not sufficient.
```

## ğŸ”§ Automatic Relevance Judgments (Qrels)

We used the following keyword-matching approach to assign relevance scores to passages based on topic overlap:

```python
# Keyword matching qrel generation
if word_matches >= 3:
    rel_score = 2  # Highly relevant
elif word_matches >= 1:
    rel_score = 1  # Somewhat relevant
else:
    rel_score = 0  # Not relevant
```

Qrels are generated by matching topic titles + descriptions with passage text (excluding stopwords), then counting keyword overlap.

Each output line is in TREC format: `<topic_id> 0 <passage_id> <relevance_score>`

## Evaluation Plans

We plan to evaluate retrieval effectiveness using multiple retrieval models:

- **Sparse Retrieval**: BM25, BM25+RM3
- **Dense Retrieval**: Various transformer-based embedding models
- **Hybrid Retrieval**: Combining sparse and dense signals

The evaluation will be conducted using standard IR metrics:
- Mean Average Precision (MAP)
- Normalized Discounted Cumulative Gain (NDCG)
- Precision at k (P@k)
- Mean Reciprocal Rank (MRR)

## TODO (Help Wanted! ğŸ™‹â€â™€ï¸ğŸ™‹)

We plan to evaluate retrieval effectiveness using multiple models (BM25, dense, hybrid) and assess generated responses with LLMs in a TREC-style human judgment loop. Contributions welcome!
