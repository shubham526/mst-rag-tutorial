{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Domain-Specific RAG: A Practical Tutorial\n"
      ],
      "metadata": {
        "id": "zVwrsNZ6JmpL"
      },
      "id": "zVwrsNZ6JmpL"
    },
    {
      "cell_type": "markdown",
      "id": "d5e9133c",
      "metadata": {
        "id": "d5e9133c"
      },
      "source": [
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        " - Understand the basic concepts of Retrieval-Augmented Generation (RAG).\n",
        " - Implement a simple RAG pipeline using pre-built indexes and models.\n",
        " - Apply domain-specific prompting and evaluation metrics to tailor RAG to different use cases.\n",
        " - Explore the impact of different retrieval and generation parameters on the performance of a RAG system.\n",
        "\n",
        "## Exercises and Challenges (Optional)\n",
        "\n",
        " - **Experiment with different datasets:** Try using other pre-built indexes or creating your own indexes from a dataset of your choice.\n",
        " - **Fine-tune the retrieval model:** Explore fine-tuning the bi-encoder or cross-encoder to improve retrieval accuracy for your domain.\n",
        " - **Evaluate different LLMs:** Compare the performance of different LLMs for answer generation in your use case.\n",
        " - **Build a user interface:** Develop a simple web application or chatbot that integrates your RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üñ•Ô∏è System Requirements & Setup\n",
        "\n",
        "### System Requirements\n",
        "\n",
        "This tutorial uses deep learning models and vector similarity search that benefit from:\n",
        "\n",
        "- **Compute Resources:**\n",
        "  - **RAM:** Minimum 8GB, recommended 16GB+\n",
        "  - **GPU:** Recommended for faster processing (especially when using cross-encoders)\n",
        "  - **Storage:** ~2GB for pre-built indexes and models\n",
        "  - **Google Colab:** This notebook runs well on a free Colab instance with T4 GPU\n",
        "\n",
        "### üß∞ Tools & Libraries We'll Use\n",
        "\n",
        "This tutorial leverages several key libraries:\n",
        "\n",
        "- **[sentence-transformers](https://www.sbert.net/)**: For embedding generation and cross-encoding\n",
        "- **[FAISS](https://github.com/facebookresearch/faiss)**: For efficient similarity search\n",
        "- **[ir_datasets](https://ir-datasets.com/)**: For accessing benchmark datasets\n",
        "- **[Mistral AI](https://mistral.ai/)**: For answer generation using LLMs\n",
        "- **[Hugging Face Hub](https://huggingface.co/docs/hub/index)**: For accessing pre-built indexes\n",
        "\n",
        "### üîë Mistral API Setup\n",
        "\n",
        "To use the Mistral AI API for answer generation:\n",
        "\n",
        "1. Create a Mistral AI account at [https://console.mistral.ai/](https://console.mistral.ai/)\n",
        "2. Generate an API key from your account dashboard\n",
        "3. Add your key to this notebook:\n",
        "   ```python\n",
        "   os.environ[\"MISTRAL_API_KEY\"] = \"YOUR_API_KEY_HERE\"  # Replace with your actual key\n",
        "   ```"
      ],
      "metadata": {
        "id": "F2UYQPOcRWIW"
      },
      "id": "F2UYQPOcRWIW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è RAG Architecture Overview\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant information from external knowledge sources before generating responses. This approach significantly improves accuracy and allows LLMs to access domain-specific information without retraining.\n",
        "\n",
        "![RAG Architecture Overview](https://drive.google.com/uc?export=view&id=1U28sDU5JAE-UfIfT9-BZ_4FEHktpuiah)\n",
        "\n",
        "### Core Components of RAG:\n",
        "\n",
        "1. **Document Processing Pipeline**\n",
        "   - **Collection**: Gathering domain-specific documents, articles, or knowledge bases\n",
        "   - **Chunking**: Breaking documents into manageable segments (paragraphs or sections)\n",
        "   - **Embedding**: Converting text chunks into dense vector representations\n",
        "\n",
        "2. **Retrieval System**\n",
        "   - **Query Processing**: Converting user questions into the same vector space\n",
        "   - **Vector Search**: Finding relevant document chunks using similarity measures\n",
        "   - **Reranking**: Further refining results using more sophisticated models\n",
        "\n",
        "3. **Generation System**\n",
        "   - **Context Assembly**: Combining retrieved information into a prompt\n",
        "   - **LLM Integration**: Sending the enriched prompt to an LLM\n",
        "   - **Response Generation**: Creating coherent, accurate, and contextual answers\n",
        "\n",
        "### Why Domain-Specific RAG Matters\n",
        "\n",
        "Different domains require specialized knowledge and specific response formats:\n",
        "- Medical advice needs evidence-based information and appropriate caveats\n",
        "- Technical support requires precise, actionable instructions\n",
        "- Educational content should be structured for different learning levels\n",
        "\n",
        "In this tutorial, we'll customize each component of the RAG pipeline for specific domains."
      ],
      "metadata": {
        "id": "6nTXBSgXLBCZ"
      },
      "id": "6nTXBSgXLBCZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß© Domain-Specific RAG Architecture\n",
        "\n",
        "Standard RAG pipelines can be enhanced by domain-specific customizations at each stage:\n",
        "\n",
        "### Domain Customization Points:\n",
        "\n",
        "1. **Data Sources & Preprocessing**\n",
        "   - Scientific: Research papers, clinical trials, structured abstracts\n",
        "   - Technical: Documentation, forums, code repositories\n",
        "   - Educational: Textbooks, lecture notes, graded materials\n",
        "\n",
        "2. **Embedding & Retrieval**\n",
        "   - Domain-specific models (e.g., BioBERT for medical, CodeBERT for programming)\n",
        "   - Customized chunking strategies (e.g., by sections, by concepts)\n",
        "   - Specialized ranking functions (e.g., recency weighting for news)\n",
        "\n",
        "3. **Prompt Engineering**\n",
        "   - Domain-appropriate instructions, terminology, and formats\n",
        "   - Specialty-specific constraints and guardrails\n",
        "   - Role-specific personas (e.g., educator, researcher, consultant)\n",
        "\n",
        "4. **Evaluation Metrics**\n",
        "   - Scientific: Citation accuracy, evidence quality\n",
        "   - Technical: Step completeness, actionability\n",
        "   - Educational: Clarity at appropriate learning level\n",
        "\n",
        "In this tutorial, we'll focus on practical implementations of these customizations."
      ],
      "metadata": {
        "id": "hONTg1nTNf4s"
      },
      "id": "hONTg1nTNf4s"
    },
    {
      "cell_type": "markdown",
      "id": "CbFr6thfWeFt",
      "metadata": {
        "id": "CbFr6thfWeFt"
      },
      "source": [
        "## üìö Datasets\n",
        "\n",
        "This tutorial uses a diverse set of datasets from the [BEIR benchmark collection](https://github.com/beir-cellar/beir) (via `ir_datasets`) to demonstrate how Retrieval-Augmented Generation (RAG) can be applied across multiple real-world domains.\n",
        "\n",
        "These datasets represent realistic information needs from different user groups, such as researchers, developers, educators, and the general public. Pre-built indexes have been provided for each dataset to ensure the tutorial runs smoothly in a short time window (~30 minutes), avoiding the overhead of on-the-fly index construction.\n",
        "\n",
        "### üß™ Scientific Research Domain\n",
        "- `beir/trec-covid`: Focused on COVID-19 research questions using the CORD-19 corpus.\n",
        "- `beir/scifact`: Scientific claim verification, where the system must support or refute claims using scientific abstracts.\n",
        "- `beir/nfcorpus`: Non-factoid biomedical QA, based on user questions from the NLM‚Äôs PubMed Helpdesk.\n",
        "\n",
        "### üõ†Ô∏è Technical Support Domain\n",
        "- `beir/cqadupstack/android`: Community Q&A data from Stack Exchange on Android development.\n",
        "- `beir/cqadupstack/webmasters`: Web hosting and webmaster technical queries.\n",
        "- `beir/cqadupstack/unix`: Unix/Linux command-line and scripting support.\n",
        "\n",
        "### üéì Education & Library Domain\n",
        "- `beir/natural-questions`: Real user questions from Google Search and answers from Wikipedia.\n",
        "- `beir/hotpotqa`: Multi-hop QA dataset requiring reasoning over multiple Wikipedia documents.\n",
        "- `beir/nfcorpus`: Also used here for medically-themed educational queries.\n",
        "\n",
        "### üîç Fact Verification Domain\n",
        "- `beir/fever`: Fact-checking based on Wikipedia claims.\n",
        "- `beir/climate-fever`: Focused on climate change-related claims and evidence.\n",
        "- `beir/scifact`: Also shared here for scientific claim verification.\n",
        "\n",
        "### üè• Healthcare Information Domain\n",
        "- `beir/nfcorpus`: Used for health-related literature review and question answering.\n",
        "- `beir/trec-covid`: Reused here to address health policy and treatment questions during the pandemic.\n",
        "\n",
        "> ‚ÑπÔ∏è Each domain has one **default dataset** selected (the first in each list), but you can explore other datasets in that domain using the dropdown selector. All datasets are accessed through the `ir_datasets` library, and prebuilt indexes are provided to ensure quick experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rZX8hgtUOTYY",
      "metadata": {
        "id": "rZX8hgtUOTYY"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8KlzcZr0Nx56",
      "metadata": {
        "id": "8KlzcZr0Nx56"
      },
      "source": [
        "Install required packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4d95ec",
      "metadata": {
        "id": "ae4d95ec"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers transformers torch numpy faiss-cpu tqdm ir_datasets ir_measures pandas matplotlib ipywidgets huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8J113gylN4cn",
      "metadata": {
        "id": "8J113gylN4cn"
      },
      "source": [
        "Import required libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "073b8036",
      "metadata": {
        "id": "073b8036"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import ir_datasets\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import faiss\n",
        "import pickle\n",
        "import ir_measures\n",
        "from ir_measures import *\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32oGSSCiN-hf",
      "metadata": {
        "id": "32oGSSCiN-hf"
      },
      "source": [
        "Set up GPU/CPU device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3cd6eb5",
      "metadata": {
        "id": "c3cd6eb5"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ddc06tcby-BF",
      "metadata": {
        "id": "Ddc06tcby-BF"
      },
      "outputs": [],
      "source": [
        "HUB_REPO_ID = \"ShubhamC/rag-tutorial-prebuilt-indexes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mvcQlQOX-9x5",
      "metadata": {
        "id": "mvcQlQOX-9x5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Basic RAG Pipeline: A Minimal Example\n",
        "\n",
        "Before diving into domain customization, let's understand the basic RAG workflow with a minimal working example. This gives us a foundation to build upon.\n",
        "\n",
        "### The Four Essential Steps:\n",
        "\n",
        "1. Load a document collection\n",
        "2. Convert user query to a vector\n",
        "3. Retrieve relevant documents\n",
        "4. Generate an answer using retrieved information"
      ],
      "metadata": {
        "id": "5pfi1u53MHtB"
      },
      "id": "5pfi1u53MHtB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick demonstration of a minimal RAG pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# 1. Create a tiny document collection (normally this would be loaded from a database)\n",
        "mini_docs = [\n",
        "    \"RAG stands for Retrieval-Augmented Generation in AI systems.\",\n",
        "    \"Embedding models convert text into numerical vectors.\",\n",
        "    \"FAISS is a library for efficient similarity search in vector spaces.\",\n",
        "    \"Language models generate text based on provided prompts.\"\n",
        "]\n",
        "\n",
        "print(\"1. Loaded document collection with\", len(mini_docs), \"documents\\n\")\n",
        "\n",
        "# 2. Create embeddings for documents\n",
        "mini_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "doc_embeddings = mini_model.encode(mini_docs)\n",
        "print(\"2. Created document embeddings with shape:\", doc_embeddings.shape, \"\\n\")\n",
        "\n",
        "# Create a simple vector index\n",
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_embeddings)\n",
        "print(\"3. Built search index with\", index.ntotal, \"vectors\\n\")\n",
        "\n",
        "# 3. Process a query and retrieve\n",
        "query = \"What is RAG in AI?\"\n",
        "query_vector = mini_model.encode([query])\n",
        "D, I = index.search(query_vector, k=2)  # Get top 2 results\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nRetrieved documents:\")\n",
        "for i, doc_idx in enumerate(I[0]):\n",
        "    print(f\"[{i+1}] {mini_docs[doc_idx]} (distance: {D[0][i]:.2f})\")\n",
        "\n",
        "# 4. Generate answer (simplified simulation without actual LLM API call)\n",
        "print(\"\\n4. Generated Answer:\")\n",
        "print(f\"RAG (Retrieval-Augmented Generation) is an AI technique that enhances language models by retrieving relevant information before generating responses. It combines the knowledge retrieval capabilities of search systems with the text generation abilities of large language models.\")"
      ],
      "metadata": {
        "id": "W64sBt1qMPFS"
      },
      "id": "W64sBt1qMPFS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "-sQtRBD5OGMJ",
      "metadata": {
        "id": "-sQtRBD5OGMJ"
      },
      "source": [
        "## About Pre-built Indexes\n",
        "\n",
        "For this tutorial, we're using pre-built indexes to save time. The indexes were created in advance\n",
        "using datasets from various sources:\n",
        "\n",
        "- BEIR (Benchmarking IR): Contains various IR tasks including scientific literature, news articles, etc.\n",
        "- Stack Exchange collections: Technical Q&A across domains\n",
        "- CORD-19: COVID-19 related research papers\n",
        "- NQ (Natural Questions): General knowledge Q&A\n",
        "\n",
        "The pre-built indexes include:\n",
        "1. Document corpus with text and titles\n",
        "2. Document embeddings using sentence-transformers\n",
        "3. FAISS index for efficient retrieval\n",
        "4. Sample queries with relevance judgments\n",
        "\n",
        "In a typical RAG pipeline, you would need to:\n",
        "1. Download and process a dataset\n",
        "2. Create document embeddings\n",
        "3. Build a search index\n",
        "4. Perform retrieval\n",
        "5. Generate answers with LLM\n",
        "\n",
        "For this tutorial, steps 1-3 are already done for you with the pre-built indexes.\n",
        "We'll focus on steps 4-5: retrieval and generation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6efFjY3tW1CE",
      "metadata": {
        "id": "6efFjY3tW1CE"
      },
      "source": [
        "\n",
        "## ‚öôÔ∏è How the Pre-built Indexes Were Created\n",
        "\n",
        "To ensure smooth performance during this tutorial, we‚Äôve prepared **pre-built retrieval indexes** for all datasets in advance. This saves time and avoids requiring participants to download large corpora or compute document embeddings live.\n",
        "\n",
        "We used the script below to generate these indexes using the `ir_datasets`, `sentence-transformers`, and `faiss` libraries.\n",
        "\n",
        "### üõ†Ô∏è Index Construction Pipeline\n",
        "For each dataset, we followed this process:\n",
        "\n",
        "1. **Load the dataset** using `ir_datasets`, including documents, queries, and relevance judgments (if available).\n",
        "2. **Preprocess each document** by combining its title and text (if both are available).\n",
        "3. **Generate dense embeddings** using the [`msmarco-distilbert-base-v3`](https://huggingface.co/sentence-transformers/msmarco-distilbert-base-v3) SentenceTransformer model.\n",
        "4. **Normalize embeddings** and index them using **FAISS** with an inner product search (cosine similarity on normalized vectors).\n",
        "5. **Store metadata**, including:\n",
        "   - A pickled dictionary of document texts and titles\n",
        "   - The FAISS index (`faiss_index.bin`)\n",
        "   - The NumPy matrix of document embeddings\n",
        "   - Sample queries and relevance judgments for evaluation\n",
        "\n",
        "This was implemented via a Python script (`create_prebuilt_indexes.py`) using the following function:\n",
        "\n",
        "```python\n",
        "create_prebuilt_index(dataset_name=\"beir/trec-covid\",\n",
        "                      output_dir=\"prebuilt_indexes/beir_trec-covid\",\n",
        "                      model_name=\"sentence-transformers/msmarco-distilbert-base-v3\")\n",
        "```\n",
        "\n",
        "You can modify and rerun this script to generate new indexes using your own datasets or preferred embedding models.\n",
        "\n",
        "### üìÅ Files in Each Index\n",
        "Each prebuilt index contains:\n",
        "- `corpus.pkl`: A dictionary mapping document IDs to `{text, title}`\n",
        "- `embeddings.npy`: Dense vectors for each document\n",
        "- `faiss_index.bin`: FAISS index for fast retrieval\n",
        "- `doc_ids.pkl`: Mapping of embedding rows to document IDs\n",
        "- `sample_queries.pkl`: Example queries for demonstration\n",
        "- `qrels.pkl`: Relevance judgments (if available)\n",
        "\n",
        "> ‚úÖ **Note:** In this tutorial, we only load these prebuilt files and use them directly, skipping embedding and indexing steps to keep things interactive and lightweight.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eapf2b5QOi8f",
      "metadata": {
        "id": "Eapf2b5QOi8f"
      },
      "source": [
        "## üéØ Use Cases in This Tutorial\n",
        "\n",
        "This tutorial explores how Retrieval-Augmented Generation (RAG) can be customized for different real-world domains. We've defined five practical **use cases**, each mapped to datasets available in the [`ir_datasets`](https://ir-datasets.com/) library and paired with domain-specific example queries and prompts.\n",
        "\n",
        "Each use case provides:\n",
        "- A curated set of relevant datasets\n",
        "- Example queries based on real-world information needs\n",
        "- A system prompt that guides the language model‚Äôs tone and behavior\n",
        "\n",
        "These use cases demonstrate how RAG can go beyond generic QA to serve **domain-specific goals** like academic research, troubleshooting, or fact verification.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ 1. Scientific Research\n",
        "Designed for tasks like literature review, scientific explanation, and research comprehension.\n",
        "\n",
        "- **Default Dataset:** `beir/trec-covid`\n",
        "- **Other Datasets:** `beir/scifact`, `beir/nfcorpus`\n",
        "- **Example Queries:**\n",
        "  - What are the most effective treatments for severe COVID-19?\n",
        "  - How does mRNA vaccine technology work?\n",
        "- **Prompt:** _You are a scientific research assistant. Provide accurate, evidence-based answers with appropriate scientific context and caveats._\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è 2. Technical Support\n",
        "Targets IT and developer support tasks using Stack Exchange-style technical Q&A data.\n",
        "\n",
        "- **Default Dataset:** `beir/cqadupstack/android`\n",
        "- **Other Datasets:** `beir/cqadupstack/webmasters`, `beir/cqadupstack/unix`\n",
        "- **Example Queries:**\n",
        "  - How do I fix network connectivity issues with Android devices?\n",
        "  - What's causing my app to crash on startup?\n",
        "- **Prompt:** _You are a technical support specialist. Provide clear, step-by-step solutions to technical problems with practical troubleshooting advice._\n",
        "\n",
        "---\n",
        "\n",
        "### üéì 3. Education & Library\n",
        "Supports research help, study guides, and educational content generation.\n",
        "\n",
        "- **Default Dataset:** `beir/natural-questions`\n",
        "- **Other Datasets:** `beir/hotpotqa`, `beir/nfcorpus`\n",
        "- **Example Queries:**\n",
        "  - What teaching methods are most effective for student engagement?\n",
        "  - How did World War II impact post-war economic development?\n",
        "- **Prompt:** _You are an educational assistant. Provide informative, well-structured answers suitable for learners, with clear explanations of complex concepts._\n",
        "\n",
        "---\n",
        "\n",
        "### üîç 4. Fact Verification\n",
        "Focuses on verifying potentially controversial or widely debated claims using factual sources.\n",
        "\n",
        "- **Default Dataset:** `beir/fever`\n",
        "- **Other Datasets:** `beir/scifact`, `beir/climate-fever`\n",
        "- **Example Queries:**\n",
        "  - Do vaccines cause autism?\n",
        "  - Is 5G technology harmful to human health?\n",
        "- **Prompt:** _You are a fact-checking assistant. Provide balanced, evidence-based assessments of claims with references to sources where possible._\n",
        "\n",
        "---\n",
        "\n",
        "### üè• 5. Healthcare Information\n",
        "Geared toward health and medical information, patient education, and clinical understanding.\n",
        "\n",
        "- **Default Dataset:** `beir/nfcorpus`\n",
        "- **Other Datasets:** `beir/trec-covid`\n",
        "- **Example Queries:**\n",
        "  - What are the potential side effects of statin medications?\n",
        "  - How effective is cognitive behavioral therapy for anxiety?\n",
        "- **Prompt:** _You are a healthcare information assistant. Provide evidence-based answers about medical topics, while noting that this is not medical advice. Focus on established research and clinical guidelines._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EzTwRQt3OnOI",
      "metadata": {
        "id": "EzTwRQt3OnOI"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7i7NqCVDXl7W",
      "metadata": {
        "id": "7i7NqCVDXl7W"
      },
      "source": [
        "Select RAG Use Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97aaa635",
      "metadata": {
        "id": "97aaa635"
      },
      "outputs": [],
      "source": [
        "# Dictionary of use cases with their descriptions, datasets, and example queries\n",
        "# Focus on datasets publicly available in ir_datasets but using prebuilt indexes\n",
        "use_cases = {\n",
        "    \"Scientific Research\": {\n",
        "        \"description\": \"Support for scientific literature review, fact verification, and research paper comprehension\",\n",
        "        \"datasets\": [\"beir/trec-covid\", \"beir/scifact\", \"beir/nfcorpus\"],\n",
        "        \"default_dataset\": \"beir/trec-covid\",\n",
        "        \"example_queries\": [\n",
        "            \"What are the most effective treatments for severe COVID-19?\",\n",
        "            \"How does mRNA vaccine technology work?\",\n",
        "            \"What evidence supports aerosol transmission of respiratory viruses?\",\n",
        "            \"What is the relationship between diet and cancer prevention?\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are a scientific research assistant. Provide a clear, accurate, and evidence-based answer to the user's question using only the retrieved documents. Cite supporting information from the context explicitly, and include appropriate scientific context, limitations, and caveats where applicable. Do not speculate beyond the provided material.\"\n",
        "\n",
        "    },\n",
        "    \"Technical Support\": {\n",
        "        \"description\": \"IT helpdesk, programming assistance, and technical knowledge base\",\n",
        "        \"datasets\": [\"beir/cqadupstack/android\", \"beir/cqadupstack/webmasters\", \"beir/cqadupstack/unix\"],\n",
        "        \"default_dataset\": \"beir/cqadupstack/android\",\n",
        "        \"example_queries\": [\n",
        "            \"How do I fix network connectivity issues with Android devices?\",\n",
        "            \"What's causing my app to crash on startup?\",\n",
        "            \"How to implement pagination in a mobile application?\",\n",
        "            \"Best practices for securing an Android device\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are a technical support specialist. Based on the retrieved documents, provide a concise and practical solution to the user's problem. Structure your answer step-by-step, cite relevant technical details, and ensure instructions are clear and executable. Avoid speculation or unsupported suggestions.\"\n",
        "\n",
        "    },\n",
        "    \"Education & Library\": {\n",
        "        \"description\": \"Enhanced research assistance, study materials, and educational content\",\n",
        "        \"datasets\": [\"beir/nfcorpus\", \"beir/natural-questions\", \"beir/hotpotqa\"],\n",
        "        \"default_dataset\": \"beir/natural-questions\",\n",
        "        \"example_queries\": [\n",
        "            \"What teaching methods are most effective for student engagement?\",\n",
        "            \"How did World War II impact economic development in post-war Europe?\",\n",
        "            \"What are the key differences between behaviorist and constructivist learning theories?\",\n",
        "            \"How does cellular respiration relate to photosynthesis?\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are an educational assistant. Using the retrieved documents, provide a well-structured, accurate, and learner-friendly explanation of the topic. Break down complex concepts into simpler terms, include definitions or examples when needed, and ensure your tone is clear, supportive, and informative. Ground your answer entirely in the context provided.\"\n",
        "\n",
        "    },\n",
        "    \"Fact Verification\": {\n",
        "        \"description\": \"Verifying claims and statements across various sources\",\n",
        "        \"datasets\": [\"beir/fever\", \"beir/scifact\", \"beir/climate-fever\"],\n",
        "        \"default_dataset\": \"beir/fever\",\n",
        "        \"example_queries\": [\n",
        "            \"Is climate change primarily caused by human activities?\",\n",
        "            \"Do vaccines cause autism?\",\n",
        "            \"Does vitamin C prevent the common cold?\",\n",
        "            \"Is 5G technology harmful to human health?\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are a fact-checking assistant. Using only the retrieved documents, assess the accuracy of the user's claim. Provide a balanced, evidence-based analysis with references to specific statements or sources from the documents. If the evidence is inconclusive, clearly state the uncertainty and avoid speculation.\"\n",
        "\n",
        "    },\n",
        "    \"Healthcare Information\": {\n",
        "        \"description\": \"Medical literature review, patient education, and clinical guidelines\",\n",
        "        \"datasets\": [\"beir/nfcorpus\", \"beir/trec-covid\"],\n",
        "        \"default_dataset\": \"beir/nfcorpus\",\n",
        "        \"example_queries\": [\n",
        "            \"What is the relationship between diet and heart disease?\",\n",
        "            \"How effective is cognitive behavioral therapy for anxiety disorders?\",\n",
        "            \"What are the potential side effects of statin medications?\",\n",
        "            \"How does family history affect cancer risk assessment?\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are a healthcare information assistant. Based on the retrieved documents, provide an accurate, evidence-based summary addressing the user's question. Do not offer medical advice. Instead, focus on established research findings, clinical guidelines, and clearly state any risks, limitations, or uncertainties in the available evidence. Ground your answer entirely in the context provided.\"\n",
        "    },\n",
        "    \"Campus Info\": {\n",
        "        \"description\": \"Ask questions about services, offices, and procedures listed on the S&T website.\",\n",
        "        \"datasets\": [\"custom_mst_site\"],\n",
        "        \"default_dataset\": \"custom_mst_site\",\n",
        "        \"example_queries\": [\n",
        "            \"Where is the ISSS office located?\",\n",
        "            \"What services are offered to sponsored international students?\",\n",
        "            \"How can I contact the Graduate Studies department?\",\n",
        "            \"Where can I find the event calendar?\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are a helpful campus assistant for Missouri S&T. Using only the information available in the retrieved content, answer the user's question clearly and accurately. Reference the exact page or section when possible. Be concise, avoid speculation, and maintain a friendly and professional tone.\"\n",
        "\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lVPeDpm-0l8A",
      "metadata": {
        "id": "lVPeDpm-0l8A"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create dropdowns\n",
        "use_case_dropdown = widgets.Dropdown(\n",
        "    options=list(use_cases.keys()),\n",
        "    description='Use Case:',\n",
        "    value=\"Scientific Research\"\n",
        ")\n",
        "\n",
        "dataset_dropdown = widgets.Dropdown(\n",
        "    options=use_cases[\"Scientific Research\"][\"datasets\"],\n",
        "    description='Dataset:'\n",
        ")\n",
        "\n",
        "# Step 2: Update dataset list when use case changes\n",
        "def update_datasets(change):\n",
        "    new_use_case = change['new']\n",
        "    new_datasets = use_cases[new_use_case][\"datasets\"]\n",
        "    dataset_dropdown.options = new_datasets\n",
        "    dataset_dropdown.value = use_cases[new_use_case][\"default_dataset\"]\n",
        "\n",
        "use_case_dropdown.observe(update_datasets, names='value')\n",
        "\n",
        "# Step 3: Display both dropdowns\n",
        "display(use_case_dropdown, dataset_dropdown)\n",
        "\n",
        "# Step 4: Function to confirm selections and assign variables\n",
        "def confirm_selection(_):\n",
        "    global use_case, selected_dataset\n",
        "    use_case = use_case_dropdown.value\n",
        "    selected_dataset = dataset_dropdown.value\n",
        "\n",
        "    clear_output()\n",
        "    print(f\"‚úÖ Use Case Selected: {use_case}\")\n",
        "    print(f\"‚úÖ Dataset Selected: {selected_dataset}\")\n",
        "    print(f\"\\nDescription:\\n{use_cases[use_case]['description']}\")\n",
        "    print(\"\\nExample queries:\")\n",
        "    for query in use_cases[use_case]['example_queries']:\n",
        "        print(f\"- {query}\")\n",
        "\n",
        "# Step 6: Confirm button\n",
        "confirm_button = widgets.Button(description=\"Confirm Selection\", button_style='success')\n",
        "confirm_button.on_click(confirm_selection)\n",
        "display(confirm_button)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z3DPNnrYOrlC",
      "metadata": {
        "id": "Z3DPNnrYOrlC"
      },
      "source": [
        "Define the paths where pre-built index files should be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6bddba8",
      "metadata": {
        "id": "b6bddba8"
      },
      "outputs": [],
      "source": [
        "base_path = f\"prebuilt_indexes/{selected_dataset.replace('/', '_')}\"\n",
        "corpus_path = f\"{base_path}/corpus.pkl\"\n",
        "embeddings_path = f\"{base_path}/embeddings.npy\"\n",
        "faiss_index_path = f\"{base_path}/faiss_index.bin\"\n",
        "doc_ids_path = f\"{base_path}/doc_ids.pkl\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JETEj6-zOtkV",
      "metadata": {
        "id": "JETEj6-zOtkV"
      },
      "source": [
        "Create the directory if it doesn't exist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "632842f7",
      "metadata": {
        "id": "632842f7"
      },
      "outputs": [],
      "source": [
        "os.makedirs(base_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4DP_hlZhOxDV",
      "metadata": {
        "id": "4DP_hlZhOxDV"
      },
      "source": [
        "Define the download URLs for each dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98_HGPjvO0mM",
      "metadata": {
        "id": "98_HGPjvO0mM"
      },
      "source": [
        "Function to generate sample corpus based on the selected dataset and use case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19931975",
      "metadata": {
        "id": "19931975"
      },
      "outputs": [],
      "source": [
        "def generate_sample_corpus(dataset_name, current_use_case):\n",
        "    \"\"\"Generate a sample corpus with domain-specific content based on the dataset name.\"\"\"\n",
        "    corpus = {}\n",
        "\n",
        "    if current_use_case == \"Scientific Research\":\n",
        "        # Scientific research content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            if \"covid\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"COVID-19 Research Paper {i}\",\n",
        "                    \"text\": f\"This paper investigates the effects of COVID-19 on respiratory health. We conducted a study with {100+i} patients and found significant correlations between viral load and symptom severity. The study suggests that early intervention with antiviral medications may reduce hospitalization rates.\"\n",
        "                }\n",
        "            elif \"scifact\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Scientific Study on Topic {i}\",\n",
        "                    \"text\": f\"Our research demonstrates that hypothesis {i} is supported by experimental evidence. The data shows a statistically significant effect (p<0.05) across multiple trials. These findings contradict previous assumptions and suggest a new mechanism for this phenomenon.\"\n",
        "                }\n",
        "            else:\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Scientific Paper {i}\",\n",
        "                    \"text\": f\"This research examines the relationship between diet and health outcomes. Analysis of data from {500+i} participants shows significant associations between consumption of processed foods and increased risk of chronic diseases. The findings highlight the importance of dietary interventions in public health strategies.\"\n",
        "                }\n",
        "\n",
        "    elif current_use_case == \"Technical Support\":\n",
        "        # Technical support content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            if \"android\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Android Technical Issue {i}\",\n",
        "                    \"text\": f\"Users experiencing battery drain on Android devices should check for apps running in the background. The issue often occurs after system updates or when location services are constantly active. To fix this, go to Settings > Battery > Battery Usage and identify power-hungry applications. Restricting background activity for these apps can significantly improve battery life.\"\n",
        "                }\n",
        "            elif \"webmasters\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Web Development Problem {i}\",\n",
        "                    \"text\": f\"When implementing responsive designs, developers often encounter issues with viewport rendering on mobile devices. To address this, ensure your HTML includes the proper meta viewport tag. Also check that media queries are correctly implemented to handle different screen sizes. Testing across multiple devices is essential to verify responsive behavior.\"\n",
        "                }\n",
        "            else:\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Technical Solution {i}\",\n",
        "                    \"text\": f\"A common error when setting up network connections is incorrect DNS configuration. To troubleshoot, first verify that the DNS server addresses are correct. Then flush the DNS cache to ensure old records aren't causing conflicts. If problems persist, try using alternative DNS servers to determine if the issue is with your ISP's DNS resolution.\"\n",
        "                }\n",
        "\n",
        "    elif current_use_case == \"Education & Library\":\n",
        "        # Educational content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            if \"questions\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Educational Topic {i}\",\n",
        "                    \"text\": f\"This article explains the fundamental concepts of learning theories. Constructivism emphasizes how learners actively build knowledge through experience and reflection, while behaviorism focuses on observable behaviors and environmental conditioning. Understanding these frameworks helps educators design more effective teaching strategies that accommodate different learning styles and cognitive processes.\"\n",
        "                }\n",
        "            elif \"hotpot\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Historical Event {i}\",\n",
        "                    \"text\": f\"The Industrial Revolution transformed economic systems through mechanization and factory production. Beginning in Britain in the late 18th century, it spread throughout Europe and North America, fundamentally changing social structures and labor practices. Technological innovations like the steam engine drove unprecedented growth while creating new challenges related to urbanization, working conditions, and economic inequality.\"\n",
        "                }\n",
        "            else:\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Educational Resource {i}\",\n",
        "                    \"text\": f\"This educational material covers key scientific concepts for secondary education. Topics include cellular biology, chemical reactions, and physical laws. The content is structured to build conceptual understanding through progressive learning objectives, providing examples and applications relevant to students' daily experiences.\"\n",
        "                }\n",
        "\n",
        "    elif current_use_case == \"Fact Verification\":\n",
        "        # Fact verification content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            if \"fever\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Fact Check Article {i}\",\n",
        "                    \"text\": f\"Climate scientists have reached overwhelming consensus that human activities are the primary driver of observed climate change since the mid-20th century. Multiple independent lines of evidence support this conclusion, including atmospheric CO2 measurements, temperature records, and climate model projections. Natural factors alone cannot explain the rapid warming observed in recent decades.\"\n",
        "                }\n",
        "            elif \"scifact\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Scientific Claim Assessment {i}\",\n",
        "                    \"text\": f\"Research has consistently failed to find evidence supporting a causal link between vaccines and autism. Multiple large-scale epidemiological studies involving millions of children have found no association between vaccination and autism spectrum disorders. The original study suggesting this link was retracted due to methodological flaws and ethical violations.\"\n",
        "                }\n",
        "            else:\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Fact Verification {i}\",\n",
        "                    \"text\": f\"Analysis of the claim that 5G technology poses health risks finds insufficient scientific evidence to support this assertion. While all radiation sources deserve study, 5G operates using non-ionizing radiation that lacks sufficient energy to damage cellular DNA. Current research indicates that exposure levels from 5G infrastructure fall well below international safety guidelines.\"\n",
        "                }\n",
        "\n",
        "    elif current_use_case == \"Healthcare Information\":\n",
        "        # Healthcare content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            if \"nfcorpus\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Nutrition Research Study {i}\",\n",
        "                    \"text\": f\"This review examines the relationship between dietary patterns and cardiovascular health. Evidence consistently shows that diets rich in fruits, vegetables, whole grains, and lean proteins are associated with reduced risk of heart disease. Conversely, high consumption of processed foods, saturated fats, and added sugars correlates with increased cardiovascular risk factors including hypertension and elevated cholesterol.\"\n",
        "                }\n",
        "            elif \"covid\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"COVID-19 Treatment Review {i}\",\n",
        "                    \"text\": f\"Clinical trials evaluating antiviral medications for COVID-19 have shown varying degrees of efficacy. Early treatment with certain antivirals may reduce symptom duration and hospitalization risk in high-risk populations. However, treatment effectiveness depends on timing, viral variants, and patient characteristics. Comprehensive management approaches typically include supportive care alongside targeted therapies.\"\n",
        "                }\n",
        "            else:\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Medical Research {i}\",\n",
        "                    \"text\": f\"This study examines treatment efficacy for anxiety disorders, comparing cognitive behavioral therapy (CBT) with pharmacological interventions. Meta-analysis of clinical trials indicates that CBT produces outcomes comparable to medication for many patients, with potentially more durable effects after treatment discontinuation. Combined approaches often yield superior results, particularly for severe or treatment-resistant cases.\"\n",
        "                }\n",
        "    elif current_use_case == \"Campus Info\":\n",
        "        # Campus information content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            corpus[doc_id] = {\n",
        "                \"title\": f\"Missouri S&T Page {i}\",\n",
        "                \"text\": f\"This page contains information about Missouri S&T services and offices. The International Student and Scholar Services (ISSS) office provides support for sponsored international students including visa assistance, cultural activities, and academic guidance. Students can contact the Graduate Studies department for information about graduate programs, thesis requirements, and funding opportunities.\"\n",
        "            }\n",
        "\n",
        "    else:\n",
        "        # Generic content for any other use case\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            corpus[doc_id] = {\n",
        "                \"title\": f\"Document {i}\",\n",
        "                \"text\": f\"This is sample text for document {i}. It contains information relevant to various queries in this domain. The content includes key facts, explanations, and examples that might be useful for answering questions on this topic.\"\n",
        "            }\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rOIeg38vO5eS",
      "metadata": {
        "id": "rOIeg38vO5eS"
      },
      "source": [
        "Download the files if they don't exist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a472f867",
      "metadata": {
        "id": "a472f867"
      },
      "outputs": [],
      "source": [
        "# --- Path Setup ---\n",
        "repo_folder_name = selected_dataset.replace('/', '_')\n",
        "base_path = f\"prebuilt_indexes/{repo_folder_name}\"\n",
        "os.makedirs(base_path, exist_ok=True) # Create directory if it doesn't exist\n",
        "\n",
        "# List of files expected for an index (Keep this)\n",
        "files_to_download = [\"corpus.pkl\", \"embeddings.npy\", \"faiss_index.bin\", \"doc_ids.pkl\"] # Add qrels.pkl etc. if needed\n",
        "\n",
        "print(f\"Checking/downloading pre-built indexes for {selected_dataset} from HF Hub: {HUB_REPO_ID}...\")\n",
        "\n",
        "# --- Download Loop (Using HF Hub) ---\n",
        "all_files_exist = True\n",
        "for file_name in files_to_download:\n",
        "    local_file_path = os.path.join(base_path, file_name)\n",
        "    if not os.path.exists(local_file_path):\n",
        "        all_files_exist = False # Mark that at least one file needs downloading\n",
        "        print(f\"Downloading {file_name}...\")\n",
        "        try:\n",
        "            # Construct the path *within* the Hub repository\n",
        "            # Assumes you uploaded into folders named like 'beir_trec-covid' etc.\n",
        "            path_in_repo = f\"{repo_folder_name}/{file_name}\"\n",
        "\n",
        "            # Use hf_hub_download\n",
        "            downloaded_path = hf_hub_download(\n",
        "                repo_id=HUB_REPO_ID,\n",
        "                filename=path_in_repo,\n",
        "                repo_type=\"dataset\", # Specify it's a dataset repo\n",
        "                local_dir=base_path, # Download directly into the target folder\n",
        "                local_dir_use_symlinks=False # Avoids potential symlink issues\n",
        "            )\n",
        "            # Double-check file exists at the expected final path\n",
        "            if not os.path.exists(local_file_path):\n",
        "                 if os.path.exists(downloaded_path) and downloaded_path != local_file_path:\n",
        "                     # Move if hf_hub_download placed it slightly differently (rare with local_dir)\n",
        "                     os.rename(downloaded_path, local_file_path)\n",
        "                     print(f\"Moved downloaded file to {local_file_path}\")\n",
        "                 else:\n",
        "                      raise FileNotFoundError(f\"Download failed or file not found at expected path {local_file_path} or download path {downloaded_path} for {file_name}\")\n",
        "\n",
        "            print(f\"Successfully downloaded {file_name} to {local_file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR downloading {file_name} from Hugging Face Hub: {e}\")\n",
        "            print(f\"Check connection and ensure the file exists at 'datasets/{HUB_REPO_ID}/tree/main/{path_in_repo}' on the Hub.\")\n",
        "            # --- Optional: Keep your fallback logic ---\n",
        "            print(f\"‚ö†Ô∏è Creating sample data for {file_name} since download failed\")\n",
        "            # Adapt this fallback logic based on your generate_sample_corpus function and other needs\n",
        "            if file_name == \"corpus.pkl\":\n",
        "                 try:\n",
        "                     # Ensure generate_sample_corpus and use_case are defined earlier\n",
        "                     sample_corpus = generate_sample_corpus(selected_dataset, use_case)\n",
        "                     with open(local_file_path, 'wb') as f: pickle.dump(sample_corpus, f)\n",
        "                 except NameError:\n",
        "                     print(\"generate_sample_corpus or use_case not defined. Cannot create sample data.\")\n",
        "                 except Exception as sample_e:\n",
        "                     print(f\"Error creating sample corpus: {sample_e}\")\n",
        "            elif file_name == \"doc_ids.pkl\":\n",
        "                 try:\n",
        "                     sample_doc_ids = [f\"doc{i}\" for i in range(20)]\n",
        "                     with open(local_file_path, 'wb') as f: pickle.dump(sample_doc_ids, f)\n",
        "                 except Exception as sample_e:\n",
        "                     print(f\"Error creating sample doc_ids: {sample_e}\")\n",
        "            elif file_name == \"embeddings.npy\":\n",
        "                 try:\n",
        "                     sample_embeddings = np.random.rand(20, 384).astype(np.float32)\n",
        "                     np.save(local_file_path, sample_embeddings)\n",
        "                 except Exception as sample_e:\n",
        "                     print(f\"Error creating sample embeddings: {sample_e}\")\n",
        "            elif file_name == \"faiss_index.bin\":\n",
        "                 try:\n",
        "                     sample_index = faiss.IndexFlatL2(384); sample_index.add(np.random.rand(20, 384).astype(np.float32))\n",
        "                     faiss.write_index(sample_index, local_file_path)\n",
        "                 except Exception as sample_e:\n",
        "                     print(f\"Error creating sample faiss_index: {sample_e}\")\n",
        "            # --- End Optional Fallback ---\n",
        "\n",
        "# Optional check message\n",
        "if all_files_exist:\n",
        "    print(\"All required index files already exist locally.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e58b58e5",
      "metadata": {
        "id": "e58b58e5"
      },
      "outputs": [],
      "source": [
        "print(f\"Loading pre-built indexes for {selected_dataset}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1vJ7Q4eRPPDX",
      "metadata": {
        "id": "1vJ7Q4eRPPDX"
      },
      "source": [
        "Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ce835e",
      "metadata": {
        "id": "79ce835e"
      },
      "outputs": [],
      "source": [
        "print(\"Loading document corpus...\")\n",
        "with open(corpus_path, 'rb') as f:\n",
        "    corpus = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jqm_3NxMPRiJ",
      "metadata": {
        "id": "Jqm_3NxMPRiJ"
      },
      "source": [
        "Load document IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1eaa94",
      "metadata": {
        "id": "ac1eaa94"
      },
      "outputs": [],
      "source": [
        "print(\"Loading document IDs...\")\n",
        "with open(doc_ids_path, 'rb') as f:\n",
        "    doc_ids = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qTz7sMV0PTdc",
      "metadata": {
        "id": "qTz7sMV0PTdc"
      },
      "source": [
        "Load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85d00b9a",
      "metadata": {
        "id": "85d00b9a"
      },
      "outputs": [],
      "source": [
        "print(\"Loading document embeddings...\")\n",
        "doc_embeddings = np.load(embeddings_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XTjUnCxgPV63",
      "metadata": {
        "id": "XTjUnCxgPV63"
      },
      "source": [
        "Load FAISS index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179727a3",
      "metadata": {
        "id": "179727a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Loading FAISS index...\")\n",
        "index = faiss.read_index(faiss_index_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07900094",
      "metadata": {
        "id": "07900094"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(f\"Successfully loaded pre-built indexes for {selected_dataset}\")\n",
        "    print(f\"Corpus size: {len(corpus)} documents\")\n",
        "    print(f\"Embeddings shape: {doc_embeddings.shape}\")\n",
        "    print(f\"FAISS index size: {index.ntotal} vectors\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error building indexes: {e}\")\n",
        "    print(f\"‚ö†Ô∏è Creating sample data for demonstration\")\n",
        "\n",
        "    # Create sample corpus\n",
        "    corpus = {}\n",
        "    for i in range(100):\n",
        "        doc_id = f\"doc{i}\"\n",
        "        corpus[doc_id] = {\n",
        "            'title': f\"Sample Document {i}\",\n",
        "            'text': f\"This is sample text for document {i} related to {use_case}. It contains information relevant to various queries in this domain. The content includes key facts, explanations, and examples that might be useful for answering questions on this topic.\"\n",
        "        }\n",
        "\n",
        "    doc_ids = list(corpus.keys())\n",
        "\n",
        "    # Create sample embeddings\n",
        "    doc_embeddings = np.random.rand(len(doc_ids), 384).astype(np.float32)\n",
        "    faiss.normalize_L2(doc_embeddings)\n",
        "\n",
        "    # Create FAISS index\n",
        "    index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
        "    index.add(doc_embeddings)\n",
        "\n",
        "    # Save sample data\n",
        "    with open(corpus_path, 'wb') as f:\n",
        "        pickle.dump(corpus, f)\n",
        "\n",
        "    with open(doc_ids_path, 'wb') as f:\n",
        "        pickle.dump(doc_ids, f)\n",
        "\n",
        "    np.save(embeddings_path, doc_embeddings)\n",
        "    faiss.write_index(index, faiss_index_path)\n",
        "\n",
        "    print(f\"‚úÖ Created sample data with {len(corpus)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Upz5gD_gP0Jm",
      "metadata": {
        "id": "Upz5gD_gP0Jm"
      },
      "source": [
        "Load queries and qrels from ir_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c55c178",
      "metadata": {
        "id": "3c55c178"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def load_custom_topics_json(json_path):\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    topics_list = data[\"topics\"] if \"topics\" in data else data\n",
        "    queries = {}\n",
        "    for topic in topics_list:\n",
        "        topic_id = str(topic.get(\"id\") or topic.get(\"number\"))\n",
        "        queries[topic_id] = {\n",
        "            \"id\": topic_id,\n",
        "            \"text\": topic.get(\"title\", \"\").strip(),\n",
        "            \"description\": topic.get(\"description\", \"\").strip(),\n",
        "            \"narrative\": topic.get(\"narrative\", \"\").strip()\n",
        "        }\n",
        "    print(f\"‚úÖ Loaded {len(queries)} topics from JSON\")\n",
        "    return queries\n",
        "\n",
        "def load_custom_qrels_txt(qrels_path):\n",
        "    qrels_dict = {}\n",
        "    with open(qrels_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 4:\n",
        "                topic_id, _, doc_id, relevance = parts\n",
        "                if topic_id not in qrels_dict:\n",
        "                    qrels_dict[topic_id] = {}\n",
        "                qrels_dict[topic_id][doc_id] = int(relevance)\n",
        "    print(f\"‚úÖ Loaded qrels for {len(qrels_dict)} topics\")\n",
        "    return qrels_dict\n",
        "\n",
        "def select_sample_queries(queries, qrels_dict, max_queries=3):\n",
        "    sample = {}\n",
        "    for topic_id, query_info in queries.items():\n",
        "        if topic_id in qrels_dict:\n",
        "            sample[topic_id] = query_info\n",
        "        if len(sample) >= max_queries:\n",
        "            break\n",
        "    return sample\n",
        "\n",
        "# === Drop-in replacement begins here ===\n",
        "\n",
        "try:\n",
        "    if selected_dataset == \"custom_mst_site\":\n",
        "        print(\"üì¶ Loading queries and qrels for custom MST dataset...\")\n",
        "\n",
        "        base_path = \"prebuilt_indexes/custom_mst_site\"\n",
        "        topics_path = os.path.join(base_path, \"topics.json\")\n",
        "        qrels_path = os.path.join(base_path, \"auto_qrels.txt\")\n",
        "\n",
        "        queries = load_custom_topics_json(topics_path)\n",
        "        qrels_dict = load_custom_qrels_txt(qrels_path)\n",
        "        sample_queries = select_sample_queries(queries, qrels_dict)\n",
        "\n",
        "        print(f\"‚úÖ Selected {len(sample_queries)} sample queries for demonstration\")\n",
        "\n",
        "    else:\n",
        "        # Fall back to ir_datasets version\n",
        "        import ir_datasets\n",
        "        print(\"üì• Loading queries and relevance judgments from ir_datasets...\")\n",
        "        dataset = ir_datasets.load(selected_dataset)\n",
        "\n",
        "        queries = {}\n",
        "        qrels_dict = {}\n",
        "\n",
        "        try:\n",
        "            next(dataset.queries_iter())\n",
        "            for query in dataset.queries_iter():\n",
        "                query_id = query.query_id if hasattr(query, 'query_id') else getattr(query, '_id', f'q{len(queries)}')\n",
        "                if hasattr(query, 'text'):\n",
        "                    query_text = query.text\n",
        "                elif hasattr(query, 'title'):\n",
        "                    query_text = query.title\n",
        "                elif hasattr(query, 'query'):\n",
        "                    query_text = query.query\n",
        "                else:\n",
        "                    query_text = \"\"\n",
        "                    for field in dir(query):\n",
        "                        if not field.startswith('_') and isinstance(getattr(query, field), str) and field != 'query_id':\n",
        "                            query_text = getattr(query, field)\n",
        "                            break\n",
        "\n",
        "                queries[query_id] = {'text': query_text, 'id': query_id}\n",
        "                if len(queries) >= 100:\n",
        "                    break\n",
        "            print(f\"‚úÖ Loaded {len(queries)} queries from ir_datasets\")\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è No queries found in ir_datasets\")\n",
        "\n",
        "        try:\n",
        "            next(dataset.qrels_iter())\n",
        "            for qrel in dataset.qrels_iter():\n",
        "                if not hasattr(qrel, 'query_id') or not hasattr(qrel, 'doc_id') or not hasattr(qrel, 'relevance'):\n",
        "                    continue\n",
        "                if qrel.query_id not in qrels_dict:\n",
        "                    qrels_dict[qrel.query_id] = {}\n",
        "                qrels_dict[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
        "            print(f\"‚úÖ Loaded relevance judgments for {len(qrels_dict)} queries\")\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è No relevance judgments found in ir_datasets\")\n",
        "\n",
        "        sample_queries = {}\n",
        "        if queries and qrels_dict:\n",
        "            for query_id, query_info in queries.items():\n",
        "                if query_id in qrels_dict and len(sample_queries) < 3:\n",
        "                    sample_queries[query_id] = query_info\n",
        "        if not sample_queries and queries:\n",
        "            for i, (query_id, query_info) in enumerate(queries.items()):\n",
        "                if i < 3:\n",
        "                    sample_queries[query_id] = query_info\n",
        "\n",
        "        print(f\"‚úÖ Selected {len(sample_queries)} sample queries for demonstration\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error loading queries and qrels:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28VwLFi1P3yw",
      "metadata": {
        "id": "28VwLFi1P3yw"
      },
      "source": [
        "## Initialize retrieval models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rb2aGb0sP7W7",
      "metadata": {
        "id": "Rb2aGb0sP7W7"
      },
      "source": [
        "Load bi-encoder model for query encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "373ef42b",
      "metadata": {
        "id": "373ef42b"
      },
      "outputs": [],
      "source": [
        "\n",
        "biencoder_model_name = \"sentence-transformers/msmarco-distilbert-base-v3\" # @param [\"sentence-transformers/msmarco-distilbert-base-v3\", \"sentence-transformers/all-mpnet-base-v2\", \"sentence-transformers/all-MiniLM-L6-v2\"]\n",
        "bi_encoder = SentenceTransformer(biencoder_model_name)\n",
        "bi_encoder.to(device)\n",
        "print(f\"Loaded bi-encoder model: {biencoder_model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FwUkrIc7P9Pv",
      "metadata": {
        "id": "FwUkrIc7P9Pv"
      },
      "source": [
        "Load cross-encoder model for reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda9dcce",
      "metadata": {
        "id": "dda9dcce"
      },
      "outputs": [],
      "source": [
        "\n",
        "crossencoder_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\" # @param [\"cross-encoder/ms-marco-MiniLM-L-6-v2\", \"cross-encoder/ms-marco-MiniLM-L-12-v2\", \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"]\n",
        "cross_encoder = CrossEncoder(crossencoder_model_name)\n",
        "cross_encoder.to(device)\n",
        "print(f\"Loaded cross-encoder model: {crossencoder_model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FIJv1VzZP_EG",
      "metadata": {
        "id": "FIJv1VzZP_EG"
      },
      "source": [
        "Load LLM for answer generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ec1d94",
      "metadata": {
        "id": "12ec1d94"
      },
      "outputs": [],
      "source": [
        "# Set your Mistral API key (preferably from environment variable)\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"abcdwfgh\"  # Replace with your actual key\n",
        "print(\"Mistral API configured and ready to use\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tsfqSQo2QE6l",
      "metadata": {
        "id": "tsfqSQo2QE6l"
      },
      "source": [
        "## üìè Domain-Specific Evaluation: Beyond Standard Metrics\n",
        "\n",
        "Understanding how to evaluate RAG systems properly is critical for real-world applications. Different domains require different evaluation approaches:\n",
        "\n",
        "### Scientific Research Evaluation\n",
        "- **Factual Accuracy**: Are all scientific claims supported by the retrieved documents?\n",
        "- **Citation Quality**: Does the system properly attribute information to sources?\n",
        "- **Uncertainty Handling**: Does the response appropriately express limitations and caveats?\n",
        "\n",
        "### Technical Support Evaluation\n",
        "- **Actionability**: Can a user follow the instructions without additional information?\n",
        "- **Correctness**: Do the steps actually resolve the technical issue?\n",
        "- **Safety**: Are proper warnings included for potentially harmful operations?\n",
        "\n",
        "### Educational Content Evaluation\n",
        "- **Comprehensibility**: Is the content appropriate for the intended education level?\n",
        "- **Scaffolding**: Does the explanation build concepts in a logical order?\n",
        "- **Engagement**: Does the content use appropriate examples and explanations?\n",
        "\n",
        "### Healthcare Information Evaluation\n",
        "- **Clinical Accuracy**: Is the medical information correct and up-to-date?\n",
        "- **Completeness**: Are important warnings, contraindications, or limitations mentioned?\n",
        "- **Clarity**: Is medical terminology appropriately explained?\n",
        "\n",
        "### Implementing Custom Evaluation\n",
        "In this tutorial, we use standard IR metrics (precision, recall, etc.) augmented with domain-specific considerations. In production systems, consider implementing:\n",
        "\n",
        "1. Human-in-the-loop evaluation pipelines\n",
        "2. Domain expert review for critical applications\n",
        "3. Automated checks for domain-specific requirements\n",
        "\n",
        "Run the next cell to see how we can implement custom evaluation metrics:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def evaluate_scientific_rag(query, retrieved_docs, generated_answer, ground_truth=None):\n",
        "    \"\"\"Evaluate a scientific RAG response with domain-specific metrics\"\"\"\n",
        "\n",
        "    # 1. Check for citations in the answer\n",
        "    citation_pattern = r'\\[(\\d+)\\]|\\(([A-Za-z\\s]+,\\s*\\d{4})\\)'\n",
        "    has_citations = bool(re.search(citation_pattern, generated_answer))\n",
        "\n",
        "    # 2. Check if answer mentions limitations or uncertainty\n",
        "    uncertainty_terms = ['may', 'might', 'could', 'possibly', 'suggests',\n",
        "                         'limited evidence', 'more research', 'not conclusive']\n",
        "    has_uncertainty = any(term in generated_answer.lower() for term in uncertainty_terms)\n",
        "\n",
        "    # 3. Check relevance of retrieved documents (simplified)\n",
        "    # In practice, this might use a trained classifier or human evaluation\n",
        "    relevance_score = sum(doc['cross_score'] for doc in retrieved_docs) / len(retrieved_docs)\n",
        "\n",
        "    # 4. Calculate a combined scientific quality score (example only)\n",
        "    scientific_quality = (\n",
        "        (2 if has_citations else 0) +\n",
        "        (1 if has_uncertainty else 0) +\n",
        "        min(2, relevance_score / 3)  # Scale to 0-2 range\n",
        "    ) / 5  # Normalize to 0-1\n",
        "\n",
        "    # Return evaluation results\n",
        "    return {\n",
        "        'has_citations': has_citations,\n",
        "        'acknowledges_uncertainty': has_uncertainty,\n",
        "        'avg_relevance_score': relevance_score,\n",
        "        'scientific_quality_score': scientific_quality,\n",
        "    }\n",
        "\n",
        "# Example usage with a mock result\n",
        "example_query = \"What is the effectiveness of remdesivir for COVID-19?\"\n",
        "example_docs = [{\"cross_score\": 7.5}, {\"cross_score\": 6.8}, {\"cross_score\": 6.2}]\n",
        "example_answer = \"Studies suggest remdesivir may improve recovery time in some COVID-19 patients, though more research is needed to confirm its effectiveness across different patient populations [1].\"\n",
        "\n",
        "scientific_eval = evaluate_scientific_rag(\n",
        "    example_query,\n",
        "    example_docs,\n",
        "    example_answer\n",
        ")\n",
        "\n",
        "print(\"Scientific Domain Evaluation:\")\n",
        "for metric, value in scientific_eval.items():\n",
        "    if isinstance(value, bool):\n",
        "        print(f\"- {metric}: {'‚úÖ Yes' if value else '‚ùå No'}\")\n",
        "    elif isinstance(value, float):\n",
        "        print(f\"- {metric}: {value:.2f}\")"
      ],
      "metadata": {
        "id": "K9V3w0dKOR4R"
      },
      "id": "K9V3w0dKOR4R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Domain-Specific Evaluation in Practice\n",
        "\n",
        "For each domain, you would develop appropriate evaluation metrics:\n",
        "\n",
        "1. **Scientific Domain**: Citation practices, uncertainty acknowledgment, evidence quality\n",
        "2. **Technical Support**: Step clarity, command accuracy, safety considerations\n",
        "3. **Educational Content**: Grade-level appropriateness, concept scaffolding, engagement\n",
        "4. **Healthcare Information**: Clinical accuracy, completeness of warnings, clarity\n",
        "\n",
        "These custom evaluations can be:\n",
        "- **Automated**: For measurable aspects like presence of citations\n",
        "- **Semi-automated**: Using classifiers trained on expert-labeled examples\n",
        "- **Human-in-the-loop**: Expert review for critical applications\n",
        "\n",
        "In production RAG systems, combining standard IR metrics with these domain-specific evaluations provides a more complete picture of system performance."
      ],
      "metadata": {
        "id": "fhqLBPYmO57M"
      },
      "id": "fhqLBPYmO57M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b69d73a8",
      "metadata": {
        "id": "b69d73a8"
      },
      "outputs": [],
      "source": [
        "def get_domain_metrics(domain):\n",
        "    \"\"\"Return appropriate evaluation metrics for the selected domain.\"\"\"\n",
        "    # Base metrics for all domains\n",
        "    base_metrics = [\n",
        "        nDCG@5, P@5, R@5, AP\n",
        "    ]\n",
        "\n",
        "    # Domain-specific additional metrics\n",
        "    domain_specific = {\n",
        "        \"Scientific Research\": [\n",
        "            nDCG@10,         # More comprehensive literature review (deeper results)\n",
        "            R@10,            # Higher recall important for research\n",
        "            RR               # First relevant result important for fact-checking\n",
        "        ],\n",
        "        \"Technical Support\": [\n",
        "            RR,              # First relevant result critical for troubleshooting\n",
        "            P@1, P@3,        # Precision at very top results important\n",
        "            Rprec            # Balances precision/recall\n",
        "        ],\n",
        "        \"Education & Library\": [\n",
        "            nDCG@10,         # Students often explore more results\n",
        "            RBP(p=0.8),      # Models patience of students browsing results\n",
        "            Judged@10        # Coverage of assessment for educational content\n",
        "        ],\n",
        "        \"Legal Document Analysis\": [\n",
        "            RBP(p=0.9),      # Very high precision focus (legal requires accuracy)\n",
        "            nDCG@20,         # May need to review many documents for legal research\n",
        "            infAP            # Handles incomplete judgments common in legal collections\n",
        "        ],\n",
        "        \"Healthcare Information\": [\n",
        "            P@1, P@3,        # Top results critical for medical information\n",
        "            RR,              # First relevant result important for clinical questions\n",
        "            ERR              # Models utility with diminishing returns\n",
        "        ],\n",
        "        \"Campus Info\": [\n",
        "            P@1, P@3,        # Precise information location important\n",
        "            RR,              # First relevant result critical\n",
        "            nDCG@5           # Overall relevance ranking\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return base_metrics + domain_specific.get(domain, [])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11yqWpgqQI2a",
      "metadata": {
        "id": "11yqWpgqQI2a"
      },
      "source": [
        "## Define RAG functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218ecdc0",
      "metadata": {
        "id": "218ecdc0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def retrieve_documents(query, top_k_first_stage=100, top_k_reranked=5):\n",
        "    \"\"\"\n",
        "    Two-stage retrieval:\n",
        "    1. Retrieve top_k_first_stage documents using pre-built FAISS index\n",
        "    2. Rerank with cross-encoder to get final top_k_reranked\n",
        "    \"\"\"\n",
        "    # 1. First-stage retrieval with biencoder + FAISS\n",
        "    # Encode the query using the bi-encoder model\n",
        "    query_embedding = bi_encoder.encode(query, show_progress_bar=False, convert_to_numpy=True)\n",
        "    query_embedding = np.array([query_embedding], dtype=np.float32)\n",
        "    faiss.normalize_L2(query_embedding)  # Normalize for cosine similarity\n",
        "\n",
        "    # Search the FAISS index\n",
        "    scores, indices = index.search(query_embedding, k=min(top_k_first_stage, index.ntotal))\n",
        "\n",
        "    # 2. Reranking with cross-encoder\n",
        "    cross_encoder_candidates = []\n",
        "    for idx in indices[0]:  # indices comes as a 2D array\n",
        "        doc_id = doc_ids[idx]\n",
        "        doc_info = corpus[doc_id]\n",
        "        title = doc_info.get('title', '')\n",
        "        text = doc_info['text']\n",
        "        combined_text = f\"{title}. {text}\" if title else text\n",
        "        cross_encoder_candidates.append([query, combined_text])\n",
        "\n",
        "    # Score with cross-encoder\n",
        "    cross_scores = cross_encoder.predict(cross_encoder_candidates)\n",
        "\n",
        "    # Sort by cross-encoder scores\n",
        "    cross_results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        doc_id = doc_ids[idx]\n",
        "        biencoder_score = float(scores[0][i])  # Convert from numpy float to Python float\n",
        "        cross_score = float(cross_scores[i])   # Ensure Python float\n",
        "        cross_results.append((doc_id, biencoder_score, cross_score))\n",
        "\n",
        "    # Sort by cross-encoder score\n",
        "    cross_results = sorted(cross_results, key=lambda x: x[2], reverse=True)[:top_k_reranked]\n",
        "\n",
        "    # Format final results\n",
        "    results = []\n",
        "    for doc_id, biencoder_score, cross_score in cross_results:\n",
        "        doc_info = corpus[doc_id]\n",
        "        title = doc_info.get('title', '')\n",
        "        text = doc_info['text']\n",
        "        combined_text = f\"{title}. {text}\" if title else text\n",
        "        results.append({\n",
        "            'doc_id': doc_id,\n",
        "            'biencoder_score': biencoder_score,\n",
        "            'cross_score': cross_score,\n",
        "            'text': combined_text\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Mistral API function for answer generation\n",
        "def generate_answer(query, context, domain_prompt=\"\", max_length=500):\n",
        "    \"\"\"Generate an answer using the Mistral API based on retrieved documents and domain-specific prompting\"\"\"\n",
        "\n",
        "    # You'll need to get an API key from Mistral AI and set it as an environment variable\n",
        "    # or replace the line below with your actual API key\n",
        "    api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"MISTRAL_API_KEY environment variable not set. Please set it before calling this function.\")\n",
        "\n",
        "    # Mistral API endpoint\n",
        "    api_url = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "\n",
        "    # Create the messages list\n",
        "    messages = []\n",
        "\n",
        "    # Add system message with domain-specific prompt if provided\n",
        "    if domain_prompt:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": domain_prompt.strip()\n",
        "        })\n",
        "\n",
        "    # Add user message with context and query\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Context:\\n{context.strip()}\\n\\nQuestion: {query.strip()}\"\n",
        "    })\n",
        "\n",
        "    # Prepare the API request payload\n",
        "    payload = {\n",
        "        \"model\": \"mistral-large-latest\",  # You can change to other Mistral models as needed\n",
        "        \"messages\": messages,\n",
        "        \"max_tokens\": max_length,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.9,\n",
        "    }\n",
        "\n",
        "    # Set up headers with API key\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Make the API request\n",
        "        response = requests.post(api_url, headers=headers, json=payload)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        # Parse the JSON response\n",
        "        response_data = response.json()\n",
        "\n",
        "        # Extract the generated text\n",
        "        generated_text = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Mistral API: {e}\")\n",
        "        # Fallback to a simple response if API call fails\n",
        "        return f\"I couldn't generate a response using the Mistral API due to an error: {str(e)}\""
      ],
      "metadata": {
        "id": "njBHKpuj_s7q"
      },
      "id": "njBHKpuj_s7q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f4311d",
      "metadata": {
        "id": "d2f4311d"
      },
      "outputs": [],
      "source": [
        "def run_rag_pipeline(query, query_id=None, top_k_first_stage=100, top_k_reranked=5, domain_prompt=\"\", show_timings=False):\n",
        "    \"\"\"Run the full RAG pipeline with Mistral API for generation\"\"\"\n",
        "    # Timing dictionary\n",
        "    timings = {}\n",
        "\n",
        "    # 1. Retrieve and rerank documents\n",
        "    start_time = time.time()\n",
        "    retrieved_docs = retrieve_documents(query, top_k_first_stage, top_k_reranked)\n",
        "    timings['retrieval'] = time.time() - start_time\n",
        "\n",
        "    # 2. Format context for LLM\n",
        "    context = \"\"\n",
        "    for i, doc in enumerate(retrieved_docs):\n",
        "        context += f\"Document {i+1} [Score: {doc['cross_score']:.3f}]:\\n{doc['text']}\\n\\n\"\n",
        "\n",
        "    # 3. Generate answer with Mistral API\n",
        "    start_time = time.time()\n",
        "    answer = generate_answer(query, context, domain_prompt)\n",
        "    timings['generation'] = time.time() - start_time\n",
        "\n",
        "    # Total time\n",
        "    timings['total'] = timings['retrieval'] + timings['generation']\n",
        "\n",
        "    # Create run (for ir_measures evaluation)\n",
        "    run = {}\n",
        "    if query_id is not None:\n",
        "        run[query_id] = {doc['doc_id']: float(doc['cross_score']) for doc in retrieved_docs}\n",
        "\n",
        "    return retrieved_docs, answer, timings, run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d791376f",
      "metadata": {
        "id": "d791376f"
      },
      "outputs": [],
      "source": [
        "def evaluate_with_ir_measures(run, qrels, metrics_list=None):\n",
        "    \"\"\"\n",
        "    Evaluate a run using ir_measures library\n",
        "\n",
        "    Args:\n",
        "        run: Dict mapping query_id -> {doc_id -> score}\n",
        "        qrels: Dict mapping query_id -> {doc_id -> relevance}\n",
        "        metrics_list: List of ir_measures metric objects (default: common metrics)\n",
        "\n",
        "    Returns:\n",
        "        Dict of metric results\n",
        "    \"\"\"\n",
        "    if metrics_list is None:\n",
        "        # Define default metrics to evaluate\n",
        "        metrics_list = [\n",
        "            nDCG@5, nDCG@10,       # Normalized Discounted Cumulative Gain\n",
        "            P@5, P@10,             # Precision at k\n",
        "            R@5, R@10,             # Recall at k\n",
        "            AP,                    # Average Precision\n",
        "            RR                      # Reciprocal Rank\n",
        "        ]\n",
        "\n",
        "    # Calculate aggregate metrics\n",
        "    try:\n",
        "        results = ir_measures.calc_aggregate(metrics_list, qrels, run)\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {e}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12047b0b",
      "metadata": {
        "id": "12047b0b"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "def display_rag_results(query, retrieved_docs, answer, timings=None, metrics=None, use_case=None):\n",
        "    \"\"\"Display RAG results in a formatted way with domain-specific formatting\"\"\"\n",
        "\n",
        "    display(HTML(f\"<h2 style='color:#eee;'>RAG Results for {use_case if use_case else 'Query'}</h2>\"))\n",
        "    display(HTML(f\"<h3 style='color:#ccc;'>üìù Query: {query}</h3>\"))\n",
        "\n",
        "    # Display metrics if available\n",
        "    if metrics:\n",
        "        display(HTML(\"<h3 style='color:#ccc;'>üìä Retrieval Metrics:</h3>\"))\n",
        "        metrics_html = \"<table style='color:#ddd; border-collapse: collapse;'>\"\n",
        "        metrics_html += \"<tr><th style='padding: 4px 10px;'>Metric</th><th style='padding: 4px 10px;'>Value</th></tr>\"\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            metrics_html += f\"<tr><td style='padding: 4px 10px;'>{metric_name}</td><td style='padding: 4px 10px;'>{metric_value:.4f}</td></tr>\"\n",
        "        metrics_html += \"</table>\"\n",
        "        display(HTML(metrics_html))\n",
        "\n",
        "    # Display retrieved documents\n",
        "    display(HTML(\"<h3 style='color:#ccc;'>üìö Retrieved Documents:</h3>\"))\n",
        "    for i, doc in enumerate(retrieved_docs):\n",
        "        display(Markdown(f\"**Document {i+1}:**\"))\n",
        "        display(Markdown(f\"- **Biencoder Score:** {doc['biencoder_score']:.3f}\"))\n",
        "        display(Markdown(f\"- **Cross-Encoder Score:** {doc['cross_score']:.3f}\"))\n",
        "        display(Markdown(f\"- **Document ID:** {doc['doc_id']}\"))\n",
        "        display(Markdown(f\"- **Text:** {doc['text'][:300]}...\"))\n",
        "        print()\n",
        "\n",
        "    # Display generated answer\n",
        "    display(HTML(\"<h3 style='color:#ccc;'>ü§ñ Generated Answer:</h3>\"))\n",
        "\n",
        "    # Apply consistent dark-mode-friendly, domain-specific styling\n",
        "    color_map = {\n",
        "        \"Scientific Research\": \"#3498db\",\n",
        "        \"Technical Support\": \"#2ecc71\",\n",
        "        \"Education & Library\": \"#f39c12\",\n",
        "        \"Legal Document Analysis\": \"#9b59b6\",\n",
        "        \"Healthcare Information\": \"#e74c3c\",\n",
        "        \"Campus Info\": \"#16a085\"\n",
        "    }\n",
        "    border_color = color_map.get(use_case, \"#7f8c8d\")  # default gray\n",
        "\n",
        "    answer_html = f'''\n",
        "    <div style=\"\n",
        "        border-left: 5px solid {border_color};\n",
        "        padding: 12px 16px;\n",
        "        margin: 10px 0;\n",
        "        background-color: #1e1e1e;\n",
        "        color: #e0e0e0;\n",
        "        border-radius: 8px;\n",
        "        font-size: 15px;\n",
        "        line-height: 1.6;\">\n",
        "        {answer}\n",
        "    </div>\n",
        "    '''\n",
        "\n",
        "    display(HTML(answer_html))\n",
        "\n",
        "    # Display timings if available\n",
        "    if timings:\n",
        "        display(HTML(\"<h3 style='color:#ccc;'>‚è±Ô∏è Performance Metrics:</h3>\"))\n",
        "        timings_html = \"<table style='color:#ddd; border-collapse: collapse;'>\"\n",
        "        timings_html += \"<tr><th style='padding: 4px 10px;'>Metric</th><th style='padding: 4px 10px;'>Time (seconds)</th></tr>\"\n",
        "        timings_html += f\"<tr><td style='padding: 4px 10px;'>Retrieval time</td><td style='padding: 4px 10px;'>{timings['retrieval']:.3f}</td></tr>\"\n",
        "        timings_html += f\"<tr><td style='padding: 4px 10px;'>Generation time</td><td style='padding: 4px 10px;'>{timings['generation']:.3f}</td></tr>\"\n",
        "        timings_html += f\"<tr><td style='padding: 4px 10px;'>Total RAG time</td><td style='padding: 4px 10px;'>{timings['total']:.3f}</td></tr>\"\n",
        "        timings_html += \"</table>\"\n",
        "        display(HTML(timings_html))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ghNuIbt1QX1b",
      "metadata": {
        "id": "ghNuIbt1QX1b"
      },
      "source": [
        "## Run RAG on Sample Queries for the Selected Use Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f848de",
      "metadata": {
        "id": "76f848de"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n=== Sample Queries from {use_case} Use Case ===\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wy70LBlPQlsP",
      "metadata": {
        "id": "Wy70LBlPQlsP"
      },
      "source": [
        "Get domain-specific prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c467244",
      "metadata": {
        "id": "2c467244"
      },
      "outputs": [],
      "source": [
        "domain_prompt = use_cases[use_case][\"domain_prompt\"]\n",
        "domain_metrics = get_domain_metrics(use_case)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(domain_prompt)\n",
        "print(domain_metrics)"
      ],
      "metadata": {
        "id": "ld2SvvNLGA25"
      },
      "id": "ld2SvvNLGA25",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "oFht3r2XQkIU",
      "metadata": {
        "id": "oFht3r2XQkIU"
      },
      "source": [
        "Store results for each sample query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b567550b",
      "metadata": {
        "id": "b567550b"
      },
      "outputs": [],
      "source": [
        "sample_results = []\n",
        "combined_run = {}  # For evaluation across all queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d55470",
      "metadata": {
        "id": "56d55470"
      },
      "outputs": [],
      "source": [
        "for query_id, query_info in sample_queries.items():\n",
        "    query_text = query_info['text']\n",
        "    print(f\"Running RAG for query: '{query_text}'\")\n",
        "\n",
        "    # Run the RAG pipeline with domain-specific prompt\n",
        "    retrieved_docs, answer, timings, run = run_rag_pipeline(\n",
        "        query_text,\n",
        "        query_id=query_id,\n",
        "        top_k_first_stage=100,\n",
        "        top_k_reranked=5,\n",
        "        domain_prompt=domain_prompt\n",
        "    )\n",
        "\n",
        "    # Combine run for overall evaluation\n",
        "    combined_run.update(run)\n",
        "\n",
        "    # Store results\n",
        "    sample_results.append({\n",
        "        'query_id': query_id,\n",
        "        'query_text': query_text,\n",
        "        'retrieved_docs': retrieved_docs,\n",
        "        'answer': answer,\n",
        "        'timings': timings,\n",
        "        'run': run\n",
        "    })\n",
        "\n",
        "    # Display a brief summary\n",
        "    print(f\"- Retrieved {len(retrieved_docs)} documents\")\n",
        "    print(f\"- Answer length: {len(answer)} characters\")\n",
        "    print(\"-----------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö†Ô∏è **Note on Performance Without GPU**\n",
        "\n",
        "\n",
        "Running the full RAG pipeline without GPU access can be quite slow‚Äîtaking up to 10 minutes per query.\n",
        "\n",
        "To make the tutorial more accessible and responsive, we‚Äôve included precomputed results below. This allows you to work through the rest of the notebook smoothly without waiting.\n",
        "\n",
        "If you decide to run the actual pipeline later (once you have GPU access or more time), simply comment out the cell below to avoid duplicate execution."
      ],
      "metadata": {
        "id": "zyKAHK7QS1Z0"
      },
      "id": "zyKAHK7QS1Z0"
    },
    {
      "cell_type": "code",
      "source": [
        "sample_results = [\n",
        "  {\n",
        "    'query_id': '1',\n",
        "    'query_text': 'what is the origin of COVID-19',\n",
        "    'retrieved_docs': [\n",
        "      {\n",
        "        'doc_id': '4dtk1kyh',\n",
        "        'biencoder_score': 0.7120170593261719,\n",
        "        'cross_score': 9.036142349243164,\n",
        "        'text': 'Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using Artificial Intelligence. Origin of the COVID-19 virus has been intensely debated in the scientific community since the first infected cases were detected in December 2019. The disease has caused a global pandemic, leading to deaths of thousands of people across the world and thus finding origin of this novel coronavirus is important in responding and controlling the pandemic. Recent research results suggest that bats or pangolins might be the original hosts for the virus based on comparative studies using its genomic sequences. This paper investigates the COVID-19 virus origin by using artificial intelligence (AI) and raw genomic sequences of the virus. More than 300 genome sequences of COVID-19 infected cases collected from different countries are explored and analysed using unsupervised clustering methods. The results obtained from various AI-enabled experiments using clustering algorithms demonstrate that all examined COVID-19 virus genomes belong to a cluster that also contains bat and pangolin coronavirus genomes. This provides evidences strongly supporting scientific hypotheses that bats and pangolins are probable hosts for the COVID-19 virus. At the whole genome analysis level, our findings also indicate that bats are more likely the hosts for the COVID-19 virus than pangolins.'\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 'utsr0zv7',\n",
        "        'biencoder_score': 0.7024656534194946,\n",
        "        'cross_score': 8.175335884094238,\n",
        "        'text': 'The Human Coronavirus Disease COVID-19: Its Origin, Characteristics, and Insights into Potential Drugs and Its Mechanisms. The emerging coronavirus disease (COVID-19) swept across the world, affecting more than 200 countries and territories. Genomic analysis suggests that the COVID-19 virus originated in bats and transmitted to humans through unknown intermediate hosts in the Wuhan seafood market, China, in December of 2019. This virus belongs to the Betacoronavirus group, the same group of the 2003 severe acute respiratory syndrome coronavirus (SARS-CoV), and for the similarity, it was named SARS-CoV-2. Given the lack of registered clinical therapies or vaccines, many physicians and scientists are investigating previously used clinical drugs for COVID-19 treatment. In this review, we aim to provide an overview of the CoVs origin, pathogenicity, and genomic structure, with a focus on SARS-CoV-2. Besides, we summarize the recently investigated drugs that constitute an option for COVID-19 treatment.'\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 'v99vlnox',\n",
        "        'biencoder_score': 0.6849621534347534,\n",
        "        'cross_score': 8.007100105285645,\n",
        "        'text': 'COVID-19 in South Korea. A novel coronavirus (severe acute respiratory syndrome-CoV-2) that initially originated from Wuhan, China, in December 2019 has already caused a pandemic. While this novel coronavirus disease (COVID-19) frequently induces mild diseases, it has also generated severe diseases among certain populations, including older-aged individuals with underlying diseases, such as cardiovascular disease and diabetes. As of 31 March 2020, a total of 9786 confirmed cases with COVID-19 have been reported in South Korea. South Korea has the highest diagnostic rate for COVID-19, which has been the major contributor in overcoming this outbreak. We are trying to reduce the reproduction number of COVID-19 to less than one and eventually succeed in controlling this outbreak using methods such as contact tracing, quarantine, testing, isolation, social distancing and school closure. This report aimed to describe the current situation of COVID-19 in South Korea and our response to this outbreak.'\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 'nvofyg16',\n",
        "        'biencoder_score': 0.6862881779670715,\n",
        "        'cross_score': 7.909483909606934,\n",
        "        'text': 'Covid-19 in South Korea.. A novel coronavirus (severe acute respiratory syndrome-CoV-2) that initially originated from Wuhan, China, in December 2019 has already caused a pandemic. While this novel coronavirus disease (covid-19) frequently induces mild diseases, it has also generated severe diseases among certain populations, including older-aged individuals with underlying diseases, such as cardiovascular disease and diabetes. As of 31 March 2020, a total of 9786 confirmed cases with covid-19 have been reported in South Korea. South Korea has the highest diagnostic rate for covid-19, which has been the major contributor in overcoming this outbreak. We are trying to reduce the reproduction number of covid-19 to less than one and eventually succeed in controlling this outbreak using methods such as contact tracing, quarantine, testing, isolation, social distancing and school closure. This report aimed to describe the current situation of covid-19 in South Korea and our response to this outbreak.'\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 'sh7lrdou',\n",
        "        'biencoder_score': 0.7032864093780518,\n",
        "        'cross_score': 7.851293087005615,\n",
        "        'text': 'The epidemiology and pathogenesis of coronavirus disease (COVID-19) outbreak. Coronavirus disease (COVID-19) is caused by SARS-COV2 and represents the causative agent of a potentially fatal disease that is of great global public health concern. Based on the large number of infected people that were exposed to the wet animal market in Wuhan City, China, it is suggested that this is likely the zoonotic origin of COVID-19. Person-to-person transmission of COVID-19 infection led to the isolation of patients that were subsequently administered a variety of treatments. Extensive measures to reduce person-to-person transmission of COVID-19 have been implemented to control the current outbreak. Special attention and efforts to protect or reduce transmission should be applied in susceptible populations including children, health care providers, and elderly people. In this review, we highlights the symptoms, epidemiology, transmission, pathogenesis, phylogenetic analysis and future directions to control the spread of this fatal disease.'\n",
        "      }\n",
        "    ],\n",
        "    'answer': 'Scientific research assistants are responsible for conducting thorough research and analyzing scientific data to answer specific scientific questions. They work closely with their supervisors and peers to develop and refine research strategies, conduct experiments, collect and analyze data, write and edit research papers, present their work at scientific conferences and publish their results in peer-reviewed journals. Their work is crucial in advancing scientific knowledge and improving human health and wellbeing.',\n",
        "    'timings': {\n",
        "      'retrieval': 34.01727652549744,\n",
        "      'generation': 595.5995147228241,\n",
        "      'total': 629.6167912483215\n",
        "    },\n",
        "    'run': {\n",
        "      '1': {\n",
        "        '4dtk1kyh': 9.036142349243164,\n",
        "        'utsr0zv7': 8.175335884094238,\n",
        "        'v99vlnox': 8.007100105285645,\n",
        "        'nvofyg16': 7.909483909606934,\n",
        "        'sh7lrdou': 7.851293087005615\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    'query_id': '2',\n",
        "    'query_text': 'how does the coronavirus respond to changes in the weather',\n",
        "    'retrieved_docs': [\n",
        "      {\n",
        "        'doc_id': 'w7ycc07b',\n",
        "        'biencoder_score': 0.4392872452735901,\n",
        "        'cross_score': 3.4371156692504883,\n",
        "        'text': \"Does weather affect the growth rate of COVID-19, a study to comprehend transmission dynamics on human health. Abstract The undefendable outbreak of novel coronavirus (SARS-COV-2) lead to a global health emergency due to its higher transmission rate and longer symptomatic duration, created a health surge in a short time. Since Nov 2019 the outbreak in China, the virus is spreading exponentially everywhere. The current study focuses on the relationship between environmental parameters and the growth rate of COVID-19. The statistical analysis suggests that the temperature changes retarded the growth rate and found that -6.28¬∞C and +14.51¬∞C temperature is the favorable range for COVID-19 growth. Gutenberg- Richter's relationship is used to estimate the mean daily rate of exceedance of confirmed cases concerning the change in temperature. Indeed, temperature is the most influential parameter that reduces the growth at the rate of 13-17 cases/day with a 1¬∞C rise in temperature.\"\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 'w5kjmw88',\n",
        "        'biencoder_score': 0.45745065808296204,\n",
        "        'cross_score': 3.3182454109191895,\n",
        "        'text': 'Weathering the pandemic: How the Caribbean Basin can use viral and environmental patterns to predict, prepare, and respond to COVID-19. The 2020 coronavirus pandemic is developing at different paces throughout the world. Some areas, like the Caribbean Basin, have yet to see the virus strike at full force. When it does, there is reasonable evidence to suggest the consequent COVID-19 outbreaks will overwhelm healthcare systems and economies. This is particularly concerning in the Caribbean as pandemics can have disproportionately higher mortality impacts on lower and middle-income countries. Preliminary observations from our team and others suggest that temperature and climatological factors could influence the spread of this novel coronavirus, making spatiotemporal predictions of its infectiousness possible. This review studies geographic and time-based distribution of known respiratory viruses in the Caribbean Basin in an attempt to foresee how the pandemic will develop in this region. This review is meant to aid in planning short- and long-term interventions to manage outbreaks at the international, national, and subnational levels in the region.'\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 'gan10za0',\n",
        "        'biencoder_score': 0.4419426918029785,\n",
        "        'cross_score': 3.288236379623413,\n",
        "        'text': 'Weathering the pandemic: How the Caribbean Basin can use viral and environmental patterns to predict, prepare and respond to COVID‚Äê19. The 2020 coronavirus pandemic is developing at different paces throughout the world. Some areas, like the Caribbean Basin, have yet to see the virus strike at full force. When it does, there is reasonable evidence to suggest the consequent COVID‚Äê19 outbreaks will overwhelm healthcare systems and economies. This is particularly concerning in the Caribbean as pandemics can have disproportionately higher mortality impacts on lower and middle income countries. Preliminary observations from our team and others suggest that temperature and climatological factors could influence the spread of this novel coronavirus, making spatiotemporal predictions of its infectiousness possible. This review studies geographic and time‚Äêbased distribution of known respiratory viruses in the Caribbean Basin in an attempt to foresee how the pandemic will develop in this region. This review is meant to aid in planning short‚Äê and long‚Äêterm interventions to manage outbreaks at the international, national and sub‚Äênational levels in the region. This article is protected by copyright. All rights reserved.'\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 'pdww20r4',\n",
        "        'biencoder_score': 0.48396408557891846,\n",
        "        'cross_score': 2.5067882537841797,\n",
        "        'text': 'The behaviour changes in response to COVID-19 pandemic within Malaysia. The novel coronavirus infection, COVID-19, is a pandemic that currently affects the whole world. During this period, Malaysians displayed a variety of behaviour changes as a response to COVID-19, including panic buying, mass travelling during movement restriction and even absconding from treatment facilities. This article attempts to explore some of these behaviour changes from a behaviourist perspective in order to get a better understanding of the rationale behind the changes.'\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 'zespmk29',\n",
        "        'biencoder_score': 0.47616642713546753,\n",
        "        'cross_score': 2.3425703048706055,\n",
        "        'text': 'How diseases rise and fall with the seasons‚Äîand what it could mean for coronavirus. Scientists and doctors have observed for thousands of years that some diseases, such as polio and influenza, rise and fall with the seasons But why? Ongoing research in animals and humans suggests a variety of causes, including changes in the environment (like pH, temperature, and humidity) and even seasonal and daily changes to our own immune systems Figuring out those answers could one day make all the difference in minimizing the impact of infectious disease outbreaks‚Äîsuch as coronavirus disease 2019'\n",
        "      }\n",
        "    ],\n",
        "    'answer': 'Scientists have been studying the behavioral changes that have emerged during the ongoing global crisis caused by the novel corona virus (sars-cov2). The article \"The Behavior Changes in Responses of Malaysian Citizens Towards the Corona Virus Pandemic\" provides a comprehensive analysis of how people have responded to this crisis. It explores the reasons behind these changes and how they may impact the future of public health and disease prevention. Additionally, \"How Diseases Rise and Fall With the Seasons‚ÄîAnd What It Could Mean for Coronovirus\" discusses the potential implications of weather patterns on disease transmission. Overall, these articles provide valuable insights',\n",
        "    'timings': {\n",
        "      'retrieval': 20.592284440994263,\n",
        "      'generation': 817.0070235729218,\n",
        "      'total': 837.599308013916\n",
        "    },\n",
        "    'run': {\n",
        "      '2': {\n",
        "        'w7ycc07b': 3.4371156692504883,\n",
        "        'w5kjmw88': 3.3182454109191895,\n",
        "        'gan10za0': 3.288236379623413,\n",
        "        'pdww20r4': 2.5067882537841797,\n",
        "        'zespmk29': 2.3425703048706055\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    'query_id': '3',\n",
        "    'query_text': 'will SARS-CoV2 infected people develop immunity? Is cross protection possible?',\n",
        "    'retrieved_docs': [\n",
        "      {\n",
        "        'doc_id': 'eo4ehcjv',\n",
        "        'biencoder_score': 0.6343124508857727,\n",
        "        'cross_score': 5.052664756774902,\n",
        "        'text': \"Children's vaccines do not induce cross reactivity against SARS-CoV.. In contrast with adults, children infected by severe acute respiratory syndrome-corona virus (SARS-CoV) develop milder clinical symptoms. Because of this, it is speculated that children vaccinated with various childhood vaccines might develop cross immunity against SARS-CoV. Antisera and T cells from mice immunised with various vaccines were used to determine whether they developed cross reactivity against SARS-CoV. The results showed no marked cross reactivity against SARS-CoV, which implies that the reduced symptoms among children infected by SARS-CoV may be caused by other factors.\"\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 'yigj0u3n',\n",
        "        'biencoder_score': 0.6495019197463989,\n",
        "        'cross_score': 4.585306167602539,\n",
        "        'text': 'Serologic cross-reactivity of SARS-CoV-2 with endemic and seasonal Betacoronaviruses. In order to properly understand the spread of SARS-CoV-2 infection and development of humoral immunity, researchers have evaluated the presence of serum antibodies of people worldwide experiencing the pandemic. These studies rely on the use of recombinant proteins from the viral genome in order to identify serum antibodies that recognize SARS-CoV-2 epitopes. Here, we discuss the cross-reactivity potential of SARS-CoV-2 antibodies with the full spike proteins of four other Betacoronaviruses that cause disease in humans, MERS-CoV, SARS-CoV, HCoV-OC43, and HCoV-HKU1. Using enzyme-linked immunosorbent assays (ELISAs), we detected the potential cross-reactivity of antibodies against SARS-CoV-2 towards the four other coronaviruses, with the strongest cross-recognition between SARS-CoV-2 and SARS /MERS-CoV antibodies, as expected based on sequence homology of their respective spike proteins. Further analysis of cross-reactivity could provide informative data that could lead to intelligently designed pan-coronavirus therapeutics or vaccines.'\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 'buwz6lu3',\n",
        "        'biencoder_score': 0.6658353805541992,\n",
        "        'cross_score': 4.557277202606201,\n",
        "        'text': 'Lack of cross-neutralization by SARS patient sera towards SARS-CoV-2. Despite initial findings indicating that SARS-CoV and SARS-CoV-2 are genetically related belonging to the same virus species and that the two viruses used the same entry receptor, angiotensin-converting enzyme 2 (ACE2), our data demonstrated that there is no detectable cross-neutralization by SARS patient sera against SARS-CoV-2. We also found that there are significant levels of neutralizing antibodies in recovered SARS patients 9-17 years after initial infection. These findings will be of significant use in guiding the development of serologic tests, formulating convalescent plasma therapy strategies, and assessing the longevity of protective immunity for SARS-related coronaviruses in general as well as vaccine efficacy.'\n",
        "      },\n",
        "      {\n",
        "        'doc_id': '8i1u1a9t',\n",
        "        'biencoder_score': 0.6663722991943359,\n",
        "        'cross_score': 4.5535807609558105,\n",
        "        'text': 'Lack of cross-neutralization by SARS patient sera towards SARS-CoV-2. Despite initial findings indicating that SARS-CoV and SARS-CoV-2 are genetically related belonging to the same virus species and that the two viruses used the same entry receptor, angiotensin-converting enzyme 2 (ACE2), our data demonstrated that there is no detectable cross-neutralization by SARS patient sera against SARS-CoV-2. We also found that there are significant levels of neutralizing antibodies in recovered SARS patients 9‚Äì17 years after initial infection. These findings will be of significant use in guiding the development of serologic tests, formulating convalescent plasma therapy strategies, and assessing the longevity of protective immunity for SARS-related coronaviruses in general as well as vaccine efficacy.'\n",
        "      },\n",
        "      {\n",
        "        'doc_id': 't3sjv4hv',\n",
        "        'biencoder_score': 0.6718544960021973,\n",
        "        'cross_score': 4.239818572998047,\n",
        "        'text': 'SARS-CoV-2 infection protects against rechallenge in rhesus macaques. An understanding of protective immunity to SARS-CoV-2 is critical for vaccine and public health strategies aimed at ending the global COVID-19 pandemic. A key unanswered question is whether infection with SARS-CoV-2 results in protective immunity against re-exposure. We developed a rhesus macaque model of SARS-CoV-2 infection and observed that macaques had high viral loads in the upper and lower respiratory tract, humoral and cellular immune responses, and pathologic evidence of viral pneumonia. Following initial viral clearance, animals were rechallenged with SARS-CoV-2 and showed 5 log10 reductions in median viral loads in bronchoalveolar lavage and nasal mucosa compared with primary infection. Anamnestic immune responses following rechallenge suggested that protection was mediated by immunologic control. These data show that SARS-CoV-2 infection induced protective immunity against re-exposure in nonhuman primates.'\n",
        "      }\n",
        "    ],\n",
        "    'answer': 'Certainly! The evidence presented in these documents supports the idea that people who have recovered from Sars-Cov2 may develop some level of immuno-tolerance against the virus. However, the specific mechanisms by which this occurs are not yet fully understood. Cross-protection, or the ability of someone who has been exposed to a particular pathogen to develop protection against subsequent infections, is a complex process that involves a variety of factors, including the type and severity of exposure, genetic makeup of the individual, host response to antigenic stimulation (e.g. Immune activation, cytokine release), and the duration and strength of protection afforded by the previous inoc',\n",
        "    'timings': {\n",
        "      'retrieval': 26.644312143325806,\n",
        "      'generation': 955.8774149417877,\n",
        "      'total': 982.5217270851135\n",
        "    },\n",
        "    'run': {\n",
        "      '3': {\n",
        "        'eo4ehcjv': 5.052664756774902,\n",
        "        'yigj0u3n': 4.585306167602539,\n",
        "        'buwz6lu3': 4.557277202606201,\n",
        "        '8i1u1a9t': 4.5535807609558105,\n",
        "        't3sjv4hv': 4.239818572998047\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "KuEMFWk4SuRw"
      },
      "id": "KuEMFWk4SuRw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_run = {}\n",
        "\n",
        "for result in sample_results:\n",
        "    query_id = result[\"query_id\"]\n",
        "    run_dict = result[\"run\"]\n",
        "\n",
        "    for qid, doc_scores in run_dict.items():\n",
        "        for doc_id, score in doc_scores.items():\n",
        "            combined_run[f\"{qid} {doc_id}\"] = score\n"
      ],
      "metadata": {
        "id": "mIpJp9sDUHy3"
      },
      "id": "mIpJp9sDUHy3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "O0i9ACgQQgSZ",
      "metadata": {
        "id": "O0i9ACgQQgSZ"
      },
      "source": [
        "Evaluate all sample queries using ir_measures with domain-specific metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd771a0e",
      "metadata": {
        "id": "fd771a0e"
      },
      "outputs": [],
      "source": [
        "if combined_run and qrels_dict:\n",
        "    print(\"\\n=== Evaluation with ir_measures ===\\n\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics_results = evaluate_with_ir_measures(combined_run, qrels_dict, domain_metrics)\n",
        "\n",
        "    # Display results\n",
        "    print(\"Overall Evaluation Results:\")\n",
        "    for metric, value in metrics_results.items():\n",
        "        print(f\"  - {metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oC-h_ErXQeCW",
      "metadata": {
        "id": "oC-h_ErXQeCW"
      },
      "source": [
        "Detailed Results Viewer with Domain-Specific Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f02f69ea",
      "metadata": {
        "id": "f02f69ea"
      },
      "outputs": [],
      "source": [
        "query_index = 0  # @param [\"0\", \"1\", \"2\"] {type:\"raw\"}\n",
        "show_timings = True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a56bea3",
      "metadata": {
        "id": "7a56bea3"
      },
      "outputs": [],
      "source": [
        "if sample_results and query_index < len(sample_results):\n",
        "    result = sample_results[query_index]\n",
        "\n",
        "    # Evaluate this specific query with ir_measures using domain-specific metrics\n",
        "    if qrels_dict and result['query_id'] in qrels_dict:\n",
        "        metrics = evaluate_with_ir_measures(\n",
        "            result['run'],\n",
        "            qrels_dict,\n",
        "            domain_metrics\n",
        "        )\n",
        "    else:\n",
        "        metrics = None\n",
        "\n",
        "    # Display results with domain-specific formatting\n",
        "    display_rag_results(\n",
        "        result['query_text'],\n",
        "        result['retrieved_docs'],\n",
        "        result['answer'],\n",
        "        result['timings'] if show_timings else None,\n",
        "        metrics,\n",
        "        use_case\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TNrdtf--QpBD",
      "metadata": {
        "id": "TNrdtf--QpBD"
      },
      "source": [
        "## Try RAG with Your Own Query for this Domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "140ee90b",
      "metadata": {
        "id": "140ee90b"
      },
      "outputs": [],
      "source": [
        "\n",
        "user_query = \"What evidence supports the effectiveness of Remdesivir in treating COVID-19 patients?\" # @param {type:\"string\"}\n",
        "top_k_first_stage = 100 # @param {type:\"slider\", min:10, max:500, step:10}\n",
        "top_k_reranked = 5 # @param {type:\"slider\", min:1, max:20, step:1}\n",
        "show_timings = True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe00227",
      "metadata": {
        "id": "fbe00227"
      },
      "outputs": [],
      "source": [
        "# If no query is provided, use a default example from the selected use case\n",
        "if not user_query:\n",
        "    user_query = use_cases[use_case][\"example_queries\"][0]\n",
        "    print(f\"Using example query: \\\"{user_query}\\\"\")\n",
        "    print(\"Enter your own query above to try something different!\")\n",
        "else:\n",
        "    print(f\"Processing custom query: \\\"{user_query}\\\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743382ff",
      "metadata": {
        "id": "743382ff"
      },
      "outputs": [],
      "source": [
        "# Get domain-specific prompt\n",
        "domain_prompt = use_cases[use_case][\"domain_prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff590fe1",
      "metadata": {
        "id": "ff590fe1"
      },
      "outputs": [],
      "source": [
        "# Run the RAG pipeline with domain-specific prompt\n",
        "retrieved_docs, answer, timings, _ = run_rag_pipeline(\n",
        "    user_query,\n",
        "    top_k_first_stage=top_k_first_stage,\n",
        "    top_k_reranked=top_k_reranked,\n",
        "    domain_prompt=domain_prompt,\n",
        "    show_timings=show_timings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e33cc4",
      "metadata": {
        "id": "71e33cc4"
      },
      "outputs": [],
      "source": [
        "# Display results with domain-specific formatting\n",
        "display_rag_results(\n",
        "    user_query,\n",
        "    retrieved_docs,\n",
        "    answer,\n",
        "    timings if show_timings else None,\n",
        "    use_case=use_case\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä Understanding the Results\n",
        "\n",
        "Let's analyze what happened in this RAG example:\n",
        "\n",
        "1. **Retrieval Quality**:\n",
        "   - Examine the relevance scores of retrieved documents\n",
        "   - Higher cross-encoder scores indicate better matches to the query\n",
        "   - Does the first document contain the information needed?\n",
        "\n",
        "2. **Answer Completeness**:\n",
        "   - Check if the generated answer incorporates information from the retrieved documents\n",
        "   - Is the answer comprehensive or missing key information?\n",
        "\n",
        "3. **Domain Appropriateness**:\n",
        "   - Does the response match the expected format for this domain?\n",
        "   - For scientific content: Are citations and caveats included?\n",
        "   - For technical support: Are steps clear and actionable?\n",
        "   - For educational content: Is the explanation accessible and structured?\n",
        "\n",
        "Try modifying the query slightly and observe how the retrieval results change."
      ],
      "metadata": {
        "id": "rrEuzlMHNASF"
      },
      "id": "rrEuzlMHNASF"
    },
    {
      "cell_type": "markdown",
      "id": "MVbi-jDfQ1ku",
      "metadata": {
        "id": "MVbi-jDfQ1ku"
      },
      "source": [
        "\n",
        "## Best Practices for RAG in Each Domain\n",
        "\n",
        "### Scientific Research\n",
        "- Use domain-specific embeddings trained on scientific literature.\n",
        "- Implement citation tracking to show provenance of information.\n",
        "- Consider combining with a knowledge graph for concept relationships.\n",
        "- Include publication date as a ranking factor for recency.\n",
        "- Use subject-specific vocabularies for query expansion.\n",
        "\n",
        "### Technical Support\n",
        "- Optimize for high precision in top results.\n",
        "- Implement conversational context to track troubleshooting steps.\n",
        "- Use step detection to format answers as procedures.\n",
        "- Consider hybrid retrieval combining semantic and keyword search.\n",
        "- Add feedback mechanisms to improve answer quality over time.\n",
        "\n",
        "### Education & Library\n",
        "- Organize retrieved content by difficulty/education level.\n",
        "- Implement learning path generation based on prerequisite concepts.\n",
        "- Support multiple learning styles in answer generation.\n",
        "- Include multimedia content recommendations when available.\n",
        "- Use readability scores to match content to learner level.\n",
        "\n",
        "### Legal Document Analysis\n",
        "- Prioritize precision and traceability of information.\n",
        "- Implement jurisdiction detection for relevant legal standards.\n",
        "- Track legal precedent relationships between documents.\n",
        "- Use specialized legal embeddings trained on case law.\n",
        "- Include confidence scores and disclaimers in generated answers.\n",
        "\n",
        "### Healthcare Information\n",
        "- Implement evidence quality assessment for medical information.\n",
        "- Include recency as a critical ranking factor.\n",
        "- Use medical taxonomy mapping for query expansion.\n",
        "- Apply stricter fact verification for medical claims.\n",
        "- Include appropriate medical disclaimers in answers.\n",
        "\n",
        "### Campus Info\n",
        "- Use location-aware retrieval for campus service queries.\n",
        "- Implement up-to-date checking for office hours and contact info.\n",
        "- Include department hierarchy awareness for better routing.\n",
        "- Use hybrid search combining exact name matching with semantic.\n",
        "- Prioritize official pages over event announcements.\n",
        "\n",
        "---\n",
        "\n",
        "## General Implementation Guidance for RAG\n",
        "\n",
        "When implementing RAG for a specific domain, also consider:\n",
        "\n",
        "- **Pre-processing:** Apply domain-specific cleaning and normalization.\n",
        "- **Chunking strategy:** Adjust document chunking based on domain document structure.\n",
        "- **Embedding models:** Fine-tune or select embeddings trained on domain data.\n",
        "- **LLM prompting:** Craft specialized prompts with domain context.\n",
        "- **Post-processing:** Format answers according to domain conventions.\n",
        "\n",
        "If you do not have a domain-specific use case:\n",
        "\n",
        "- Select appropriate embedding models for your specific use case.\n",
        "- Experiment with chunking strategies based on your document structure.\n",
        "- Fine-tune retrieval parameters (k values, reranking) based on evaluation.\n",
        "- Craft effective prompts that include relevant context for the LLM.\n",
        "- Implement user feedback mechanisms to improve over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Troubleshooting Common Issues\n",
        "\n",
        "When implementing RAG systems, you may encounter these common challenges:\n",
        "\n",
        "### Retrieval Problems\n",
        "\n",
        "1. **Irrelevant Documents Retrieved**\n",
        "   - **Symptoms**: Retrieved documents don't match query intent\n",
        "   - **Solutions**:\n",
        "     - Try different embedding models (domain-specific if available)\n",
        "     - Adjust chunking strategy (smaller or larger chunks)\n",
        "     - Implement query expansion or reformulation\n",
        "     - Add a reranking step with a cross-encoder\n",
        "\n",
        "2. **Missing Information**\n",
        "   - **Symptoms**: System can't find information you know exists in the corpus\n",
        "   - **Solutions**:\n",
        "     - Increase the number of retrieved documents (k)\n",
        "     - Try hybrid retrieval (combine dense and sparse methods)\n",
        "     - Check document preprocessing (ensure no information loss)\n",
        "\n",
        "### Generation Issues\n",
        "\n",
        "1. **Hallucinations Despite RAG**\n",
        "   - **Symptoms**: Model generates incorrect information despite relevant context\n",
        "   - **Solutions**:\n",
        "     - Adjust prompt to emphasize using only retrieved information\n",
        "     - Implement fact-checking or confidence scoring\n",
        "     - Use a model with better instruction following\n",
        "\n",
        "2. **Poor Integration of Retrieved Content**\n",
        "   - **Symptoms**: Answer doesn't incorporate relevant retrieved information\n",
        "   - **Solutions**:\n",
        "     - Improve context formatting in the prompt\n",
        "     - Try different LLMs or parameter settings\n",
        "     - Test chain-of-thought or step-by-step reasoning\n",
        "\n",
        "### API and Resource Issues\n",
        "\n",
        "1. **Slow Performance**\n",
        "   - Try batch processing where possible\n",
        "   - Consider quantized models or optimized libraries\n",
        "   - Use caching for repeated queries\n",
        "\n",
        "2. **API Failures**\n",
        "   - Implement robust error handling and retries\n",
        "   - Have fallback models or approaches ready\n",
        "   - Cache results whenever possible\n",
        "\n",
        "If you encounter any of these issues during the tutorial, raise your hand, and we'll work through them together."
      ],
      "metadata": {
        "id": "EG8AXP5RN4xN"
      },
      "id": "EG8AXP5RN4xN"
    },
    {
      "cell_type": "markdown",
      "id": "GZiEEIdXQ_qo",
      "metadata": {
        "id": "GZiEEIdXQ_qo"
      },
      "source": [
        "## Evaluation Summary for Current Use Case"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rC2DNBhTRUnY",
      "metadata": {
        "id": "rC2DNBhTRUnY"
      },
      "source": [
        "## üéì Conclusion: Building Effective Domain-Specific RAG Systems\n",
        "\n",
        "This tutorial has demonstrated how to build and customize RAG systems for different domains using prebuilt indexes from the `ir_datasets` library. By leveraging public datasets and sentence-transformer embeddings, we've implemented practical RAG solutions tailored to real-world use cases.\n",
        "\n",
        "### Core Principles for Domain-Specific RAG\n",
        "\n",
        "1. **Know Your Domain**\n",
        "   - Understand the specific information needs and constraints\n",
        "   - Identify domain-specific language, terminology, and concepts\n",
        "   - Recognize the appropriate evidence standards and response formats\n",
        "\n",
        "2. **Customize Each Component**\n",
        "   - Data sources: Select and preprocess domain-appropriate content\n",
        "   - Embedding models: Choose or fine-tune for domain language\n",
        "   - Retrieval strategy: Optimize parameters for domain needs\n",
        "   - Prompting: Design domain-specific instructions and constraints\n",
        "   - Evaluation: Measure what matters for your specific use case\n",
        "\n",
        "3. **Iterate with Feedback**\n",
        "   - Collect domain expert evaluation on system outputs\n",
        "   - Analyze failure cases to identify improvement areas\n",
        "   - Continuously update knowledge bases as domains evolve\n",
        "\n",
        "### Why Domain-Specific RAG Matters\n",
        "\n",
        "Adapting RAG systems to specific domains significantly improves performance:\n",
        "\n",
        "- **Specialized Knowledge**: Retrieve content that aligns with domain-specific needs\n",
        "- **Improved Contextual Understanding**: Domain-aware prompting leads to higher-quality answers\n",
        "- **Targeted Evaluation**: Metrics tailored to domain objectives provide realistic assessments\n",
        "- **Enhanced User Experience**: Formatting and output are better suited to user expectations\n",
        "- **Higher Relevance**: Focused retrieval leads to better precision and recall\n",
        "\n",
        "### From Tutorial to Production\n",
        "\n",
        "To move from this tutorial to production-ready systems:\n",
        "\n",
        "1. **Scale Your Data Pipeline**\n",
        "   - Implement efficient document ingestion workflows\n",
        "   - Set up regular knowledge base updates\n",
        "   - Consider hybrid storage solutions for different content types\n",
        "\n",
        "2. **Optimize for Performance**\n",
        "   - Benchmark and optimize vector search for your scale\n",
        "   - Implement caching strategies for common queries\n",
        "   - Consider quantization for embedding models\n",
        "\n",
        "3. **Enhance with Advanced Techniques**\n",
        "   - Query rewriting and decomposition\n",
        "   - Multi-stage retrieval architectures\n",
        "   - Ensemble methods for improved accuracy\n",
        "   - Hybrid search combining sparse and dense methods\n",
        "   - Sophisticated chunking strategies (overlapping, hierarchical, semantic)\n",
        "   - Self-reflection and validation capabilities\n",
        "\n",
        "4. **Implement Robust Monitoring**\n",
        "   - Track retrieval quality metrics over time\n",
        "   - Monitor for knowledge gaps and hallucinations\n",
        "   - Implement feedback loops from end users\n",
        "\n",
        "### Suggested Next Steps\n",
        "\n",
        "To expand upon this tutorial:\n",
        "\n",
        "- **Build Your Own Indexes**:\n",
        "  - Use datasets from `ir_datasets` such as `beir/trec-covid`, `nfcorpus`, `cqadupstack`\n",
        "  - Generate dense embeddings using Sentence Transformers\n",
        "  - Build and store FAISS indexes for fast retrieval\n",
        "\n",
        "- **Customize for Your Domain**:\n",
        "  - Create tailored prompt templates\n",
        "  - Design custom evaluation methods\n",
        "  - Use visualizations relevant to your domain\n",
        "  - Implement hybrid retrieval suited to the task\n",
        "\n",
        "- **Explore Advanced RAG Architectures**:\n",
        "  - Experiment with multi-hop RAG for complex reasoning\n",
        "  - Try Hypothetical Document Embeddings (HyDE)\n",
        "  - Integrate knowledge graphs with vector search\n",
        "  - Implement fine-tuning for domain-specific retrievers\n",
        "\n",
        "### Further Learning Resources\n",
        "\n",
        "- **[ir_datasets](https://ir-datasets.com/)** & [GitHub](https://github.com/allenai/ir_datasets)\n",
        "- **[BEIR Benchmark](https://github.com/beir-cellar/beir)**\n",
        "- **[HuggingFace Datasets](https://huggingface.co/datasets?search=irds)**\n",
        "- **[Sentence Transformers](https://www.sbert.net/)**\n",
        "- **[FAISS Library](https://github.com/facebookresearch/faiss)**\n",
        "- **[LangChain Docs](https://python.langchain.com/docs/modules/data_connection/retrievers/)**\n",
        "- **[LlamaIndex Docs](https://docs.llamaindex.ai/)**\n",
        "\n",
        "The field of RAG is rapidly evolving, with new techniques and models emerging regularly. By focusing on domain-specific adaptations, you can build systems that deliver truly valuable and trustworthy information experiences across scientific research, technical support, education, fact verification, healthcare, and beyond."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FBiftLCvwT_L",
      "metadata": {
        "id": "FBiftLCvwT_L"
      },
      "source": [
        "\n",
        "## üß≠ A Note on Real-World RAG: Handling Multiple Data Sources with Query Routing\n",
        "\n",
        "In this tutorial, we‚Äôve simplified the setup by letting you choose a single **use case** (e.g., *Scientific Research*, *Technical Support*), and loading **one pre-built index** corresponding to a default dataset for that domain (like `beir/trec-covid` or `beir/cqadupstack/android`). All queries are executed against this single, focused index.\n",
        "\n",
        "This works well for learning the core components of Retrieval-Augmented Generation (RAG) in a controlled environment. However, in production systems, the reality is far more complex.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Real-World RAG Involves Many Diverse Sources\n",
        "\n",
        "Imagine building a RAG system for an enterprise or large-scale application. A single unified index often isn‚Äôt practical or efficient. Real-world systems typically need to answer questions using content drawn from multiple **heterogeneous sources**, such as:\n",
        "\n",
        "- ‚úÖ Internal technical documentation (e.g., Confluence, GitHub wikis)  \n",
        "- ‚úÖ Public product FAQs and marketing sites  \n",
        "- ‚úÖ A database of customer support tickets or live chat logs  \n",
        "- ‚úÖ Regulatory PDFs, whitepapers, or clinical guidelines  \n",
        "- ‚úÖ Web content or external APIs\n",
        "\n",
        "Indexing all of this into one giant vector store is usually **not scalable**, and can dilute retrieval precision. Instead, most real-world systems maintain **multiple specialized indexes**, each optimized for a different content type, domain, or access policy.\n",
        "\n",
        "---\n",
        "\n",
        "### üö¶ Enter Query Routing\n",
        "\n",
        "So how does a system decide **which index (or tool) to query** for a given user question?\n",
        "\n",
        "This is where **Query Routing** becomes essential. A *Query Router* acts like a smart traffic controller inside the RAG pipeline. Before retrieving documents, it analyzes the incoming query and chooses the most relevant source(s) ‚Äî or even routes the request to external tools like search APIs, databases, or reasoning modules.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Common Query Routing Strategies\n",
        "\n",
        "Here are several routing strategies used in modern RAG systems:\n",
        "\n",
        "1. **LLM-Based Routing**  \n",
        "   Use a lightweight [language model](https://arxiv.org/abs/2303.11366) to classify or analyze the query intent. The LLM chooses the best index by comparing the query to high-level descriptions of each source.  \n",
        "   *Example:* ‚ÄúWhat is the refund policy for software licenses?‚Äù ‚Üí routed to the policy documents index.\n",
        "\n",
        "2. **Semantic Routing (Embedding Similarity)**  \n",
        "   Compute a dense vector (embedding) of the query, then compare it to ‚Äúrepresentative vectors‚Äù for each index. This can be done using [FAISS](https://github.com/facebookresearch/faiss), [ScaNN](https://github.com/google-research/google-research/tree/master/scann), or [milvus](https://milvus.io/).  \n",
        "   *Example:* Map queries to closest-matching domains using cosine similarity.\n",
        "\n",
        "3. **Keyword or Rule-Based Routing**  \n",
        "   Set up simple keyword triggers or regex patterns to map queries to sources.  \n",
        "   *Example:* Queries containing ‚Äúerror code‚Äù or ‚Äústack trace‚Äù ‚Üí routed to technical docs.  \n",
        "   This is fast and interpretable, but brittle if query language varies a lot.\n",
        "\n",
        "4. **Metadata Filtering within Shared Indexes**  \n",
        "   If you choose to merge multiple datasets into one index, you can still route using metadata fields (e.g., `document_source = 'faq'`). This lets you use **filters** to scope retrieval only to relevant subsets.\n",
        "\n",
        "---\n",
        "\n",
        "### üèóÔ∏è Hybrid Routing Is Common\n",
        "\n",
        "In practice, many systems combine these approaches:\n",
        "\n",
        "- Use **keyword filtering** for high-precision rules  \n",
        "- Fall back to **semantic routing** for open-ended questions  \n",
        "- Use an **LLM router** for nuanced intent classification  \n",
        "- Combine results from multiple indexes and rerank with a **cross-encoder** or **relevance model**\n",
        "\n",
        "Frameworks like [LangChain](https://docs.langchain.com/docs/components/retrievers/router-retriever/) and [Haystack](https://docs.haystack.deepset.ai/docs/routing_queries) offer modules for building query routers out-of-the-box.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Why We Didn‚Äôt Use Query Routing *Here*\n",
        "\n",
        "For this tutorial, we kept the setup simple by pre-selecting one dataset per domain and routing all queries to a single index. This helps focus on learning RAG fundamentals like retrieval, reranking, and generation ‚Äî without worrying about architectural complexity.\n",
        "\n",
        "But in real-world applications ‚Äî especially in enterprise, healthcare, legal, or customer support scenarios ‚Äî **query routing is essential** to handle:\n",
        "- Information silos\n",
        "- Data volume\n",
        "- Index-specific latency\n",
        "- Access control\n",
        "\n",
        "If you're building a production-ready RAG system, consider implementing query routing as an early design decision.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Further Reading & Examples\n",
        "\n",
        "- [Query Routing in LangChain](https://docs.langchain.com/docs/components/retrievers/router-retriever/)\n",
        "- [Multihop RAG Routing in Google's FiD](https://arxiv.org/abs/2007.00849)\n",
        "- [Retrieval Strategies in Haystack](https://docs.haystack.deepset.ai/docs/routing_queries)\n",
        "- [Prompting for Tool Use](https://arxiv.org/abs/2302.12337) in Multi-Tool LLM systems"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}