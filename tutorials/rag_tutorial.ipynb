{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Domain-Specific RAG: A Practical Tutorial\n"
      ],
      "metadata": {
        "id": "zVwrsNZ6JmpL"
      },
      "id": "zVwrsNZ6JmpL"
    },
    {
      "cell_type": "markdown",
      "id": "d5e9133c",
      "metadata": {
        "id": "d5e9133c"
      },
      "source": [
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        " - Understand the basic concepts of Retrieval-Augmented Generation (RAG).\n",
        " - Implement a simple RAG pipeline using pre-built indexes and models.\n",
        " - Apply domain-specific prompting and evaluation metrics to tailor RAG to different use cases.\n",
        " - Explore the impact of different retrieval and generation parameters on the performance of a RAG system.\n",
        "\n",
        "## Exercises and Challenges (Optional)\n",
        "\n",
        " - **Experiment with different datasets:** Try using other pre-built indexes or creating your own indexes from a dataset of your choice.\n",
        " - **Fine-tune the retrieval model:** Explore fine-tuning the bi-encoder or cross-encoder to improve retrieval accuracy for your domain.\n",
        " - **Evaluate different LLMs:** Compare the performance of different LLMs for answer generation in your use case.\n",
        " - **Build a user interface:** Develop a simple web application or chatbot that integrates your RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üñ•Ô∏è System Requirements & Setup\n",
        "\n",
        "### System Requirements\n",
        "\n",
        "This tutorial uses deep learning models and vector similarity search that benefit from:\n",
        "\n",
        "- **Compute Resources:**\n",
        "  - **RAM:** Minimum 8GB, recommended 16GB+\n",
        "  - **GPU:** Recommended for faster processing (especially when using cross-encoders)\n",
        "  - **Storage:** ~2GB for pre-built indexes and models\n",
        "  - **Google Colab:** This notebook runs well on a free Colab instance with T4 GPU\n",
        "\n",
        "### üß∞ Tools & Libraries We'll Use\n",
        "\n",
        "This tutorial leverages several key libraries:\n",
        "\n",
        "- **[sentence-transformers](https://www.sbert.net/)**: For embedding generation and cross-encoding\n",
        "- **[FAISS](https://github.com/facebookresearch/faiss)**: For efficient similarity search\n",
        "- **[ir_datasets](https://ir-datasets.com/)**: For accessing benchmark datasets\n",
        "- **[Mistral AI](https://mistral.ai/)**: For answer generation using LLMs\n",
        "- **[Hugging Face Hub](https://huggingface.co/docs/hub/index)**: For accessing pre-built indexes\n",
        "\n",
        "### üîë Mistral API Setup\n",
        "\n",
        "To use the Mistral AI API for answer generation:\n",
        "\n",
        "1. Create a Mistral AI account at [https://console.mistral.ai/](https://console.mistral.ai/)\n",
        "2. Generate an API key from your account dashboard\n",
        "3. Add your key to this notebook:\n",
        "   ```python\n",
        "   os.environ[\"MISTRAL_API_KEY\"] = \"YOUR_API_KEY_HERE\"  # Replace with your actual key\n",
        "   ```"
      ],
      "metadata": {
        "id": "F2UYQPOcRWIW"
      },
      "id": "F2UYQPOcRWIW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è RAG Architecture Overview\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant information from external knowledge sources before generating responses. This approach significantly improves accuracy and allows LLMs to access domain-specific information without retraining.\n",
        "\n",
        "![RAG Architecture Overview](https://drive.google.com/uc?export=view&id=1U28sDU5JAE-UfIfT9-BZ_4FEHktpuiah)\n",
        "\n",
        "### Core Components of RAG:\n",
        "\n",
        "1. **Document Processing Pipeline**\n",
        "   - **Collection**: Gathering domain-specific documents, articles, or knowledge bases\n",
        "   - **Chunking**: Breaking documents into manageable segments (paragraphs or sections)\n",
        "   - **Embedding**: Converting text chunks into dense vector representations\n",
        "\n",
        "2. **Retrieval System**\n",
        "   - **Query Processing**: Converting user questions into the same vector space\n",
        "   - **Vector Search**: Finding relevant document chunks using similarity measures\n",
        "   - **Reranking**: Further refining results using more sophisticated models\n",
        "\n",
        "3. **Generation System**\n",
        "   - **Context Assembly**: Combining retrieved information into a prompt\n",
        "   - **LLM Integration**: Sending the enriched prompt to an LLM\n",
        "   - **Response Generation**: Creating coherent, accurate, and contextual answers\n",
        "\n",
        "### Why Domain-Specific RAG Matters\n",
        "\n",
        "Different domains require specialized knowledge and specific response formats:\n",
        "- Medical advice needs evidence-based information and appropriate caveats\n",
        "- Technical support requires precise, actionable instructions\n",
        "- Educational content should be structured for different learning levels\n",
        "\n",
        "In this tutorial, we'll customize each component of the RAG pipeline for specific domains."
      ],
      "metadata": {
        "id": "6nTXBSgXLBCZ"
      },
      "id": "6nTXBSgXLBCZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß© Domain-Specific RAG Architecture\n",
        "\n",
        "Standard RAG pipelines can be enhanced by domain-specific customizations at each stage:\n",
        "\n",
        "### Domain Customization Points:\n",
        "\n",
        "1. **Data Sources & Preprocessing**\n",
        "   - Scientific: Research papers, clinical trials, structured abstracts\n",
        "   - Technical: Documentation, forums, code repositories\n",
        "   - Educational: Textbooks, lecture notes, graded materials\n",
        "\n",
        "2. **Embedding & Retrieval**\n",
        "   - Domain-specific models (e.g., BioBERT for medical, CodeBERT for programming)\n",
        "   - Customized chunking strategies (e.g., by sections, by concepts)\n",
        "   - Specialized ranking functions (e.g., recency weighting for news)\n",
        "\n",
        "3. **Prompt Engineering**\n",
        "   - Domain-appropriate instructions, terminology, and formats\n",
        "   - Specialty-specific constraints and guardrails\n",
        "   - Role-specific personas (e.g., educator, researcher, consultant)\n",
        "\n",
        "4. **Evaluation Metrics**\n",
        "   - Scientific: Citation accuracy, evidence quality\n",
        "   - Technical: Step completeness, actionability\n",
        "   - Educational: Clarity at appropriate learning level\n",
        "\n",
        "In this tutorial, we'll focus on practical implementations of these customizations."
      ],
      "metadata": {
        "id": "hONTg1nTNf4s"
      },
      "id": "hONTg1nTNf4s"
    },
    {
      "cell_type": "markdown",
      "id": "CbFr6thfWeFt",
      "metadata": {
        "id": "CbFr6thfWeFt"
      },
      "source": [
        "## üìö Datasets\n",
        "\n",
        "This tutorial uses a diverse set of datasets from the [BEIR benchmark collection](https://github.com/beir-cellar/beir) (via `ir_datasets`) to demonstrate how Retrieval-Augmented Generation (RAG) can be applied across multiple real-world domains.\n",
        "\n",
        "These datasets represent realistic information needs from different user groups, such as researchers, developers, educators, and the general public. Pre-built indexes have been provided for each dataset to ensure the tutorial runs smoothly in a short time window (~30 minutes), avoiding the overhead of on-the-fly index construction.\n",
        "\n",
        "### üß™ Scientific Research Domain\n",
        "- `beir/trec-covid`: Focused on COVID-19 research questions using the CORD-19 corpus.\n",
        "- `beir/scifact`: Scientific claim verification, where the system must support or refute claims using scientific abstracts.\n",
        "- `beir/nfcorpus`: Non-factoid biomedical QA, based on user questions from the NLM‚Äôs PubMed Helpdesk.\n",
        "\n",
        "### üõ†Ô∏è Technical Support Domain\n",
        "- `beir/cqadupstack/android`: Community Q&A data from Stack Exchange on Android development.\n",
        "- `beir/cqadupstack/webmasters`: Web hosting and webmaster technical queries.\n",
        "- `beir/cqadupstack/unix`: Unix/Linux command-line and scripting support.\n",
        "\n",
        "### üéì Education & Library Domain\n",
        "- `beir/natural-questions`: Real user questions from Google Search and answers from Wikipedia.\n",
        "- `beir/hotpotqa`: Multi-hop QA dataset requiring reasoning over multiple Wikipedia documents.\n",
        "- `beir/nfcorpus`: Also used here for medically-themed educational queries.\n",
        "\n",
        "### üîç Fact Verification Domain\n",
        "- `beir/fever`: Fact-checking based on Wikipedia claims.\n",
        "- `beir/climate-fever`: Focused on climate change-related claims and evidence.\n",
        "- `beir/scifact`: Also shared here for scientific claim verification.\n",
        "\n",
        "### üè• Healthcare Information Domain\n",
        "- `beir/nfcorpus`: Used for health-related literature review and question answering.\n",
        "- `beir/trec-covid`: Reused here to address health policy and treatment questions during the pandemic.\n",
        "\n",
        "> ‚ÑπÔ∏è Each domain has one **default dataset** selected (the first in each list), but you can explore other datasets in that domain using the dropdown selector. All datasets are accessed through the `ir_datasets` library, and prebuilt indexes are provided to ensure quick experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rZX8hgtUOTYY",
      "metadata": {
        "id": "rZX8hgtUOTYY"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8KlzcZr0Nx56",
      "metadata": {
        "id": "8KlzcZr0Nx56"
      },
      "source": [
        "Install required packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4d95ec",
      "metadata": {
        "id": "ae4d95ec"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers transformers torch numpy faiss-cpu tqdm ir_datasets ir_measures pandas matplotlib ipywidgets huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8J113gylN4cn",
      "metadata": {
        "id": "8J113gylN4cn"
      },
      "source": [
        "Import required libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "073b8036",
      "metadata": {
        "id": "073b8036"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import ir_datasets\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import faiss\n",
        "import pickle\n",
        "import ir_measures\n",
        "from ir_measures import *\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32oGSSCiN-hf",
      "metadata": {
        "id": "32oGSSCiN-hf"
      },
      "source": [
        "Set up GPU/CPU device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3cd6eb5",
      "metadata": {
        "id": "c3cd6eb5"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ddc06tcby-BF",
      "metadata": {
        "id": "Ddc06tcby-BF"
      },
      "outputs": [],
      "source": [
        "HUB_REPO_ID = \"ShubhamC/rag-tutorial-prebuilt-indexes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mvcQlQOX-9x5",
      "metadata": {
        "id": "mvcQlQOX-9x5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Basic RAG Pipeline: A Minimal Example\n",
        "\n",
        "Before diving into domain customization, let's understand the basic RAG workflow with a minimal working example. This gives us a foundation to build upon.\n",
        "\n",
        "### The Four Essential Steps:\n",
        "\n",
        "1. Load a document collection\n",
        "2. Convert user query to a vector\n",
        "3. Retrieve relevant documents\n",
        "4. Generate an answer using retrieved information"
      ],
      "metadata": {
        "id": "5pfi1u53MHtB"
      },
      "id": "5pfi1u53MHtB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick demonstration of a minimal RAG pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# 1. Create a tiny document collection (normally this would be loaded from a database)\n",
        "mini_docs = [\n",
        "    \"RAG stands for Retrieval-Augmented Generation in AI systems.\",\n",
        "    \"Embedding models convert text into numerical vectors.\",\n",
        "    \"FAISS is a library for efficient similarity search in vector spaces.\",\n",
        "    \"Language models generate text based on provided prompts.\"\n",
        "]\n",
        "\n",
        "print(\"1. Loaded document collection with\", len(mini_docs), \"documents\\n\")\n",
        "\n",
        "# 2. Create embeddings for documents\n",
        "mini_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "doc_embeddings = mini_model.encode(mini_docs)\n",
        "print(\"2. Created document embeddings with shape:\", doc_embeddings.shape, \"\\n\")\n",
        "\n",
        "# Create a simple vector index\n",
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_embeddings)\n",
        "print(\"3. Built search index with\", index.ntotal, \"vectors\\n\")\n",
        "\n",
        "# 3. Process a query and retrieve\n",
        "query = \"What is RAG in AI?\"\n",
        "query_vector = mini_model.encode([query])\n",
        "D, I = index.search(query_vector, k=2)  # Get top 2 results\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nRetrieved documents:\")\n",
        "for i, doc_idx in enumerate(I[0]):\n",
        "    print(f\"[{i+1}] {mini_docs[doc_idx]} (distance: {D[0][i]:.2f})\")\n",
        "\n",
        "# 4. Generate answer (simplified simulation without actual LLM API call)\n",
        "print(\"\\n4. Generated Answer:\")\n",
        "print(f\"RAG (Retrieval-Augmented Generation) is an AI technique that enhances language models by retrieving relevant information before generating responses. It combines the knowledge retrieval capabilities of search systems with the text generation abilities of large language models.\")"
      ],
      "metadata": {
        "id": "W64sBt1qMPFS"
      },
      "id": "W64sBt1qMPFS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "-sQtRBD5OGMJ",
      "metadata": {
        "id": "-sQtRBD5OGMJ"
      },
      "source": [
        "## About Pre-built Indexes\n",
        "\n",
        "For this tutorial, we're using pre-built indexes to save time. The indexes were created in advance\n",
        "using datasets from various sources:\n",
        "\n",
        "- BEIR (Benchmarking IR): Contains various IR tasks including scientific literature, news articles, etc.\n",
        "- Stack Exchange collections: Technical Q&A across domains\n",
        "- CORD-19: COVID-19 related research papers\n",
        "- NQ (Natural Questions): General knowledge Q&A\n",
        "\n",
        "The pre-built indexes include:\n",
        "1. Document corpus with text and titles\n",
        "2. Document embeddings using sentence-transformers\n",
        "3. FAISS index for efficient retrieval\n",
        "4. Sample queries with relevance judgments\n",
        "\n",
        "In a typical RAG pipeline, you would need to:\n",
        "1. Download and process a dataset\n",
        "2. Create document embeddings\n",
        "3. Build a search index\n",
        "4. Perform retrieval\n",
        "5. Generate answers with LLM\n",
        "\n",
        "For this tutorial, steps 1-3 are already done for you with the pre-built indexes.\n",
        "We'll focus on steps 4-5: retrieval and generation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eapf2b5QOi8f",
      "metadata": {
        "id": "Eapf2b5QOi8f"
      },
      "source": [
        "## üéØ Use Cases in This Tutorial\n",
        "\n",
        "This tutorial explores how Retrieval-Augmented Generation (RAG) can be customized for different real-world domains. We've defined five practical **use cases**, each mapped to datasets available in the [`ir_datasets`](https://ir-datasets.com/) library and paired with domain-specific example queries and prompts.\n",
        "\n",
        "Each use case provides:\n",
        "- A curated set of relevant datasets\n",
        "- Example queries based on real-world information needs\n",
        "- A system prompt that guides the language model‚Äôs tone and behavior\n",
        "\n",
        "These use cases demonstrate how RAG can go beyond generic QA to serve **domain-specific goals** like academic research, troubleshooting, or fact verification.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ 1. Scientific Research\n",
        "Designed for tasks like literature review, scientific explanation, and research comprehension.\n",
        "\n",
        "- **Default Dataset:** `beir/trec-covid`\n",
        "- **Other Datasets:** `beir/scifact`, `beir/nfcorpus`\n",
        "- **Example Queries:**\n",
        "  - What are the most effective treatments for severe COVID-19?\n",
        "  - How does mRNA vaccine technology work?\n",
        "- **Prompt:** _You are a scientific research assistant. Provide accurate, evidence-based answers with appropriate scientific context and caveats._\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è 2. Technical Support\n",
        "Targets IT and developer support tasks using Stack Exchange-style technical Q&A data.\n",
        "\n",
        "- **Default Dataset:** `beir/cqadupstack/android`\n",
        "- **Other Datasets:** `beir/cqadupstack/webmasters`, `beir/cqadupstack/unix`\n",
        "- **Example Queries:**\n",
        "  - How do I fix network connectivity issues with Android devices?\n",
        "  - What's causing my app to crash on startup?\n",
        "- **Prompt:** _You are a technical support specialist. Provide clear, step-by-step solutions to technical problems with practical troubleshooting advice._\n",
        "\n",
        "---\n",
        "\n",
        "### üéì 3. Education & Library\n",
        "Supports research help, study guides, and educational content generation.\n",
        "\n",
        "- **Default Dataset:** `beir/natural-questions`\n",
        "- **Other Datasets:** `beir/hotpotqa`, `beir/nfcorpus`\n",
        "- **Example Queries:**\n",
        "  - What teaching methods are most effective for student engagement?\n",
        "  - How did World War II impact post-war economic development?\n",
        "- **Prompt:** _You are an educational assistant. Provide informative, well-structured answers suitable for learners, with clear explanations of complex concepts._\n",
        "\n",
        "---\n",
        "\n",
        "### üîç 4. Fact Verification\n",
        "Focuses on verifying potentially controversial or widely debated claims using factual sources.\n",
        "\n",
        "- **Default Dataset:** `beir/fever`\n",
        "- **Other Datasets:** `beir/scifact`, `beir/climate-fever`\n",
        "- **Example Queries:**\n",
        "  - Do vaccines cause autism?\n",
        "  - Is 5G technology harmful to human health?\n",
        "- **Prompt:** _You are a fact-checking assistant. Provide balanced, evidence-based assessments of claims with references to sources where possible._\n",
        "\n",
        "---\n",
        "\n",
        "### üè• 5. Healthcare Information\n",
        "Geared toward health and medical information, patient education, and clinical understanding.\n",
        "\n",
        "- **Default Dataset:** `beir/nfcorpus`\n",
        "- **Other Datasets:** `beir/trec-covid`\n",
        "- **Example Queries:**\n",
        "  - What are the potential side effects of statin medications?\n",
        "  - How effective is cognitive behavioral therapy for anxiety?\n",
        "- **Prompt:** _You are a healthcare information assistant. Provide evidence-based answers about medical topics, while noting that this is not medical advice. Focus on established research and clinical guidelines._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EzTwRQt3OnOI",
      "metadata": {
        "id": "EzTwRQt3OnOI"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7i7NqCVDXl7W",
      "metadata": {
        "id": "7i7NqCVDXl7W"
      },
      "source": [
        "Select RAG Use Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97aaa635",
      "metadata": {
        "id": "97aaa635"
      },
      "outputs": [],
      "source": [
        "# Dictionary of use cases with their descriptions, datasets, and example queries\n",
        "# Focus on datasets publicly available in ir_datasets but using prebuilt indexes\n",
        "use_cases = {\n",
        "    \"Scientific Research\": {\n",
        "        \"description\": \"Support for scientific literature review, fact verification, and research paper comprehension\",\n",
        "        \"datasets\": [\"beir/trec-covid\", \"beir/scifact\", \"beir/nfcorpus\"],\n",
        "        \"default_dataset\": \"beir/trec-covid\",\n",
        "        \"example_queries\": [\n",
        "            \"What are the most effective treatments for severe COVID-19?\",\n",
        "            \"How does mRNA vaccine technology work?\",\n",
        "            \"What evidence supports aerosol transmission of respiratory viruses?\",\n",
        "            \"What is the relationship between diet and cancer prevention?\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are a scientific research assistant. Provide a clear, accurate, and evidence-based answer to the user's question using only the retrieved documents. Cite supporting information from the context explicitly, and include appropriate scientific context, limitations, and caveats where applicable. Do not speculate beyond the provided material.\"\n",
        "\n",
        "    },\n",
        "    \"Technical Support\": {\n",
        "        \"description\": \"IT helpdesk, programming assistance, and technical knowledge base\",\n",
        "        \"datasets\": [\"beir/cqadupstack/android\", \"beir/cqadupstack/webmasters\", \"beir/cqadupstack/unix\"],\n",
        "        \"default_dataset\": \"beir/cqadupstack/android\",\n",
        "        \"example_queries\": [\n",
        "            \"How do I fix network connectivity issues with Android devices?\",\n",
        "            \"What's causing my app to crash on startup?\",\n",
        "            \"How to implement pagination in a mobile application?\",\n",
        "            \"Best practices for securing an Android device\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are a technical support specialist. Based on the retrieved documents, provide a concise and practical solution to the user's problem. Structure your answer step-by-step, cite relevant technical details, and ensure instructions are clear and executable. Avoid speculation or unsupported suggestions.\"\n",
        "\n",
        "    },\n",
        "    \"Education & Library\": {\n",
        "        \"description\": \"Enhanced research assistance, study materials, and educational content\",\n",
        "        \"datasets\": [\"beir/nfcorpus\", \"beir/natural-questions\", \"beir/hotpotqa\"],\n",
        "        \"default_dataset\": \"beir/natural-questions\",\n",
        "        \"example_queries\": [\n",
        "            \"What teaching methods are most effective for student engagement?\",\n",
        "            \"How did World War II impact economic development in post-war Europe?\",\n",
        "            \"What are the key differences between behaviorist and constructivist learning theories?\",\n",
        "            \"How does cellular respiration relate to photosynthesis?\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are an educational assistant. Using the retrieved documents, provide a well-structured, accurate, and learner-friendly explanation of the topic. Break down complex concepts into simpler terms, include definitions or examples when needed, and ensure your tone is clear, supportive, and informative. Ground your answer entirely in the context provided.\"\n",
        "\n",
        "    },\n",
        "    \"Fact Verification\": {\n",
        "        \"description\": \"Verifying claims and statements across various sources\",\n",
        "        \"datasets\": [\"beir/fever\", \"beir/scifact\", \"beir/climate-fever\"],\n",
        "        \"default_dataset\": \"beir/fever\",\n",
        "        \"example_queries\": [\n",
        "            \"Is climate change primarily caused by human activities?\",\n",
        "            \"Do vaccines cause autism?\",\n",
        "            \"Does vitamin C prevent the common cold?\",\n",
        "            \"Is 5G technology harmful to human health?\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are a fact-checking assistant. Using only the retrieved documents, assess the accuracy of the user's claim. Provide a balanced, evidence-based analysis with references to specific statements or sources from the documents. If the evidence is inconclusive, clearly state the uncertainty and avoid speculation.\"\n",
        "\n",
        "    },\n",
        "    \"Healthcare Information\": {\n",
        "        \"description\": \"Medical literature review, patient education, and clinical guidelines\",\n",
        "        \"datasets\": [\"beir/nfcorpus\", \"beir/trec-covid\"],\n",
        "        \"default_dataset\": \"beir/nfcorpus\",\n",
        "        \"example_queries\": [\n",
        "            \"What is the relationship between diet and heart disease?\",\n",
        "            \"How effective is cognitive behavioral therapy for anxiety disorders?\",\n",
        "            \"What are the potential side effects of statin medications?\",\n",
        "            \"How does family history affect cancer risk assessment?\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are a healthcare information assistant. Based on the retrieved documents, provide an accurate, evidence-based summary addressing the user's question. Do not offer medical advice. Instead, focus on established research findings, clinical guidelines, and clearly state any risks, limitations, or uncertainties in the available evidence. Ground your answer entirely in the context provided.\"\n",
        "    },\n",
        "    \"Campus Info\": {\n",
        "        \"description\": \"Ask questions about services, offices, and procedures listed on the S&T website.\",\n",
        "        \"datasets\": [\"custom_mst_site\"],\n",
        "        \"default_dataset\": \"custom_mst_site\",\n",
        "        \"example_queries\": [\n",
        "            \"Where is the ISSS office located?\",\n",
        "            \"What services are offered to sponsored international students?\",\n",
        "            \"How can I contact the Graduate Studies department?\",\n",
        "            \"Where can I find the event calendar?\"\n",
        "        ],\n",
        "        \"domain_prompt\": \"You are a helpful campus assistant for Missouri S&T. Using only the information available in the retrieved content, answer the user's question clearly and accurately. Reference the exact page or section when possible. Be concise, avoid speculation, and maintain a friendly and professional tone.\"\n",
        "\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lVPeDpm-0l8A",
      "metadata": {
        "id": "lVPeDpm-0l8A"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create dropdowns\n",
        "use_case_dropdown = widgets.Dropdown(\n",
        "    options=list(use_cases.keys()),\n",
        "    description='Use Case:',\n",
        "    value=\"Scientific Research\"\n",
        ")\n",
        "\n",
        "dataset_dropdown = widgets.Dropdown(\n",
        "    options=use_cases[\"Scientific Research\"][\"datasets\"],\n",
        "    description='Dataset:'\n",
        ")\n",
        "\n",
        "# Step 2: Update dataset list when use case changes\n",
        "def update_datasets(change):\n",
        "    new_use_case = change['new']\n",
        "    new_datasets = use_cases[new_use_case][\"datasets\"]\n",
        "    dataset_dropdown.options = new_datasets\n",
        "    dataset_dropdown.value = use_cases[new_use_case][\"default_dataset\"]\n",
        "\n",
        "use_case_dropdown.observe(update_datasets, names='value')\n",
        "\n",
        "# Step 3: Display both dropdowns\n",
        "display(use_case_dropdown, dataset_dropdown)\n",
        "\n",
        "# Step 4: Function to confirm selections and assign variables\n",
        "def confirm_selection(_):\n",
        "    global use_case, selected_dataset\n",
        "    use_case = use_case_dropdown.value\n",
        "    selected_dataset = dataset_dropdown.value\n",
        "\n",
        "    clear_output()\n",
        "    print(f\"‚úÖ Use Case Selected: {use_case}\")\n",
        "    print(f\"‚úÖ Dataset Selected: {selected_dataset}\")\n",
        "    print(f\"\\nDescription:\\n{use_cases[use_case]['description']}\")\n",
        "    print(\"\\nExample queries:\")\n",
        "    for query in use_cases[use_case]['example_queries']:\n",
        "        print(f\"- {query}\")\n",
        "\n",
        "# Step 6: Confirm button\n",
        "confirm_button = widgets.Button(description=\"Confirm Selection\", button_style='success')\n",
        "confirm_button.on_click(confirm_selection)\n",
        "display(confirm_button)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z3DPNnrYOrlC",
      "metadata": {
        "id": "Z3DPNnrYOrlC"
      },
      "source": [
        "Define the paths where pre-built index files should be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6bddba8",
      "metadata": {
        "id": "b6bddba8"
      },
      "outputs": [],
      "source": [
        "base_path = f\"prebuilt_indexes/{selected_dataset.replace('/', '_')}\"\n",
        "corpus_path = f\"{base_path}/corpus.pkl\"\n",
        "embeddings_path = f\"{base_path}/embeddings.npy\"\n",
        "faiss_index_path = f\"{base_path}/faiss_index.bin\"\n",
        "doc_ids_path = f\"{base_path}/doc_ids.pkl\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JETEj6-zOtkV",
      "metadata": {
        "id": "JETEj6-zOtkV"
      },
      "source": [
        "Create the directory if it doesn't exist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "632842f7",
      "metadata": {
        "id": "632842f7"
      },
      "outputs": [],
      "source": [
        "os.makedirs(base_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need to delete the directory structure for some reason, you can use the code below. Otherwise ignore."
      ],
      "metadata": {
        "id": "LoFGTbhiqlR8"
      },
      "id": "LoFGTbhiqlR8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def empty_directory(directory_path):\n",
        "    \"\"\"Remove all files and subdirectories within a directory without deleting the directory itself.\"\"\"\n",
        "    try:\n",
        "        # Check if the directory exists\n",
        "        if not os.path.exists(directory_path):\n",
        "            print(f\"‚ö†Ô∏è Directory doesn't exist: {directory_path}\")\n",
        "            return False\n",
        "\n",
        "        # List all items in the directory\n",
        "        for item in os.listdir(directory_path):\n",
        "            item_path = os.path.join(directory_path, item)\n",
        "\n",
        "            # If it's a file, delete it\n",
        "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "                os.unlink(item_path)\n",
        "                print(f\"  üóëÔ∏è Deleted file: {item}\")\n",
        "\n",
        "            # If it's a directory, delete it and its contents\n",
        "            elif os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "                print(f\"  üóëÔ∏è Deleted subdirectory: {item}\")\n",
        "\n",
        "        print(f\"‚úÖ Successfully emptied directory: {directory_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error emptying directory {directory_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "def delete_directory_safely(directory_path):\n",
        "    \"\"\"Empty a directory and then delete it.\"\"\"\n",
        "    if empty_directory(directory_path):\n",
        "        try:\n",
        "            os.rmdir(directory_path)\n",
        "            print(f\"‚úÖ Successfully deleted directory: {directory_path}\")\n",
        "            return True\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Error deleting directory {directory_path}: {e}\")\n",
        "            return False\n",
        "    return False\n",
        "\n",
        "# Example usage:\n",
        "nested_dir = \"prebuilt_indexes/custom_mst_site/custom_mst_site\"\n",
        "\n",
        "# Option 1: Empty the directory first, then delete it\n",
        "# empty_directory(nested_dir)\n",
        "# os.rmdir(nested_dir)\n",
        "\n",
        "# OR Option 2: Do both operations in one function call\n",
        "delete_directory_safely(base_path)"
      ],
      "metadata": {
        "id": "qbz3DNL5pO8B"
      },
      "id": "qbz3DNL5pO8B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "98_HGPjvO0mM",
      "metadata": {
        "id": "98_HGPjvO0mM"
      },
      "source": [
        "Function to generate sample corpus based on the selected dataset and use case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19931975",
      "metadata": {
        "id": "19931975"
      },
      "outputs": [],
      "source": [
        "def generate_sample_corpus(dataset_name, current_use_case):\n",
        "    \"\"\"Generate a sample corpus with domain-specific content based on the dataset name.\"\"\"\n",
        "    corpus = {}\n",
        "\n",
        "    if current_use_case == \"Scientific Research\":\n",
        "        # Scientific research content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            if \"covid\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"COVID-19 Research Paper {i}\",\n",
        "                    \"text\": f\"This paper investigates the effects of COVID-19 on respiratory health. We conducted a study with {100+i} patients and found significant correlations between viral load and symptom severity. The study suggests that early intervention with antiviral medications may reduce hospitalization rates.\"\n",
        "                }\n",
        "            elif \"scifact\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Scientific Study on Topic {i}\",\n",
        "                    \"text\": f\"Our research demonstrates that hypothesis {i} is supported by experimental evidence. The data shows a statistically significant effect (p<0.05) across multiple trials. These findings contradict previous assumptions and suggest a new mechanism for this phenomenon.\"\n",
        "                }\n",
        "            else:\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Scientific Paper {i}\",\n",
        "                    \"text\": f\"This research examines the relationship between diet and health outcomes. Analysis of data from {500+i} participants shows significant associations between consumption of processed foods and increased risk of chronic diseases. The findings highlight the importance of dietary interventions in public health strategies.\"\n",
        "                }\n",
        "\n",
        "    elif current_use_case == \"Technical Support\":\n",
        "        # Technical support content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            if \"android\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Android Technical Issue {i}\",\n",
        "                    \"text\": f\"Users experiencing battery drain on Android devices should check for apps running in the background. The issue often occurs after system updates or when location services are constantly active. To fix this, go to Settings > Battery > Battery Usage and identify power-hungry applications. Restricting background activity for these apps can significantly improve battery life.\"\n",
        "                }\n",
        "            elif \"webmasters\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Web Development Problem {i}\",\n",
        "                    \"text\": f\"When implementing responsive designs, developers often encounter issues with viewport rendering on mobile devices. To address this, ensure your HTML includes the proper meta viewport tag. Also check that media queries are correctly implemented to handle different screen sizes. Testing across multiple devices is essential to verify responsive behavior.\"\n",
        "                }\n",
        "            else:\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Technical Solution {i}\",\n",
        "                    \"text\": f\"A common error when setting up network connections is incorrect DNS configuration. To troubleshoot, first verify that the DNS server addresses are correct. Then flush the DNS cache to ensure old records aren't causing conflicts. If problems persist, try using alternative DNS servers to determine if the issue is with your ISP's DNS resolution.\"\n",
        "                }\n",
        "\n",
        "    elif current_use_case == \"Education & Library\":\n",
        "        # Educational content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            if \"questions\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Educational Topic {i}\",\n",
        "                    \"text\": f\"This article explains the fundamental concepts of learning theories. Constructivism emphasizes how learners actively build knowledge through experience and reflection, while behaviorism focuses on observable behaviors and environmental conditioning. Understanding these frameworks helps educators design more effective teaching strategies that accommodate different learning styles and cognitive processes.\"\n",
        "                }\n",
        "            elif \"hotpot\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Historical Event {i}\",\n",
        "                    \"text\": f\"The Industrial Revolution transformed economic systems through mechanization and factory production. Beginning in Britain in the late 18th century, it spread throughout Europe and North America, fundamentally changing social structures and labor practices. Technological innovations like the steam engine drove unprecedented growth while creating new challenges related to urbanization, working conditions, and economic inequality.\"\n",
        "                }\n",
        "            else:\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Educational Resource {i}\",\n",
        "                    \"text\": f\"This educational material covers key scientific concepts for secondary education. Topics include cellular biology, chemical reactions, and physical laws. The content is structured to build conceptual understanding through progressive learning objectives, providing examples and applications relevant to students' daily experiences.\"\n",
        "                }\n",
        "\n",
        "    elif current_use_case == \"Fact Verification\":\n",
        "        # Fact verification content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            if \"fever\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Fact Check Article {i}\",\n",
        "                    \"text\": f\"Climate scientists have reached overwhelming consensus that human activities are the primary driver of observed climate change since the mid-20th century. Multiple independent lines of evidence support this conclusion, including atmospheric CO2 measurements, temperature records, and climate model projections. Natural factors alone cannot explain the rapid warming observed in recent decades.\"\n",
        "                }\n",
        "            elif \"scifact\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Scientific Claim Assessment {i}\",\n",
        "                    \"text\": f\"Research has consistently failed to find evidence supporting a causal link between vaccines and autism. Multiple large-scale epidemiological studies involving millions of children have found no association between vaccination and autism spectrum disorders. The original study suggesting this link was retracted due to methodological flaws and ethical violations.\"\n",
        "                }\n",
        "            else:\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Fact Verification {i}\",\n",
        "                    \"text\": f\"Analysis of the claim that 5G technology poses health risks finds insufficient scientific evidence to support this assertion. While all radiation sources deserve study, 5G operates using non-ionizing radiation that lacks sufficient energy to damage cellular DNA. Current research indicates that exposure levels from 5G infrastructure fall well below international safety guidelines.\"\n",
        "                }\n",
        "\n",
        "    elif current_use_case == \"Healthcare Information\":\n",
        "        # Healthcare content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            if \"nfcorpus\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Nutrition Research Study {i}\",\n",
        "                    \"text\": f\"This review examines the relationship between dietary patterns and cardiovascular health. Evidence consistently shows that diets rich in fruits, vegetables, whole grains, and lean proteins are associated with reduced risk of heart disease. Conversely, high consumption of processed foods, saturated fats, and added sugars correlates with increased cardiovascular risk factors including hypertension and elevated cholesterol.\"\n",
        "                }\n",
        "            elif \"covid\" in dataset_name.lower():\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"COVID-19 Treatment Review {i}\",\n",
        "                    \"text\": f\"Clinical trials evaluating antiviral medications for COVID-19 have shown varying degrees of efficacy. Early treatment with certain antivirals may reduce symptom duration and hospitalization risk in high-risk populations. However, treatment effectiveness depends on timing, viral variants, and patient characteristics. Comprehensive management approaches typically include supportive care alongside targeted therapies.\"\n",
        "                }\n",
        "            else:\n",
        "                corpus[doc_id] = {\n",
        "                    \"title\": f\"Medical Research {i}\",\n",
        "                    \"text\": f\"This study examines treatment efficacy for anxiety disorders, comparing cognitive behavioral therapy (CBT) with pharmacological interventions. Meta-analysis of clinical trials indicates that CBT produces outcomes comparable to medication for many patients, with potentially more durable effects after treatment discontinuation. Combined approaches often yield superior results, particularly for severe or treatment-resistant cases.\"\n",
        "                }\n",
        "    elif current_use_case == \"Campus Info\":\n",
        "        # Campus information content\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            corpus[doc_id] = {\n",
        "                \"title\": f\"Missouri S&T Page {i}\",\n",
        "                \"text\": f\"This page contains information about Missouri S&T services and offices. The International Student and Scholar Services (ISSS) office provides support for sponsored international students including visa assistance, cultural activities, and academic guidance. Students can contact the Graduate Studies department for information about graduate programs, thesis requirements, and funding opportunities.\"\n",
        "            }\n",
        "\n",
        "    else:\n",
        "        # Generic content for any other use case\n",
        "        for i in range(20):\n",
        "            doc_id = f\"doc{i}\"\n",
        "            corpus[doc_id] = {\n",
        "                \"title\": f\"Document {i}\",\n",
        "                \"text\": f\"This is sample text for document {i}. It contains information relevant to various queries in this domain. The content includes key facts, explanations, and examples that might be useful for answering questions on this topic.\"\n",
        "            }\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rOIeg38vO5eS",
      "metadata": {
        "id": "rOIeg38vO5eS"
      },
      "source": [
        "Download the files if they don't exist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a472f867",
      "metadata": {
        "id": "a472f867"
      },
      "outputs": [],
      "source": [
        "# --- Path Setup ---\n",
        "repo_folder_name = selected_dataset.replace('/', '_')\n",
        "base_path = f\"prebuilt_indexes/{repo_folder_name}\"\n",
        "os.makedirs(base_path, exist_ok=True) # Create directory if it doesn't exist\n",
        "\n",
        "# List of files expected for an index (Keep this)\n",
        "files_to_download = [\"corpus.pkl\", \"embeddings.npy\", \"faiss_index.bin\", \"doc_ids.pkl\"] # Add qrels.pkl etc. if needed\n",
        "\n",
        "print(f\"Checking/downloading pre-built indexes for {selected_dataset} from HF Hub: {HUB_REPO_ID}...\")\n",
        "\n",
        "# --- Download Loop (Using HF Hub) ---\n",
        "all_files_exist = True\n",
        "for file_name in files_to_download:\n",
        "    local_file_path = os.path.join(base_path, file_name)\n",
        "    if not os.path.exists(local_file_path):\n",
        "        all_files_exist = False # Mark that at least one file needs downloading\n",
        "        print(f\"Downloading {file_name}...\")\n",
        "        try:\n",
        "            # Construct the path *within* the Hub repository\n",
        "            # Assumes you uploaded into folders named like 'beir_trec-covid' etc.\n",
        "            path_in_repo = f\"{repo_folder_name}/{file_name}\"\n",
        "\n",
        "            # Use hf_hub_download\n",
        "            downloaded_path = hf_hub_download(\n",
        "                repo_id=HUB_REPO_ID,\n",
        "                filename=path_in_repo,\n",
        "                repo_type=\"dataset\", # Specify it's a dataset repo\n",
        "                local_dir=base_path, # Download directly into the target folder\n",
        "                local_dir_use_symlinks=False # Avoids potential symlink issues\n",
        "            )\n",
        "            # Double-check file exists at the expected final path\n",
        "            if not os.path.exists(local_file_path):\n",
        "                 if os.path.exists(downloaded_path) and downloaded_path != local_file_path:\n",
        "                     # Move if hf_hub_download placed it slightly differently (rare with local_dir)\n",
        "                     os.rename(downloaded_path, local_file_path)\n",
        "                     print(f\"Moved downloaded file to {local_file_path}\")\n",
        "                 else:\n",
        "                      raise FileNotFoundError(f\"Download failed or file not found at expected path {local_file_path} or download path {downloaded_path} for {file_name}\")\n",
        "\n",
        "            print(f\"Successfully downloaded {file_name} to {local_file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR downloading {file_name} from Hugging Face Hub: {e}\")\n",
        "            print(f\"Check connection and ensure the file exists at 'datasets/{HUB_REPO_ID}/tree/main/{path_in_repo}' on the Hub.\")\n",
        "            # --- Optional: Keep your fallback logic ---\n",
        "            print(f\"‚ö†Ô∏è Creating sample data for {file_name} since download failed\")\n",
        "            # Adapt this fallback logic based on your generate_sample_corpus function and other needs\n",
        "            if file_name == \"corpus.pkl\":\n",
        "                 try:\n",
        "                     # Ensure generate_sample_corpus and use_case are defined earlier\n",
        "                     sample_corpus = generate_sample_corpus(selected_dataset, use_case)\n",
        "                     with open(local_file_path, 'wb') as f: pickle.dump(sample_corpus, f)\n",
        "                 except NameError:\n",
        "                     print(\"generate_sample_corpus or use_case not defined. Cannot create sample data.\")\n",
        "                 except Exception as sample_e:\n",
        "                     print(f\"Error creating sample corpus: {sample_e}\")\n",
        "            elif file_name == \"doc_ids.pkl\":\n",
        "                 try:\n",
        "                     sample_doc_ids = [f\"doc{i}\" for i in range(20)]\n",
        "                     with open(local_file_path, 'wb') as f: pickle.dump(sample_doc_ids, f)\n",
        "                 except Exception as sample_e:\n",
        "                     print(f\"Error creating sample doc_ids: {sample_e}\")\n",
        "            elif file_name == \"embeddings.npy\":\n",
        "                 try:\n",
        "                     sample_embeddings = np.random.rand(20, 384).astype(np.float32)\n",
        "                     np.save(local_file_path, sample_embeddings)\n",
        "                 except Exception as sample_e:\n",
        "                     print(f\"Error creating sample embeddings: {sample_e}\")\n",
        "            elif file_name == \"faiss_index.bin\":\n",
        "                 try:\n",
        "                     sample_index = faiss.IndexFlatL2(384); sample_index.add(np.random.rand(20, 384).astype(np.float32))\n",
        "                     faiss.write_index(sample_index, local_file_path)\n",
        "                 except Exception as sample_e:\n",
        "                     print(f\"Error creating sample faiss_index: {sample_e}\")\n",
        "            # --- End Optional Fallback ---\n",
        "\n",
        "# Optional check message\n",
        "if all_files_exist:\n",
        "    print(\"All required index files already exist locally.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e58b58e5",
      "metadata": {
        "id": "e58b58e5"
      },
      "outputs": [],
      "source": [
        "print(f\"Loading pre-built indexes for {selected_dataset}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1vJ7Q4eRPPDX",
      "metadata": {
        "id": "1vJ7Q4eRPPDX"
      },
      "source": [
        "Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ce835e",
      "metadata": {
        "id": "79ce835e"
      },
      "outputs": [],
      "source": [
        "print(\"Loading document corpus...\")\n",
        "with open(corpus_path, 'rb') as f:\n",
        "    corpus = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jqm_3NxMPRiJ",
      "metadata": {
        "id": "Jqm_3NxMPRiJ"
      },
      "source": [
        "Load document IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1eaa94",
      "metadata": {
        "id": "ac1eaa94"
      },
      "outputs": [],
      "source": [
        "print(\"Loading document IDs...\")\n",
        "with open(doc_ids_path, 'rb') as f:\n",
        "    doc_ids = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qTz7sMV0PTdc",
      "metadata": {
        "id": "qTz7sMV0PTdc"
      },
      "source": [
        "Load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85d00b9a",
      "metadata": {
        "id": "85d00b9a"
      },
      "outputs": [],
      "source": [
        "print(\"Loading document embeddings...\")\n",
        "doc_embeddings = np.load(embeddings_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XTjUnCxgPV63",
      "metadata": {
        "id": "XTjUnCxgPV63"
      },
      "source": [
        "Load FAISS index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179727a3",
      "metadata": {
        "id": "179727a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Loading FAISS index...\")\n",
        "index = faiss.read_index(faiss_index_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07900094",
      "metadata": {
        "id": "07900094"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(f\"Successfully loaded pre-built indexes for {selected_dataset}\")\n",
        "    print(f\"Corpus size: {len(corpus)} documents\")\n",
        "    print(f\"Embeddings shape: {doc_embeddings.shape}\")\n",
        "    print(f\"FAISS index size: {index.ntotal} vectors\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error building indexes: {e}\")\n",
        "    print(f\"‚ö†Ô∏è Creating sample data for demonstration\")\n",
        "\n",
        "    # Create sample corpus\n",
        "    corpus = {}\n",
        "    for i in range(100):\n",
        "        doc_id = f\"doc{i}\"\n",
        "        corpus[doc_id] = {\n",
        "            'title': f\"Sample Document {i}\",\n",
        "            'text': f\"This is sample text for document {i} related to {use_case}. It contains information relevant to various queries in this domain. The content includes key facts, explanations, and examples that might be useful for answering questions on this topic.\"\n",
        "        }\n",
        "\n",
        "    doc_ids = list(corpus.keys())\n",
        "\n",
        "    # Create sample embeddings\n",
        "    doc_embeddings = np.random.rand(len(doc_ids), 384).astype(np.float32)\n",
        "    faiss.normalize_L2(doc_embeddings)\n",
        "\n",
        "    # Create FAISS index\n",
        "    index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
        "    index.add(doc_embeddings)\n",
        "\n",
        "    # Save sample data\n",
        "    with open(corpus_path, 'wb') as f:\n",
        "        pickle.dump(corpus, f)\n",
        "\n",
        "    with open(doc_ids_path, 'wb') as f:\n",
        "        pickle.dump(doc_ids, f)\n",
        "\n",
        "    np.save(embeddings_path, doc_embeddings)\n",
        "    faiss.write_index(index, faiss_index_path)\n",
        "\n",
        "    print(f\"‚úÖ Created sample data with {len(corpus)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Upz5gD_gP0Jm",
      "metadata": {
        "id": "Upz5gD_gP0Jm"
      },
      "source": [
        "Load queries and qrels from ir_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Fix the nested directory structure\n",
        "def fix_nested_directories(dataset_name):\n",
        "    parent_dir = f\"prebuilt_indexes/{dataset_name}\"\n",
        "    nested_dir = f\"{parent_dir}/{dataset_name}\"\n",
        "\n",
        "    # Check if the nested structure exists\n",
        "    if os.path.exists(nested_dir) and os.path.isdir(nested_dir):\n",
        "        print(f\"üîÑ Fixing nested directory structure for {dataset_name}...\")\n",
        "\n",
        "        # Move all files from nested dir to parent dir\n",
        "        for item in os.listdir(nested_dir):\n",
        "            src = os.path.join(nested_dir, item)\n",
        "            dst = os.path.join(parent_dir, item)\n",
        "            if not os.path.exists(dst):  # Avoid overwriting existing files\n",
        "                shutil.move(src, dst)\n",
        "                print(f\"  ‚Ü™ Moved {item} to parent directory\")\n",
        "\n",
        "        # Remove the now-empty nested directory\n",
        "        try:\n",
        "            os.rmdir(nested_dir)\n",
        "            print(f\"‚úÖ Removed empty nested directory\")\n",
        "        except OSError:\n",
        "            print(f\"‚ö†Ô∏è Couldn't remove nested directory - it may not be empty\")\n",
        "    else:\n",
        "        print(f\"‚úì No nested directory structure found for {dataset_name}\")\n",
        "\n",
        "# Call this function before trying to access the files\n",
        "fix_nested_directories(\"custom_mst_site\")"
      ],
      "metadata": {
        "id": "cKnqkTP5n_dY"
      },
      "id": "cKnqkTP5n_dY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c55c178",
      "metadata": {
        "id": "3c55c178"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def download_custom_dataset_files(repo_id, dataset_name):\n",
        "    \"\"\"\n",
        "    Download required files for custom dataset from Hugging Face\n",
        "    \"\"\"\n",
        "    base_path = f\"prebuilt_indexes/{dataset_name}\"\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "    print(f\"üì• Downloading files for {dataset_name} from Hugging Face...\")\n",
        "\n",
        "    # List of files to download\n",
        "    files_to_download = [\n",
        "        \"topics.json\",\n",
        "        \"auto_qrels.txt\"\n",
        "    ]\n",
        "\n",
        "    downloaded_files = []\n",
        "    for filename in files_to_download:\n",
        "        try:\n",
        "            # Download file from HF\n",
        "            # Use hf_hub_download\n",
        "\n",
        "            filepath = hf_hub_download(\n",
        "                repo_id=HUB_REPO_ID,\n",
        "                filename=f\"{dataset_name}/{filename}\",\n",
        "                repo_type=\"dataset\",\n",
        "                local_dir=base_path,\n",
        "                local_dir_use_symlinks=False\n",
        "            )\n",
        "\n",
        "            # Copy to our target directory\n",
        "            target_path = os.path.join(base_path, filename)\n",
        "            shutil.copy(filepath, target_path)\n",
        "            downloaded_files.append(filename)\n",
        "            print(f\"  ‚úÖ Downloaded {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Failed to download {filename}: {e}\")\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "def load_custom_topics_json(json_path):\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    topics_list = data[\"topics\"] if \"topics\" in data else data\n",
        "    queries = {}\n",
        "    for topic in topics_list:\n",
        "        topic_id = str(topic.get(\"id\") or topic.get(\"number\"))\n",
        "        queries[topic_id] = {\n",
        "            \"id\": topic_id,\n",
        "            \"text\": topic.get(\"title\", \"\").strip(),\n",
        "            \"description\": topic.get(\"description\", \"\").strip(),\n",
        "            \"narrative\": topic.get(\"narrative\", \"\").strip()\n",
        "        }\n",
        "    print(f\"‚úÖ Loaded {len(queries)} topics from JSON\")\n",
        "    return queries\n",
        "\n",
        "def load_custom_qrels_txt(qrels_path):\n",
        "    qrels_dict = {}\n",
        "    with open(qrels_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 4:\n",
        "                topic_id, _, doc_id, relevance = parts\n",
        "                if topic_id not in qrels_dict:\n",
        "                    qrels_dict[topic_id] = {}\n",
        "                qrels_dict[topic_id][doc_id] = int(relevance)\n",
        "    print(f\"‚úÖ Loaded qrels for {len(qrels_dict)} topics\")\n",
        "    return qrels_dict\n",
        "\n",
        "def select_sample_queries(queries, qrels_dict, max_queries=3):\n",
        "    sample = {}\n",
        "    for topic_id, query_info in queries.items():\n",
        "        if topic_id in qrels_dict:\n",
        "            sample[topic_id] = query_info\n",
        "        if len(sample) >= max_queries:\n",
        "            break\n",
        "    return sample\n",
        "\n",
        "try:\n",
        "    if selected_dataset == \"custom_mst_site\":\n",
        "        print(\"üì¶ Loading queries and qrels for custom MST dataset...\")\n",
        "\n",
        "        base_path = \"prebuilt_indexes/custom_mst_site\"\n",
        "        topics_path = os.path.join(base_path, \"topics.json\")\n",
        "        qrels_path = os.path.join(base_path, \"auto_qrels.txt\")\n",
        "\n",
        "        # Fix directory structure if needed\n",
        "        # fix_nested_directories(\"custom_mst_site\")\n",
        "\n",
        "        # Check if required files exist, download if not\n",
        "        if not (os.path.exists(topics_path) and os.path.exists(qrels_path)):\n",
        "            print(\"üîç Required files not found locally, downloading from Hugging Face...\")\n",
        "            # Replace \"your-hf-repo-id\" with the actual Hugging Face repository ID\n",
        "            download_custom_dataset_files(\"your-hf-repo-id\", \"custom_mst_site\")\n",
        "\n",
        "        # Now try to load the files\n",
        "        queries = load_custom_topics_json(topics_path)\n",
        "        qrels_dict = load_custom_qrels_txt(qrels_path)\n",
        "        sample_queries = select_sample_queries(queries, qrels_dict)\n",
        "\n",
        "        print(f\"‚úÖ Selected {len(sample_queries)} sample queries for demonstration\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Fall back to ir_datasets version\n",
        "        import ir_datasets\n",
        "        print(\"üì• Loading queries and relevance judgments from ir_datasets...\")\n",
        "        dataset = ir_datasets.load(selected_dataset)\n",
        "\n",
        "        queries = {}\n",
        "        qrels_dict = {}\n",
        "\n",
        "        try:\n",
        "            next(dataset.queries_iter())\n",
        "            for query in dataset.queries_iter():\n",
        "                query_id = query.query_id if hasattr(query, 'query_id') else getattr(query, '_id', f'q{len(queries)}')\n",
        "                if hasattr(query, 'text'):\n",
        "                    query_text = query.text\n",
        "                elif hasattr(query, 'title'):\n",
        "                    query_text = query.title\n",
        "                elif hasattr(query, 'query'):\n",
        "                    query_text = query.query\n",
        "                else:\n",
        "                    query_text = \"\"\n",
        "                    for field in dir(query):\n",
        "                        if not field.startswith('_') and isinstance(getattr(query, field), str) and field != 'query_id':\n",
        "                            query_text = getattr(query, field)\n",
        "                            break\n",
        "\n",
        "                queries[query_id] = {'text': query_text, 'id': query_id}\n",
        "                if len(queries) >= 100:\n",
        "                    break\n",
        "            print(f\"‚úÖ Loaded {len(queries)} queries from ir_datasets\")\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è No queries found in ir_datasets\")\n",
        "\n",
        "        try:\n",
        "            next(dataset.qrels_iter())\n",
        "            for qrel in dataset.qrels_iter():\n",
        "                if not hasattr(qrel, 'query_id') or not hasattr(qrel, 'doc_id') or not hasattr(qrel, 'relevance'):\n",
        "                    continue\n",
        "                if qrel.query_id not in qrels_dict:\n",
        "                    qrels_dict[qrel.query_id] = {}\n",
        "                qrels_dict[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
        "            print(f\"‚úÖ Loaded relevance judgments for {len(qrels_dict)} queries\")\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è No relevance judgments found in ir_datasets\")\n",
        "\n",
        "        sample_queries = {}\n",
        "        if queries and qrels_dict:\n",
        "            for query_id, query_info in queries.items():\n",
        "                if query_id in qrels_dict and len(sample_queries) < 3:\n",
        "                    sample_queries[query_id] = query_info\n",
        "        if not sample_queries and queries:\n",
        "            for i, (query_id, query_info) in enumerate(queries.items()):\n",
        "                if i < 3:\n",
        "                    sample_queries[query_id] = query_info\n",
        "\n",
        "        print(f\"‚úÖ Selected {len(sample_queries)} sample queries for demonstration\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error loading queries and qrels:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28VwLFi1P3yw",
      "metadata": {
        "id": "28VwLFi1P3yw"
      },
      "source": [
        "## Initialize retrieval models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rb2aGb0sP7W7",
      "metadata": {
        "id": "Rb2aGb0sP7W7"
      },
      "source": [
        "Load bi-encoder model for query encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "373ef42b",
      "metadata": {
        "id": "373ef42b"
      },
      "outputs": [],
      "source": [
        "\n",
        "biencoder_model_name = \"sentence-transformers/msmarco-distilbert-base-v3\" # @param [\"sentence-transformers/msmarco-distilbert-base-v3\", \"sentence-transformers/all-mpnet-base-v2\", \"sentence-transformers/all-MiniLM-L6-v2\"]\n",
        "bi_encoder = SentenceTransformer(biencoder_model_name)\n",
        "bi_encoder.to(device)\n",
        "print(f\"Loaded bi-encoder model: {biencoder_model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FwUkrIc7P9Pv",
      "metadata": {
        "id": "FwUkrIc7P9Pv"
      },
      "source": [
        "Load cross-encoder model for reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda9dcce",
      "metadata": {
        "id": "dda9dcce"
      },
      "outputs": [],
      "source": [
        "\n",
        "crossencoder_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\" # @param [\"cross-encoder/ms-marco-MiniLM-L-6-v2\", \"cross-encoder/ms-marco-MiniLM-L-12-v2\", \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"]\n",
        "cross_encoder = CrossEncoder(crossencoder_model_name)\n",
        "cross_encoder.to(device)\n",
        "print(f\"Loaded cross-encoder model: {crossencoder_model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FIJv1VzZP_EG",
      "metadata": {
        "id": "FIJv1VzZP_EG"
      },
      "source": [
        "Load LLM for answer generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ec1d94",
      "metadata": {
        "id": "12ec1d94"
      },
      "outputs": [],
      "source": [
        "# Set your Mistral API key (preferably from environment variable)\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"abcdef\"  # Replace with your actual key\n",
        "print(\"Mistral API configured and ready to use\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tsfqSQo2QE6l",
      "metadata": {
        "id": "tsfqSQo2QE6l"
      },
      "source": [
        "## üìè Domain-Specific Evaluation: Beyond Standard Metrics\n",
        "\n",
        "Understanding how to evaluate RAG systems properly is critical for real-world applications. Different domains require different evaluation approaches:\n",
        "\n",
        "### Scientific Research Evaluation\n",
        "- **Factual Accuracy**: Are all scientific claims supported by the retrieved documents?\n",
        "- **Citation Quality**: Does the system properly attribute information to sources?\n",
        "- **Uncertainty Handling**: Does the response appropriately express limitations and caveats?\n",
        "\n",
        "### Technical Support Evaluation\n",
        "- **Actionability**: Can a user follow the instructions without additional information?\n",
        "- **Correctness**: Do the steps actually resolve the technical issue?\n",
        "- **Safety**: Are proper warnings included for potentially harmful operations?\n",
        "\n",
        "### Educational Content Evaluation\n",
        "- **Comprehensibility**: Is the content appropriate for the intended education level?\n",
        "- **Scaffolding**: Does the explanation build concepts in a logical order?\n",
        "- **Engagement**: Does the content use appropriate examples and explanations?\n",
        "\n",
        "### Healthcare Information Evaluation\n",
        "- **Clinical Accuracy**: Is the medical information correct and up-to-date?\n",
        "- **Completeness**: Are important warnings, contraindications, or limitations mentioned?\n",
        "- **Clarity**: Is medical terminology appropriately explained?\n",
        "\n",
        "### Implementing Custom Evaluation\n",
        "In this tutorial, we use standard IR metrics (precision, recall, etc.) augmented with domain-specific considerations. In production systems, consider implementing:\n",
        "\n",
        "1. Human-in-the-loop evaluation pipelines\n",
        "2. Domain expert review for critical applications\n",
        "3. Automated checks for domain-specific requirements\n",
        "\n",
        "Run the next cell to see how we can implement custom evaluation metrics:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def evaluate_scientific_rag(query, retrieved_docs, generated_answer, ground_truth=None):\n",
        "    \"\"\"Evaluate a scientific RAG response with domain-specific metrics\"\"\"\n",
        "\n",
        "    # 1. Check for citations in the answer\n",
        "    citation_pattern = r'\\[(\\d+)\\]|\\(([A-Za-z\\s]+,\\s*\\d{4})\\)'\n",
        "    has_citations = bool(re.search(citation_pattern, generated_answer))\n",
        "\n",
        "    # 2. Check if answer mentions limitations or uncertainty\n",
        "    uncertainty_terms = ['may', 'might', 'could', 'possibly', 'suggests',\n",
        "                         'limited evidence', 'more research', 'not conclusive']\n",
        "    has_uncertainty = any(term in generated_answer.lower() for term in uncertainty_terms)\n",
        "\n",
        "    # 3. Check relevance of retrieved documents (simplified)\n",
        "    # In practice, this might use a trained classifier or human evaluation\n",
        "    relevance_score = sum(doc['cross_score'] for doc in retrieved_docs) / len(retrieved_docs)\n",
        "\n",
        "    # 4. Calculate a combined scientific quality score (example only)\n",
        "    scientific_quality = (\n",
        "        (2 if has_citations else 0) +\n",
        "        (1 if has_uncertainty else 0) +\n",
        "        min(2, relevance_score / 3)  # Scale to 0-2 range\n",
        "    ) / 5  # Normalize to 0-1\n",
        "\n",
        "    # Return evaluation results\n",
        "    return {\n",
        "        'has_citations': has_citations,\n",
        "        'acknowledges_uncertainty': has_uncertainty,\n",
        "        'avg_relevance_score': relevance_score,\n",
        "        'scientific_quality_score': scientific_quality,\n",
        "    }\n",
        "\n",
        "# Example usage with a mock result\n",
        "example_query = \"What is the effectiveness of remdesivir for COVID-19?\"\n",
        "example_docs = [{\"cross_score\": 7.5}, {\"cross_score\": 6.8}, {\"cross_score\": 6.2}]\n",
        "example_answer = \"Studies suggest remdesivir may improve recovery time in some COVID-19 patients, though more research is needed to confirm its effectiveness across different patient populations [1].\"\n",
        "\n",
        "scientific_eval = evaluate_scientific_rag(\n",
        "    example_query,\n",
        "    example_docs,\n",
        "    example_answer\n",
        ")\n",
        "\n",
        "print(\"Scientific Domain Evaluation:\")\n",
        "for metric, value in scientific_eval.items():\n",
        "    if isinstance(value, bool):\n",
        "        print(f\"- {metric}: {'‚úÖ Yes' if value else '‚ùå No'}\")\n",
        "    elif isinstance(value, float):\n",
        "        print(f\"- {metric}: {value:.2f}\")"
      ],
      "metadata": {
        "id": "K9V3w0dKOR4R"
      },
      "id": "K9V3w0dKOR4R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Domain-Specific Evaluation in Practice\n",
        "\n",
        "For each domain, you would develop appropriate evaluation metrics:\n",
        "\n",
        "1. **Scientific Domain**: Citation practices, uncertainty acknowledgment, evidence quality\n",
        "2. **Technical Support**: Step clarity, command accuracy, safety considerations\n",
        "3. **Educational Content**: Grade-level appropriateness, concept scaffolding, engagement\n",
        "4. **Healthcare Information**: Clinical accuracy, completeness of warnings, clarity\n",
        "\n",
        "These custom evaluations can be:\n",
        "- **Automated**: For measurable aspects like presence of citations\n",
        "- **Semi-automated**: Using classifiers trained on expert-labeled examples\n",
        "- **Human-in-the-loop**: Expert review for critical applications\n",
        "\n",
        "In production RAG systems, combining standard IR metrics with these domain-specific evaluations provides a more complete picture of system performance."
      ],
      "metadata": {
        "id": "fhqLBPYmO57M"
      },
      "id": "fhqLBPYmO57M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b69d73a8",
      "metadata": {
        "id": "b69d73a8"
      },
      "outputs": [],
      "source": [
        "def get_domain_metrics(domain):\n",
        "    \"\"\"Return appropriate evaluation metrics for the selected domain.\"\"\"\n",
        "    # Base metrics for all domains\n",
        "    base_metrics = [\n",
        "        nDCG@5, P@5, R@5, AP\n",
        "    ]\n",
        "\n",
        "    # Domain-specific additional metrics\n",
        "    domain_specific = {\n",
        "        \"Scientific Research\": [\n",
        "            nDCG@10,         # More comprehensive literature review (deeper results)\n",
        "            R@10,            # Higher recall important for research\n",
        "            RR               # First relevant result important for fact-checking\n",
        "        ],\n",
        "        \"Technical Support\": [\n",
        "            RR,              # First relevant result critical for troubleshooting\n",
        "            P@1, P@3,        # Precision at very top results important\n",
        "            Rprec            # Balances precision/recall\n",
        "        ],\n",
        "        \"Education & Library\": [\n",
        "            nDCG@10,         # Students often explore more results\n",
        "            RBP(p=0.8),      # Models patience of students browsing results\n",
        "            Judged@10        # Coverage of assessment for educational content\n",
        "        ],\n",
        "        \"Legal Document Analysis\": [\n",
        "            RBP(p=0.9),      # Very high precision focus (legal requires accuracy)\n",
        "            nDCG@20,         # May need to review many documents for legal research\n",
        "            infAP            # Handles incomplete judgments common in legal collections\n",
        "        ],\n",
        "        \"Healthcare Information\": [\n",
        "            P@1, P@3,        # Top results critical for medical information\n",
        "            RR,              # First relevant result important for clinical questions\n",
        "            ERR              # Models utility with diminishing returns\n",
        "        ],\n",
        "        \"Campus Info\": [\n",
        "            P@1, P@3,        # Precise information location important\n",
        "            RR,              # First relevant result critical\n",
        "            nDCG@5           # Overall relevance ranking\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return base_metrics + domain_specific.get(domain, [])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11yqWpgqQI2a",
      "metadata": {
        "id": "11yqWpgqQI2a"
      },
      "source": [
        "## Define RAG functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218ecdc0",
      "metadata": {
        "id": "218ecdc0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def retrieve_documents(query, top_k_first_stage=100, top_k_reranked=5):\n",
        "    \"\"\"\n",
        "    Two-stage retrieval:\n",
        "    1. Retrieve top_k_first_stage documents using pre-built FAISS index\n",
        "    2. Rerank with cross-encoder to get final top_k_reranked\n",
        "    \"\"\"\n",
        "    # 1. First-stage retrieval with biencoder + FAISS\n",
        "    # Encode the query using the bi-encoder model\n",
        "    query_embedding = bi_encoder.encode(query, show_progress_bar=False, convert_to_numpy=True)\n",
        "    query_embedding = np.array([query_embedding], dtype=np.float32)\n",
        "    faiss.normalize_L2(query_embedding)  # Normalize for cosine similarity\n",
        "\n",
        "    # Search the FAISS index\n",
        "    scores, indices = index.search(query_embedding, k=min(top_k_first_stage, index.ntotal))\n",
        "\n",
        "    # 2. Reranking with cross-encoder\n",
        "    cross_encoder_candidates = []\n",
        "    for idx in indices[0]:  # indices comes as a 2D array\n",
        "        doc_id = doc_ids[idx]\n",
        "        doc_info = corpus[doc_id]\n",
        "        title = doc_info.get('title', '')\n",
        "        text = doc_info['text']\n",
        "        combined_text = f\"{title}. {text}\" if title else text\n",
        "        cross_encoder_candidates.append([query, combined_text])\n",
        "\n",
        "    # Score with cross-encoder\n",
        "    cross_scores = cross_encoder.predict(cross_encoder_candidates)\n",
        "\n",
        "    # Sort by cross-encoder scores\n",
        "    cross_results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        doc_id = doc_ids[idx]\n",
        "        biencoder_score = float(scores[0][i])  # Convert from numpy float to Python float\n",
        "        cross_score = float(cross_scores[i])   # Ensure Python float\n",
        "        cross_results.append((doc_id, biencoder_score, cross_score))\n",
        "\n",
        "    # Sort by cross-encoder score\n",
        "    cross_results = sorted(cross_results, key=lambda x: x[2], reverse=True)[:top_k_reranked]\n",
        "\n",
        "    # Format final results\n",
        "    results = []\n",
        "    for doc_id, biencoder_score, cross_score in cross_results:\n",
        "        doc_info = corpus[doc_id]\n",
        "        title = doc_info.get('title', '')\n",
        "        text = doc_info['text']\n",
        "        combined_text = f\"{title}. {text}\" if title else text\n",
        "        results.append({\n",
        "            'doc_id': doc_id,\n",
        "            'biencoder_score': biencoder_score,\n",
        "            'cross_score': cross_score,\n",
        "            'text': combined_text\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Mistral API function for answer generation\n",
        "def generate_answer(query, context, domain_prompt=\"\", max_length=500):\n",
        "    \"\"\"Generate an answer using the Mistral API based on retrieved documents and domain-specific prompting\"\"\"\n",
        "\n",
        "    # You'll need to get an API key from Mistral AI and set it as an environment variable\n",
        "    # or replace the line below with your actual API key\n",
        "    api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"MISTRAL_API_KEY environment variable not set. Please set it before calling this function.\")\n",
        "\n",
        "    # Mistral API endpoint\n",
        "    api_url = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "\n",
        "    # Create the messages list\n",
        "    messages = []\n",
        "\n",
        "    # Add system message with domain-specific prompt if provided\n",
        "    if domain_prompt:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": domain_prompt.strip()\n",
        "        })\n",
        "\n",
        "    # Add user message with context and query\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Context:\\n{context.strip()}\\n\\nQuestion: {query.strip()}\"\n",
        "    })\n",
        "\n",
        "    # Prepare the API request payload\n",
        "    payload = {\n",
        "        \"model\": \"mistral-large-latest\",  # You can change to other Mistral models as needed\n",
        "        \"messages\": messages,\n",
        "        \"max_tokens\": max_length,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.9,\n",
        "    }\n",
        "\n",
        "    # Set up headers with API key\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Make the API request\n",
        "        response = requests.post(api_url, headers=headers, json=payload)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        # Parse the JSON response\n",
        "        response_data = response.json()\n",
        "\n",
        "        # Extract the generated text\n",
        "        generated_text = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Mistral API: {e}\")\n",
        "        # Fallback to a simple response if API call fails\n",
        "        return f\"I couldn't generate a response using the Mistral API due to an error: {str(e)}\""
      ],
      "metadata": {
        "id": "njBHKpuj_s7q"
      },
      "id": "njBHKpuj_s7q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f4311d",
      "metadata": {
        "id": "d2f4311d"
      },
      "outputs": [],
      "source": [
        "def run_rag_pipeline(query, query_id=None, top_k_first_stage=100, top_k_reranked=5, domain_prompt=\"\", show_timings=False):\n",
        "    \"\"\"Run the full RAG pipeline with Mistral API for generation\"\"\"\n",
        "    # Timing dictionary\n",
        "    timings = {}\n",
        "\n",
        "    # 1. Retrieve and rerank documents\n",
        "    start_time = time.time()\n",
        "    retrieved_docs = retrieve_documents(query, top_k_first_stage, top_k_reranked)\n",
        "    timings['retrieval'] = time.time() - start_time\n",
        "\n",
        "    # 2. Format context for LLM\n",
        "    context = \"\"\n",
        "    for i, doc in enumerate(retrieved_docs):\n",
        "        context += f\"Document {i+1} [Score: {doc['cross_score']:.3f}]:\\n{doc['text']}\\n\\n\"\n",
        "\n",
        "    # 3. Generate answer with Mistral API\n",
        "    start_time = time.time()\n",
        "    answer = generate_answer(query, context, domain_prompt)\n",
        "    timings['generation'] = time.time() - start_time\n",
        "\n",
        "    # Total time\n",
        "    timings['total'] = timings['retrieval'] + timings['generation']\n",
        "\n",
        "    # Create run (for ir_measures evaluation)\n",
        "    run = {}\n",
        "    if query_id is not None:\n",
        "        run[query_id] = {doc['doc_id']: float(doc['cross_score']) for doc in retrieved_docs}\n",
        "\n",
        "    return retrieved_docs, answer, timings, run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d791376f",
      "metadata": {
        "id": "d791376f"
      },
      "outputs": [],
      "source": [
        "def evaluate_with_ir_measures(run, qrels, metrics_list=None):\n",
        "    \"\"\"\n",
        "    Evaluate a run using ir_measures library\n",
        "\n",
        "    Args:\n",
        "        run: Dict mapping query_id -> {doc_id -> score}\n",
        "        qrels: Dict mapping query_id -> {doc_id -> relevance}\n",
        "        metrics_list: List of ir_measures metric objects (default: common metrics)\n",
        "\n",
        "    Returns:\n",
        "        Dict of metric results\n",
        "    \"\"\"\n",
        "    if metrics_list is None:\n",
        "        # Define default metrics to evaluate\n",
        "        metrics_list = [\n",
        "            nDCG@5, nDCG@10,       # Normalized Discounted Cumulative Gain\n",
        "            P@5, P@10,             # Precision at k\n",
        "            R@5, R@10,             # Recall at k\n",
        "            AP,                    # Average Precision\n",
        "            RR                      # Reciprocal Rank\n",
        "        ]\n",
        "\n",
        "    # Calculate aggregate metrics\n",
        "    try:\n",
        "        results = ir_measures.calc_aggregate(metrics_list, qrels, run)\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {e}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12047b0b",
      "metadata": {
        "id": "12047b0b"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "def display_rag_results(query, retrieved_docs, answer, timings=None, metrics=None, use_case=None):\n",
        "    \"\"\"Display RAG results in a formatted way with domain-specific formatting\"\"\"\n",
        "\n",
        "    display(HTML(f\"<h2 style='color:#eee;'>RAG Results for {use_case if use_case else 'Query'}</h2>\"))\n",
        "    display(HTML(f\"<h3 style='color:#ccc;'>üìù Query: {query}</h3>\"))\n",
        "\n",
        "    # Display metrics if available\n",
        "    if metrics:\n",
        "        display(HTML(\"<h3 style='color:#ccc;'>üìä Retrieval Metrics:</h3>\"))\n",
        "        metrics_html = \"<table style='color:#ddd; border-collapse: collapse;'>\"\n",
        "        metrics_html += \"<tr><th style='padding: 4px 10px;'>Metric</th><th style='padding: 4px 10px;'>Value</th></tr>\"\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            metrics_html += f\"<tr><td style='padding: 4px 10px;'>{metric_name}</td><td style='padding: 4px 10px;'>{metric_value:.4f}</td></tr>\"\n",
        "        metrics_html += \"</table>\"\n",
        "        display(HTML(metrics_html))\n",
        "\n",
        "    # Display retrieved documents\n",
        "    display(HTML(\"<h3 style='color:#ccc;'>üìö Retrieved Documents:</h3>\"))\n",
        "    for i, doc in enumerate(retrieved_docs):\n",
        "        display(Markdown(f\"**Document {i+1}:**\"))\n",
        "        display(Markdown(f\"- **Biencoder Score:** {doc['biencoder_score']:.3f}\"))\n",
        "        display(Markdown(f\"- **Cross-Encoder Score:** {doc['cross_score']:.3f}\"))\n",
        "        display(Markdown(f\"- **Document ID:** {doc['doc_id']}\"))\n",
        "        display(Markdown(f\"- **Text:** {doc['text'][:300]}...\"))\n",
        "        print()\n",
        "\n",
        "    # Display generated answer\n",
        "    display(HTML(\"<h3 style='color:#ccc;'>ü§ñ Generated Answer:</h3>\"))\n",
        "\n",
        "    # Apply consistent dark-mode-friendly, domain-specific styling\n",
        "    color_map = {\n",
        "        \"Scientific Research\": \"#3498db\",\n",
        "        \"Technical Support\": \"#2ecc71\",\n",
        "        \"Education & Library\": \"#f39c12\",\n",
        "        \"Legal Document Analysis\": \"#9b59b6\",\n",
        "        \"Healthcare Information\": \"#e74c3c\",\n",
        "        \"Campus Info\": \"#16a085\"\n",
        "    }\n",
        "    border_color = color_map.get(use_case, \"#7f8c8d\")  # default gray\n",
        "\n",
        "    answer_html = f'''\n",
        "    <div style=\"\n",
        "        border-left: 5px solid {border_color};\n",
        "        padding: 12px 16px;\n",
        "        margin: 10px 0;\n",
        "        background-color: #1e1e1e;\n",
        "        color: #e0e0e0;\n",
        "        border-radius: 8px;\n",
        "        font-size: 15px;\n",
        "        line-height: 1.6;\">\n",
        "        {answer}\n",
        "    </div>\n",
        "    '''\n",
        "\n",
        "    display(HTML(answer_html))\n",
        "\n",
        "    # Display timings if available\n",
        "    if timings:\n",
        "        display(HTML(\"<h3 style='color:#ccc;'>‚è±Ô∏è Performance Metrics:</h3>\"))\n",
        "        timings_html = \"<table style='color:#ddd; border-collapse: collapse;'>\"\n",
        "        timings_html += \"<tr><th style='padding: 4px 10px;'>Metric</th><th style='padding: 4px 10px;'>Time (seconds)</th></tr>\"\n",
        "        timings_html += f\"<tr><td style='padding: 4px 10px;'>Retrieval time</td><td style='padding: 4px 10px;'>{timings['retrieval']:.3f}</td></tr>\"\n",
        "        timings_html += f\"<tr><td style='padding: 4px 10px;'>Generation time</td><td style='padding: 4px 10px;'>{timings['generation']:.3f}</td></tr>\"\n",
        "        timings_html += f\"<tr><td style='padding: 4px 10px;'>Total RAG time</td><td style='padding: 4px 10px;'>{timings['total']:.3f}</td></tr>\"\n",
        "        timings_html += \"</table>\"\n",
        "        display(HTML(timings_html))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ghNuIbt1QX1b",
      "metadata": {
        "id": "ghNuIbt1QX1b"
      },
      "source": [
        "## Run RAG on Sample Queries for the Selected Use Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f848de",
      "metadata": {
        "id": "76f848de"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n=== Sample Queries from {use_case} Use Case ===\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wy70LBlPQlsP",
      "metadata": {
        "id": "Wy70LBlPQlsP"
      },
      "source": [
        "Get domain-specific prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c467244",
      "metadata": {
        "id": "2c467244"
      },
      "outputs": [],
      "source": [
        "domain_prompt = use_cases[use_case][\"domain_prompt\"]\n",
        "domain_metrics = get_domain_metrics(use_case)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(domain_prompt)\n",
        "print(domain_metrics)"
      ],
      "metadata": {
        "id": "ld2SvvNLGA25"
      },
      "id": "ld2SvvNLGA25",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "oFht3r2XQkIU",
      "metadata": {
        "id": "oFht3r2XQkIU"
      },
      "source": [
        "Store results for each sample query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b567550b",
      "metadata": {
        "id": "b567550b"
      },
      "outputs": [],
      "source": [
        "sample_results = []\n",
        "combined_run = {}  # For evaluation across all queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d55470",
      "metadata": {
        "id": "56d55470"
      },
      "outputs": [],
      "source": [
        "for query_id, query_info in sample_queries.items():\n",
        "    query_text = query_info['text']\n",
        "    print(f\"Running RAG for query: '{query_text}'\")\n",
        "\n",
        "    # Run the RAG pipeline with domain-specific prompt\n",
        "    retrieved_docs, answer, timings, run = run_rag_pipeline(\n",
        "        query_text,\n",
        "        query_id=query_id,\n",
        "        top_k_first_stage=100,\n",
        "        top_k_reranked=5,\n",
        "        domain_prompt=domain_prompt\n",
        "    )\n",
        "\n",
        "    # Combine run for overall evaluation\n",
        "    combined_run.update(run)\n",
        "\n",
        "    # Store results\n",
        "    sample_results.append({\n",
        "        'query_id': query_id,\n",
        "        'query_text': query_text,\n",
        "        'retrieved_docs': retrieved_docs,\n",
        "        'answer': answer,\n",
        "        'timings': timings,\n",
        "        'run': run\n",
        "    })\n",
        "\n",
        "    # Display a brief summary\n",
        "    print(f\"- Retrieved {len(retrieved_docs)} documents\")\n",
        "    print(f\"- Answer length: {len(answer)} characters\")\n",
        "    print(\"-----------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O0i9ACgQQgSZ",
      "metadata": {
        "id": "O0i9ACgQQgSZ"
      },
      "source": [
        "Evaluate all sample queries using `ir_measures` with domain-specific metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd771a0e",
      "metadata": {
        "id": "fd771a0e"
      },
      "outputs": [],
      "source": [
        "if combined_run and qrels_dict:\n",
        "    print(\"\\n=== Evaluation with ir_measures ===\\n\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics_results = evaluate_with_ir_measures(combined_run, qrels_dict, domain_metrics)\n",
        "\n",
        "    # Display results\n",
        "    print(\"Overall Evaluation Results:\")\n",
        "    for metric, value in metrics_results.items():\n",
        "        print(f\"  - {metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oC-h_ErXQeCW",
      "metadata": {
        "id": "oC-h_ErXQeCW"
      },
      "source": [
        "Detailed Results Viewer with Domain-Specific Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f02f69ea",
      "metadata": {
        "id": "f02f69ea"
      },
      "outputs": [],
      "source": [
        "query_index = 1  # @param [\"0\", \"1\", \"2\"] {type:\"raw\"}\n",
        "show_timings = True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a56bea3",
      "metadata": {
        "id": "7a56bea3"
      },
      "outputs": [],
      "source": [
        "if sample_results and query_index < len(sample_results):\n",
        "    result = sample_results[query_index]\n",
        "\n",
        "    # Evaluate this specific query with ir_measures using domain-specific metrics\n",
        "    if qrels_dict and result['query_id'] in qrels_dict:\n",
        "        metrics = evaluate_with_ir_measures(\n",
        "            result['run'],\n",
        "            qrels_dict,\n",
        "            domain_metrics\n",
        "        )\n",
        "    else:\n",
        "        metrics = None\n",
        "\n",
        "    # Display results with domain-specific formatting\n",
        "    display_rag_results(\n",
        "        result['query_text'],\n",
        "        result['retrieved_docs'],\n",
        "        result['answer'],\n",
        "        result['timings'] if show_timings else None,\n",
        "        metrics,\n",
        "        use_case\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TNrdtf--QpBD",
      "metadata": {
        "id": "TNrdtf--QpBD"
      },
      "source": [
        "## Try RAG with Your Own Query for this Domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "140ee90b",
      "metadata": {
        "id": "140ee90b"
      },
      "outputs": [],
      "source": [
        "\n",
        "user_query = \"What are the current research focus areas Sajal Das of Missouri S&T's Computer Science department faculty, and which projects have received major funding in the last year?\" # @param {type:\"string\"}\n",
        "top_k_first_stage = 100 # @param {type:\"slider\", min:10, max:500, step:10}\n",
        "top_k_reranked = 5 # @param {type:\"slider\", min:1, max:20, step:1}\n",
        "show_timings = True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe00227",
      "metadata": {
        "id": "fbe00227"
      },
      "outputs": [],
      "source": [
        "# If no query is provided, use a default example from the selected use case\n",
        "if not user_query:\n",
        "    user_query = use_cases[use_case][\"example_queries\"][0]\n",
        "    print(f\"Using example query: \\\"{user_query}\\\"\")\n",
        "    print(\"Enter your own query above to try something different!\")\n",
        "else:\n",
        "    print(f\"Processing custom query: \\\"{user_query}\\\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743382ff",
      "metadata": {
        "id": "743382ff"
      },
      "outputs": [],
      "source": [
        "# Get domain-specific prompt\n",
        "domain_prompt = use_cases[use_case][\"domain_prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff590fe1",
      "metadata": {
        "id": "ff590fe1"
      },
      "outputs": [],
      "source": [
        "# Run the RAG pipeline with domain-specific prompt\n",
        "retrieved_docs, answer, timings, _ = run_rag_pipeline(\n",
        "    user_query,\n",
        "    top_k_first_stage=top_k_first_stage,\n",
        "    top_k_reranked=top_k_reranked,\n",
        "    domain_prompt=domain_prompt,\n",
        "    show_timings=show_timings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e33cc4",
      "metadata": {
        "id": "71e33cc4"
      },
      "outputs": [],
      "source": [
        "# Display results with domain-specific formatting\n",
        "display_rag_results(\n",
        "    user_query,\n",
        "    retrieved_docs,\n",
        "    answer,\n",
        "    timings if show_timings else None,\n",
        "    use_case=use_case\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä Understanding the Results\n",
        "\n",
        "Let's analyze what happened in this RAG example:\n",
        "\n",
        "1. **Retrieval Quality**:\n",
        "   - Examine the relevance scores of retrieved documents\n",
        "   - Higher cross-encoder scores indicate better matches to the query\n",
        "   - Does the first document contain the information needed?\n",
        "\n",
        "2. **Answer Completeness**:\n",
        "   - Check if the generated answer incorporates information from the retrieved documents\n",
        "   - Is the answer comprehensive or missing key information?\n",
        "\n",
        "3. **Domain Appropriateness**:\n",
        "   - Does the response match the expected format for this domain?\n",
        "   - For scientific content: Are citations and caveats included?\n",
        "   - For technical support: Are steps clear and actionable?\n",
        "   - For educational content: Is the explanation accessible and structured?\n",
        "\n",
        "Try modifying the query slightly and observe how the retrieval results change."
      ],
      "metadata": {
        "id": "rrEuzlMHNASF"
      },
      "id": "rrEuzlMHNASF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstrating LLM Hallucination Without Retrieval\n",
        "\n",
        "To illustrate why the retrieval component is crucial in RAG systems, let's compare our previous RAG results with what happens when we ask the same question directly to the LLM without providing retrieved documents."
      ],
      "metadata": {
        "id": "N0tKm9JjwO5n"
      },
      "id": "N0tKm9JjwO5n"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üîç With RAG vs. Without RAG Comparison\")\n",
        "print(f\"\\nQuery: \\\"{user_query}\\\"\")\n",
        "\n",
        "# Function to generate answer without retrieval context\n",
        "def generate_answer_without_retrieval(query, use_case=\"Scientific Research\", max_length=500):\n",
        "    \"\"\"Generate an answer using the Mistral API without any retrieved context\"\"\"\n",
        "\n",
        "    api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"MISTRAL_API_KEY environment variable not set. Please set it before calling this function.\")\n",
        "\n",
        "    # Mistral API endpoint\n",
        "    api_url = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "\n",
        "    # Create domain-specific prompts without retrieval references\n",
        "    no_retrieval_prompts = {\n",
        "        \"Scientific Research\": \"You are a scientific research assistant. Provide an accurate and evidence-based answer to the user's question. Include appropriate scientific context and caveats where applicable.\",\n",
        "\n",
        "        \"Technical Support\": \"You are a technical support specialist. Provide a concise and practical solution to the user's problem. Structure your answer step-by-step, and ensure instructions are clear and executable.\",\n",
        "\n",
        "        \"Education & Library\": \"You are an educational assistant. Provide a well-structured, accurate, and learner-friendly explanation of the topic. Break down complex concepts into simpler terms and include definitions or examples when needed.\",\n",
        "\n",
        "        \"Fact Verification\": \"You are a fact-checking assistant. Provide a balanced, evidence-based analysis of the user's query. If information is inconclusive or uncertain, clearly state the limitations in current knowledge.\",\n",
        "\n",
        "        \"Healthcare Information\": \"You are a healthcare information assistant. Provide an accurate summary addressing the user's question. Note that this is not medical advice. Focus on established research findings and clinical guidelines, and clearly state any limitations in the available evidence.\",\n",
        "\n",
        "        \"Campus Info\": \"You are a helpful campus assistant for Missouri S&T. Answer the user's question clearly and accurately. Be concise and maintain a friendly and professional tone.\"\n",
        "    }\n",
        "\n",
        "    # Get the prompt based on the use case\n",
        "    no_retrieval_prompt = no_retrieval_prompts.get(use_case, \"You are a helpful assistant. Answer the user's question accurately and concisely.\")\n",
        "\n",
        "    # Create the messages list\n",
        "    messages = []\n",
        "\n",
        "    # Add system message with the no-retrieval prompt\n",
        "    messages.append({\n",
        "        \"role\": \"system\",\n",
        "        \"content\": no_retrieval_prompt.strip()\n",
        "    })\n",
        "\n",
        "    # Add user message with only the query\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": query.strip()\n",
        "    })\n",
        "\n",
        "    # Prepare the API request payload\n",
        "    payload = {\n",
        "        \"model\": \"mistral-large-latest\",  # You can change to other Mistral models as needed\n",
        "        \"messages\": messages,\n",
        "        \"max_tokens\": max_length,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.9,\n",
        "    }\n",
        "\n",
        "    # Set up headers with API key\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Make the API request\n",
        "        response = requests.post(api_url, headers=headers, json=payload)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        # Parse the JSON response\n",
        "        response_data = response.json()\n",
        "\n",
        "        # Extract the generated text\n",
        "        generated_text = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Mistral API: {e}\")\n",
        "        # Fallback to a simple response if API call fails\n",
        "        return f\"I couldn't generate a response using the Mistral API due to an error: {str(e)}\""
      ],
      "metadata": {
        "id": "FCUhWTCuwYUk"
      },
      "id": "FCUhWTCuwYUk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Generate answer without retrieval using the current use case\n",
        "no_retrieval_answer = generate_answer_without_retrieval(user_query, use_case)\n",
        "no_retrieval_time = time.time() - start_time\n",
        "\n",
        "# Display results\n",
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "display(HTML(\"<h2 style='color:#eee;'>Comparing RAG vs. No Retrieval</h2>\"))\n",
        "\n",
        "# Display the comparison in a side-by-side view\n",
        "comparison_html = f'''\n",
        "<div style=\"display: flex; flex-direction: row; gap: 20px;\">\n",
        "    <div style=\"flex: 1; border-radius: 8px; padding: 16px; background-color: #1e1e1e;\">\n",
        "        <h3 style=\"color: #3498db;\">‚úÖ With Retrieval (RAG)</h3>\n",
        "        <p style=\"color: #aaa; font-style: italic;\">Response time: {timings[\"total\"]:.3f}s</p>\n",
        "        <div style=\"border-left: 5px solid #2ecc71; padding: 12px; background-color: #2a2a2a; color: #e0e0e0; border-radius: 4px;\">\n",
        "            {answer}\n",
        "        </div>\n",
        "    </div>\n",
        "    <div style=\"flex: 1; border-radius: 8px; padding: 16px; background-color: #1e1e1e;\">\n",
        "        <h3 style=\"color: #e74c3c;\">‚ùå Without Retrieval</h3>\n",
        "        <p style=\"color: #aaa; font-style: italic;\">Response time: {no_retrieval_time:.3f}s</p>\n",
        "        <div style=\"border-left: 5px solid #e74c3c; padding: 12px; background-color: #2a2a2a; color: #e0e0e0; border-radius: 4px;\">\n",
        "            {no_retrieval_answer}\n",
        "        </div>\n",
        "    </div>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "display(HTML(comparison_html))"
      ],
      "metadata": {
        "id": "fvrH213gwiLR"
      },
      "id": "fvrH213gwiLR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç Understanding Hallucinations in LLMs\n",
        "\n",
        "In the comparison above, we can observe how the LLM's response differs with and without retrieval:\n",
        "\n",
        "1. **Without Retrieval**:\n",
        "   - The model generates a plausible-sounding answer based on its training data\n",
        "   - It may include outdated, incomplete, or incorrect information\n",
        "   - There is no way to verify claims or trace information sources\n",
        "   - The model might confidently state incorrect facts\n",
        "\n",
        "2. **With Retrieval (RAG)**:\n",
        "   - The response is grounded in specific documents\n",
        "   - Information is traceable to sources\n",
        "   - The model can acknowledge uncertainty when evidence is limited\n",
        "   - The answer quality is limited by the quality of the retrieved documents\n",
        "\n",
        "This demonstrates why RAG is essential for applications requiring factual accuracy, especially in domains like healthcare, scientific research, technical support, and education.\n",
        "\n",
        "When evaluating RAG systems, it's valuable to compare with non-RAG baselines to measure the improvement in factual accuracy and the reduction in hallucinations."
      ],
      "metadata": {
        "id": "VFHPEpKOwvXr"
      },
      "id": "VFHPEpKOwvXr"
    },
    {
      "cell_type": "markdown",
      "id": "MVbi-jDfQ1ku",
      "metadata": {
        "id": "MVbi-jDfQ1ku"
      },
      "source": [
        "\n",
        "## Best Practices for RAG in Each Domain\n",
        "\n",
        "### Scientific Research\n",
        "- Use domain-specific embeddings trained on scientific literature.\n",
        "- Implement citation tracking to show provenance of information.\n",
        "- Consider combining with a knowledge graph for concept relationships.\n",
        "- Include publication date as a ranking factor for recency.\n",
        "- Use subject-specific vocabularies for query expansion.\n",
        "\n",
        "### Technical Support\n",
        "- Optimize for high precision in top results.\n",
        "- Implement conversational context to track troubleshooting steps.\n",
        "- Use step detection to format answers as procedures.\n",
        "- Consider hybrid retrieval combining semantic and keyword search.\n",
        "- Add feedback mechanisms to improve answer quality over time.\n",
        "\n",
        "### Education & Library\n",
        "- Organize retrieved content by difficulty/education level.\n",
        "- Implement learning path generation based on prerequisite concepts.\n",
        "- Support multiple learning styles in answer generation.\n",
        "- Include multimedia content recommendations when available.\n",
        "- Use readability scores to match content to learner level.\n",
        "\n",
        "### Legal Document Analysis\n",
        "- Prioritize precision and traceability of information.\n",
        "- Implement jurisdiction detection for relevant legal standards.\n",
        "- Track legal precedent relationships between documents.\n",
        "- Use specialized legal embeddings trained on case law.\n",
        "- Include confidence scores and disclaimers in generated answers.\n",
        "\n",
        "### Healthcare Information\n",
        "- Implement evidence quality assessment for medical information.\n",
        "- Include recency as a critical ranking factor.\n",
        "- Use medical taxonomy mapping for query expansion.\n",
        "- Apply stricter fact verification for medical claims.\n",
        "- Include appropriate medical disclaimers in answers.\n",
        "\n",
        "### Campus Info\n",
        "- Use location-aware retrieval for campus service queries.\n",
        "- Implement up-to-date checking for office hours and contact info.\n",
        "- Include department hierarchy awareness for better routing.\n",
        "- Use hybrid search combining exact name matching with semantic.\n",
        "- Prioritize official pages over event announcements.\n",
        "\n",
        "---\n",
        "\n",
        "## General Implementation Guidance for RAG\n",
        "\n",
        "When implementing RAG for a specific domain, also consider:\n",
        "\n",
        "- **Pre-processing:** Apply domain-specific cleaning and normalization.\n",
        "- **Chunking strategy:** Adjust document chunking based on domain document structure.\n",
        "- **Embedding models:** Fine-tune or select embeddings trained on domain data.\n",
        "- **LLM prompting:** Craft specialized prompts with domain context.\n",
        "- **Post-processing:** Format answers according to domain conventions.\n",
        "\n",
        "If you do not have a domain-specific use case:\n",
        "\n",
        "- Select appropriate embedding models for your specific use case.\n",
        "- Experiment with chunking strategies based on your document structure.\n",
        "- Fine-tune retrieval parameters (k values, reranking) based on evaluation.\n",
        "- Craft effective prompts that include relevant context for the LLM.\n",
        "- Implement user feedback mechanisms to improve over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Troubleshooting Common Issues\n",
        "\n",
        "When implementing RAG systems, you may encounter these common challenges:\n",
        "\n",
        "### Retrieval Problems\n",
        "\n",
        "1. **Irrelevant Documents Retrieved**\n",
        "   - **Symptoms**: Retrieved documents don't match query intent\n",
        "   - **Solutions**:\n",
        "     - Try different embedding models (domain-specific if available)\n",
        "     - Adjust chunking strategy (smaller or larger chunks)\n",
        "     - Implement query expansion or reformulation\n",
        "     - Add a reranking step with a cross-encoder\n",
        "\n",
        "2. **Missing Information**\n",
        "   - **Symptoms**: System can't find information you know exists in the corpus\n",
        "   - **Solutions**:\n",
        "     - Increase the number of retrieved documents (k)\n",
        "     - Try hybrid retrieval (combine dense and sparse methods)\n",
        "     - Check document preprocessing (ensure no information loss)\n",
        "\n",
        "### Generation Issues\n",
        "\n",
        "1. **Hallucinations Despite RAG**\n",
        "   - **Symptoms**: Model generates incorrect information despite relevant context\n",
        "   - **Solutions**:\n",
        "     - Adjust prompt to emphasize using only retrieved information\n",
        "     - Implement fact-checking or confidence scoring\n",
        "     - Use a model with better instruction following\n",
        "\n",
        "2. **Poor Integration of Retrieved Content**\n",
        "   - **Symptoms**: Answer doesn't incorporate relevant retrieved information\n",
        "   - **Solutions**:\n",
        "     - Improve context formatting in the prompt\n",
        "     - Try different LLMs or parameter settings\n",
        "     - Test chain-of-thought or step-by-step reasoning\n",
        "\n",
        "### API and Resource Issues\n",
        "\n",
        "1. **Slow Performance**\n",
        "   - Try batch processing where possible\n",
        "   - Consider quantized models or optimized libraries\n",
        "   - Use caching for repeated queries\n",
        "\n",
        "2. **API Failures**\n",
        "   - Implement robust error handling and retries\n",
        "   - Have fallback models or approaches ready\n",
        "   - Cache results whenever possible\n",
        "\n",
        "If you encounter any of these issues during the tutorial, raise your hand, and we'll work through them together."
      ],
      "metadata": {
        "id": "EG8AXP5RN4xN"
      },
      "id": "EG8AXP5RN4xN"
    },
    {
      "cell_type": "markdown",
      "id": "GZiEEIdXQ_qo",
      "metadata": {
        "id": "GZiEEIdXQ_qo"
      },
      "source": [
        "## Evaluation Summary for Current Use Case"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rC2DNBhTRUnY",
      "metadata": {
        "id": "rC2DNBhTRUnY"
      },
      "source": [
        "## üéì Conclusion: Building Effective Domain-Specific RAG Systems\n",
        "\n",
        "This tutorial has demonstrated how to build and customize RAG systems for different domains using prebuilt indexes from the `ir_datasets` library. By leveraging public datasets and sentence-transformer embeddings, we've implemented practical RAG solutions tailored to real-world use cases.\n",
        "\n",
        "### Core Principles for Domain-Specific RAG\n",
        "\n",
        "1. **Know Your Domain**\n",
        "   - Understand the specific information needs and constraints\n",
        "   - Identify domain-specific language, terminology, and concepts\n",
        "   - Recognize the appropriate evidence standards and response formats\n",
        "\n",
        "2. **Customize Each Component**\n",
        "   - Data sources: Select and preprocess domain-appropriate content\n",
        "   - Embedding models: Choose or fine-tune for domain language\n",
        "   - Retrieval strategy: Optimize parameters for domain needs\n",
        "   - Prompting: Design domain-specific instructions and constraints\n",
        "   - Evaluation: Measure what matters for your specific use case\n",
        "\n",
        "3. **Iterate with Feedback**\n",
        "   - Collect domain expert evaluation on system outputs\n",
        "   - Analyze failure cases to identify improvement areas\n",
        "   - Continuously update knowledge bases as domains evolve\n",
        "\n",
        "### Why Domain-Specific RAG Matters\n",
        "\n",
        "Adapting RAG systems to specific domains significantly improves performance:\n",
        "\n",
        "- **Specialized Knowledge**: Retrieve content that aligns with domain-specific needs\n",
        "- **Improved Contextual Understanding**: Domain-aware prompting leads to higher-quality answers\n",
        "- **Targeted Evaluation**: Metrics tailored to domain objectives provide realistic assessments\n",
        "- **Enhanced User Experience**: Formatting and output are better suited to user expectations\n",
        "- **Higher Relevance**: Focused retrieval leads to better precision and recall\n",
        "\n",
        "### From Tutorial to Production\n",
        "\n",
        "To move from this tutorial to production-ready systems:\n",
        "\n",
        "1. **Scale Your Data Pipeline**\n",
        "   - Implement efficient document ingestion workflows\n",
        "   - Set up regular knowledge base updates\n",
        "   - Consider hybrid storage solutions for different content types\n",
        "\n",
        "2. **Optimize for Performance**\n",
        "   - Benchmark and optimize vector search for your scale\n",
        "   - Implement caching strategies for common queries\n",
        "   - Consider quantization for embedding models\n",
        "\n",
        "3. **Enhance with Advanced Techniques**\n",
        "   - Query rewriting and decomposition\n",
        "   - Multi-stage retrieval architectures\n",
        "   - Ensemble methods for improved accuracy\n",
        "   - Hybrid search combining sparse and dense methods\n",
        "   - Sophisticated chunking strategies (overlapping, hierarchical, semantic)\n",
        "   - Self-reflection and validation capabilities\n",
        "\n",
        "4. **Implement Robust Monitoring**\n",
        "   - Track retrieval quality metrics over time\n",
        "   - Monitor for knowledge gaps and hallucinations\n",
        "   - Implement feedback loops from end users\n",
        "\n",
        "### Suggested Next Steps\n",
        "\n",
        "To expand upon this tutorial:\n",
        "\n",
        "- **Build Your Own Indexes**:\n",
        "  - Use datasets from `ir_datasets` such as `beir/trec-covid`, `nfcorpus`, `cqadupstack`\n",
        "  - Generate dense embeddings using Sentence Transformers\n",
        "  - Build and store FAISS indexes for fast retrieval\n",
        "\n",
        "- **Customize for Your Domain**:\n",
        "  - Create tailored prompt templates\n",
        "  - Design custom evaluation methods\n",
        "  - Use visualizations relevant to your domain\n",
        "  - Implement hybrid retrieval suited to the task\n",
        "\n",
        "- **Explore Advanced RAG Architectures**:\n",
        "  - Experiment with multi-hop RAG for complex reasoning\n",
        "  - Try Hypothetical Document Embeddings (HyDE)\n",
        "  - Integrate knowledge graphs with vector search\n",
        "  - Implement fine-tuning for domain-specific retrievers\n",
        "\n",
        "### Further Learning Resources\n",
        "\n",
        "- **[ir_datasets](https://ir-datasets.com/)** & [GitHub](https://github.com/allenai/ir_datasets)\n",
        "- **[BEIR Benchmark](https://github.com/beir-cellar/beir)**\n",
        "- **[HuggingFace Datasets](https://huggingface.co/datasets?search=irds)**\n",
        "- **[Sentence Transformers](https://www.sbert.net/)**\n",
        "- **[FAISS Library](https://github.com/facebookresearch/faiss)**\n",
        "- **[LangChain Docs](https://python.langchain.com/docs/modules/data_connection/retrievers/)**\n",
        "- **[LlamaIndex Docs](https://docs.llamaindex.ai/)**\n",
        "\n",
        "The field of RAG is rapidly evolving, with new techniques and models emerging regularly. By focusing on domain-specific adaptations, you can build systems that deliver truly valuable and trustworthy information experiences across scientific research, technical support, education, fact verification, healthcare, and beyond."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FBiftLCvwT_L",
      "metadata": {
        "id": "FBiftLCvwT_L"
      },
      "source": [
        "\n",
        "## üß≠ A Note on Real-World RAG: Handling Multiple Data Sources with Query Routing\n",
        "\n",
        "In this tutorial, we‚Äôve simplified the setup by letting you choose a single **use case** (e.g., *Scientific Research*, *Technical Support*), and loading **one pre-built index** corresponding to a default dataset for that domain (like `beir/trec-covid` or `beir/cqadupstack/android`). All queries are executed against this single, focused index.\n",
        "\n",
        "This works well for learning the core components of Retrieval-Augmented Generation (RAG) in a controlled environment. However, in production systems, the reality is far more complex.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Real-World RAG Involves Many Diverse Sources\n",
        "\n",
        "Imagine building a RAG system for an enterprise or large-scale application. A single unified index often isn‚Äôt practical or efficient. Real-world systems typically need to answer questions using content drawn from multiple **heterogeneous sources**, such as:\n",
        "\n",
        "- ‚úÖ Internal technical documentation (e.g., Confluence, GitHub wikis)  \n",
        "- ‚úÖ Public product FAQs and marketing sites  \n",
        "- ‚úÖ A database of customer support tickets or live chat logs  \n",
        "- ‚úÖ Regulatory PDFs, whitepapers, or clinical guidelines  \n",
        "- ‚úÖ Web content or external APIs\n",
        "\n",
        "Indexing all of this into one giant vector store is usually **not scalable**, and can dilute retrieval precision. Instead, most real-world systems maintain **multiple specialized indexes**, each optimized for a different content type, domain, or access policy.\n",
        "\n",
        "---\n",
        "\n",
        "### üö¶ Enter Query Routing\n",
        "\n",
        "So how does a system decide **which index (or tool) to query** for a given user question?\n",
        "\n",
        "This is where **Query Routing** becomes essential. A *Query Router* acts like a smart traffic controller inside the RAG pipeline. Before retrieving documents, it analyzes the incoming query and chooses the most relevant source(s) ‚Äî or even routes the request to external tools like search APIs, databases, or reasoning modules.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Common Query Routing Strategies\n",
        "\n",
        "Here are several routing strategies used in modern RAG systems:\n",
        "\n",
        "1. **LLM-Based Routing**  \n",
        "   Use a lightweight [language model](https://arxiv.org/abs/2303.11366) to classify or analyze the query intent. The LLM chooses the best index by comparing the query to high-level descriptions of each source.  \n",
        "   *Example:* ‚ÄúWhat is the refund policy for software licenses?‚Äù ‚Üí routed to the policy documents index.\n",
        "\n",
        "2. **Semantic Routing (Embedding Similarity)**  \n",
        "   Compute a dense vector (embedding) of the query, then compare it to ‚Äúrepresentative vectors‚Äù for each index. This can be done using [FAISS](https://github.com/facebookresearch/faiss), [ScaNN](https://github.com/google-research/google-research/tree/master/scann), or [milvus](https://milvus.io/).  \n",
        "   *Example:* Map queries to closest-matching domains using cosine similarity.\n",
        "\n",
        "3. **Keyword or Rule-Based Routing**  \n",
        "   Set up simple keyword triggers or regex patterns to map queries to sources.  \n",
        "   *Example:* Queries containing ‚Äúerror code‚Äù or ‚Äústack trace‚Äù ‚Üí routed to technical docs.  \n",
        "   This is fast and interpretable, but brittle if query language varies a lot.\n",
        "\n",
        "4. **Metadata Filtering within Shared Indexes**  \n",
        "   If you choose to merge multiple datasets into one index, you can still route using metadata fields (e.g., `document_source = 'faq'`). This lets you use **filters** to scope retrieval only to relevant subsets.\n",
        "\n",
        "---\n",
        "\n",
        "### üèóÔ∏è Hybrid Routing Is Common\n",
        "\n",
        "In practice, many systems combine these approaches:\n",
        "\n",
        "- Use **keyword filtering** for high-precision rules  \n",
        "- Fall back to **semantic routing** for open-ended questions  \n",
        "- Use an **LLM router** for nuanced intent classification  \n",
        "- Combine results from multiple indexes and rerank with a **cross-encoder** or **relevance model**\n",
        "\n",
        "Frameworks like [LangChain](https://docs.langchain.com/docs/components/retrievers/router-retriever/) and [Haystack](https://docs.haystack.deepset.ai/docs/routing_queries) offer modules for building query routers out-of-the-box.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Why We Didn‚Äôt Use Query Routing *Here*\n",
        "\n",
        "For this tutorial, we kept the setup simple by pre-selecting one dataset per domain and routing all queries to a single index. This helps focus on learning RAG fundamentals like retrieval, reranking, and generation ‚Äî without worrying about architectural complexity.\n",
        "\n",
        "But in real-world applications ‚Äî especially in enterprise, healthcare, legal, or customer support scenarios ‚Äî **query routing is essential** to handle:\n",
        "- Information silos\n",
        "- Data volume\n",
        "- Index-specific latency\n",
        "- Access control\n",
        "\n",
        "If you're building a production-ready RAG system, consider implementing query routing as an early design decision.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Further Reading & Examples\n",
        "\n",
        "- [Query Routing in LangChain](https://docs.langchain.com/docs/components/retrievers/router-retriever/)\n",
        "- [Multihop RAG Routing in Google's FiD](https://arxiv.org/abs/2007.00849)\n",
        "- [Retrieval Strategies in Haystack](https://docs.haystack.deepset.ai/docs/routing_queries)\n",
        "- [Prompting for Tool Use](https://arxiv.org/abs/2302.12337) in Multi-Tool LLM systems"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}